{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbadd93c",
   "metadata": {},
   "source": [
    "# 1. \n",
    "\n",
    "What are the advantages of a CNN over a fully connected DNN for image classification?\n",
    "\n",
    "## My solution \n",
    "\n",
    "DNN work when there is a single object and it is centered in the image. CNNs work on multiple objects and it doesn't have to be centered. \n",
    "\n",
    "## Book Solution\n",
    "\n",
    "These are the main advantages of a CNN over a fully connected DNN for image classification:\n",
    "\n",
    "1. Because consecutive layers are only partially connected and because it heavily reuses its weights, a CNN has many fewer parameters than a fully connected DNN, which makes it much faster to train, reduces the risk of overfitting, and requires much less training data.\n",
    "\n",
    "2. When a CNN has learned a kernel that can detect a particular feature, it can detect that feature anywhere in the image. In contrast, when a DNN learns a feature in one location, it can detect it only in that particular location. Since images typically have very repetitive features, CNNs are able to generalize much better than DNNs for image processing tasks such as classification, using fewer training examples.\n",
    "\n",
    "3. Finally, a DNN has no prior knowledge of how pixels are organized; it does not know that nearby pixels are close. A CNNâ€™s architecture embeds this prior knowledge. Lower layers typically identify features in small areas of the images, while higher layers combine the lower-level features into larger features. This works well with most natural images, giving CNNs a decisive head start compared to DNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57137f1e",
   "metadata": {},
   "source": [
    "# 2. \n",
    "\n",
    "Consider a CNN composed of three convolutional layers, each with 3 Ã— 3 kernels, a stride of 2, and \"same\" padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200 Ã— 300 pixels.\n",
    "\n",
    "What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance? What about when training on a mini-batch of 50 images?\n",
    "\n",
    "## My solution \n",
    "I was terribly wrong. Please see the book solution\n",
    "\n",
    "RBG indicates that we have 3 channels. \n",
    "\n",
    "\n",
    "- For example, consider a convolutional layer with 5 Ã— 5 filters, outputting 200 feature maps of size 150 Ã— 100, with stride 1 and \"same\" padding. If the input is a 150 Ã— 100 RGB image (three channels), then the number of parameters is (5 Ã— 5 Ã— 3 + 1) Ã— 200 = 15,200 (the + 1 corresponds to the bias terms)\n",
    "\n",
    "\n",
    "(3 x 3 x 3 + 1) x 100 = 2800. The first layer has 2800 parameters.\n",
    "\n",
    "(3 x 3 x 3 + 1) x 200 = 5600. The second layer has 5600.\n",
    "\n",
    "(3 x 3 x 3 + 1) x 400 = 11,200. The third layer has 11,200.\n",
    "\n",
    "The total sum of paramters is 19,600. \n",
    "\n",
    "-  However, each of the 200 feature maps contains 150 Ã— 100 neurons, and each of these neurons needs to compute a weighted sum of its 5 Ã— 5 Ã— 3 = 75 inputs: thatâ€™s a total of 225 million float multiplications. For the amount of multiplications:\n",
    "\n",
    "$$ \\text{number of feature maps} * \\text{neuronsPerLayer} * \\text{ Inputs} $$\n",
    "\n",
    "(100+200+400)*(200*300)*(3*3*3) = 1,134,000,000. \n",
    "\n",
    "Or over a billion multiplications. \n",
    "\n",
    "- Moreover, if the feature maps are represented using 32-bit floats, then the convolutional layerâ€™s output will occupy 200 Ã— 150 Ã— 100 Ã— 32 = 96 million bits (12 MB) of RAM. (featureMaps*neurons*bits)\n",
    "\n",
    "700(200*300)*(32) = 1,344,000,000\n",
    "Over a billion bits. \n",
    "\n",
    "one Megabyte = 8*10^6 bits. \n",
    "\n",
    "Thus this is over 168 megabytes. For a single instance. \n",
    "\n",
    "For a batch of 50, we multiple by 50\n",
    "\n",
    "so 50(168Megabytes) = 8400 megabytes. \n",
    "\n",
    "Or 8.2 gigabytes. \n",
    "\n",
    "\n",
    "## Book Solution\n",
    "\n",
    "Letâ€™s compute how many parameters the CNN has. Since its first convolutional layer has 3 Ã— 3 kernels, and the input has three channels (red, green, and blue), each feature map has 3 Ã— 3 Ã— 3 weights, plus a bias term. Thatâ€™s 28 parameters per feature map. Since this first convolutional layer has 100 feature maps, it has a total of 2,800 parameters. The second convolutional layer has 3 Ã— 3 kernels and its input is the set of 100 feature maps of the previous layer, so each feature map has 3 Ã— 3 Ã— 100 = 900 weights, plus a bias term. Since it has 200 feature maps, this layer has 901 Ã— 200 = 180,200 parameters. Finally, the third and last convolutional layer also has 3 Ã— 3 kernels, and its input is the set of 200 feature maps of the previous layers, so each feature map has 3 Ã— 3 Ã— 200 = 1,800 weights, plus a bias term. Since it has 400 feature maps, this layer has a total of 1,801 Ã— 400 = 720,400 parameters. All in all, the CNN has 2,800 + 180,200 + 720,400 = 903,400 parameters.\n",
    "\n",
    "Now letâ€™s compute how much RAM this neural network will require (at least) when making a prediction for a single instance. First letâ€™s compute the feature map size for each layer. Since we are using a stride of 2 and \"same\" padding, the horizontal and vertical dimensions of the feature maps are divided by 2 at each layer (rounding up if necessary). So, as the input channels are 200 Ã— 300 pixels, the first layerâ€™s feature maps are 100 Ã— 150, the second layerâ€™s feature maps are 50 Ã— 75, and the third layerâ€™s feature maps are 25 Ã— 38. Since 32 bits is 4 bytes and the first convolutional layer has 100 feature maps, this first layer takes up 4 Ã— 100 Ã— 150 Ã— 100 = 6 million bytes (6 MB). The second layer takes up 4 Ã— 50 Ã— 75 Ã— 200 = 3 million bytes (3 MB). Finally, the third layer takes up 4 Ã— 25 Ã— 38 Ã— 400 = 1,520,000 bytes (about 1.5 MB). However, once a layer has been computed, the memory occupied by the previous layer can be released, so if everything is well optimized, only 6 + 3 = 9 million bytes (9 MB) of RAM will be required (when the second layer has just been computed, but the memory occupied by the first layer has not been released yet). But wait, you also need to add the memory occupied by the CNNâ€™s parameters! We computed earlier that it has 903,400 parameters, each using up 4 bytes, so this adds 3,613,600 bytes (about 3.6 MB). The total RAM required is therefore (at least) 12,613,600 bytes (about 12.6 MB).\n",
    "\n",
    "Lastly, letâ€™s compute the minimum amount of RAM required when training the CNN on a mini-batch of 50 images. During training TensorFlow uses backpropagation, which requires keeping all values computed during the forward pass until the reverse pass begins. So we must compute the total RAM required by all layers for a single instance and multiply that by 50. At this point, letâ€™s start counting in megabytes rather than bytes. We computed before that the three layers require respectively 6, 3, and 1.5 MB for each instance. Thatâ€™s a total of 10.5 MB per instance, so for 50 instances the total RAM required is 525 MB. Add to that the RAM required by the input images, which is 50 Ã— 4 Ã— 200 Ã— 300 Ã— 3 = 36 million bytes (36 MB), plus the RAM required for the model parameters, which is about 3.6 MB (computed earlier), plus some RAM for the gradients (we will neglect this since it can be released gradually as backpropagation goes down the layers during the reverse pass). We are up to a total of roughly 525 + 36 + 3.6 = 564.6 MB, and thatâ€™s really an optimistic bare minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b96b1d",
   "metadata": {},
   "source": [
    "# 3. \n",
    "\n",
    "If your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem?\n",
    "\n",
    "## My solution \n",
    "\n",
    "1. Reduce the bytes used (ie. Use 16 byte precision instead of 32).\n",
    "\n",
    "2. Try mini-batches. \n",
    "\n",
    "3. We can reduce dimensionality using a stride. \n",
    "\n",
    "4. Distribute the CNN across multiple devices. \n",
    "\n",
    "5. Remove layers. \n",
    "\n",
    "\n",
    "## Book Solution\n",
    "\n",
    "ðŸ’¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c1648",
   "metadata": {},
   "source": [
    "# 4. \n",
    "\n",
    "Why would you want to add a max pooling layer rather than a convolutional layer with the same stride?\n",
    "\n",
    "## My solution \n",
    "\n",
    "Pooling layers do not use weights. A max pooling layer will reduce computations, memory usage, and the number of parameters, a max pooling layer also introduces some level of invariance to small translations. \n",
    "\n",
    "## Book Solution\n",
    "\n",
    "A max pooling layer has no parameters at all, whereas a convolutional layer has quite a few (see the previous questions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eb0cde",
   "metadata": {},
   "source": [
    "# 5. \n",
    "\n",
    "When would you want to add a local response normalization layer?\n",
    "\n",
    "## My solution \n",
    "\n",
    "A local response localilzation layers pushes layers to specialize in visualizing a certain thing. This can lead to layers picking up a large range of features rather than all learning the same ones. \n",
    "\n",
    "## Book Solution\n",
    "\n",
    "A local response normalization layer makes the neurons that most strongly activate inhibit neurons at the same location but in neighboring feature maps, which encourages different feature maps to specialize and pushes them apart, forcing them to explore a wider range of features. It is typically used in the lower layers to have a larger pool of low-level features that the upper layers can build upon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb376ba8",
   "metadata": {},
   "source": [
    "# 6. \n",
    "\n",
    "Can you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet, ResNet, SENet, and Xception?\n",
    "\n",
    "## My solution \n",
    "\n",
    "- AlexNet: This new architecture is not only larger and deeper(than LenNet), it stacks convolutional layers directly on top one another. Intead of stackign a pooling layer on top of each convolutional layer. \n",
    "\n",
    "- GoogLeNet: Google introduced inception modules. Inception modules foces data down a 1 by 1 kernel. Allowing these layers to catch patterns at different scales.  \n",
    "\n",
    "- ResNet: This architecture introduces skip connections. Skip connections introduce inputs a few layers down, forcing out network to model on residuals. This speeds up training. \n",
    "\n",
    "- SENet: Can be put on most architectures.Looks only at the depth dimension(1by1 layers?). Determines which features are actively represented together and enchances the features it has found. This allows us to focus on relevant features. \n",
    "\n",
    "- Xception: Combines the architecture of GoogLeNet and ResNet. Replaces inception modules with depthwise seperable convolution layers. These new layers make an assumption that spatial patterns and cross-channel patterns can be modeled seperatlely. They require fewer parameters, computations, and generally perform better than regular convolutional NNs. \n",
    "\n",
    "\n",
    "## Book Solution\n",
    "\n",
    "The main innovations in AlexNet compared to LeNet-5 are that it is much larger and deeper, and it stacks convolutional layers directly on top of each other, instead of stacking a pooling layer on top of each convolutional layer. The main innovation in GoogLeNet is the introduction of inception modules, which make it possible to have a much deeper net than previous CNN architectures, with fewer parameters. ResNetâ€™s main innovation is the introduction of skip connections, which make it possible to go well beyond 100 layers. Arguably, its simplicity and consistency are also rather innovative. SENetâ€™s main innovation was the idea of using an SE block (a two-layer dense network) after every inception module in an inception network or every residual unit in a ResNet to recalibrate the relative importance of feature maps. Finally, Xceptionâ€™s main innovation was the use of depthwise separable convolutional layers, which look at spatial patterns and depthwise patterns separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911d2f34",
   "metadata": {},
   "source": [
    "# 7. \n",
    "\n",
    "What is a fully convolutional network? How can you convert a dense layer into a convolutional layer?\n",
    "\n",
    "## My solution \n",
    "\n",
    "A fully convolutional network replaces dense networks with convolutional networks. Allowing us to process images of any size, assuming they have the same number of channels. \n",
    "\n",
    "To convert a dense layer to a convolutional layer, the number of filters in the convolutional layer must be equal to the number of units in the dense layer, the filter size must be equal to the size of the input feature maps, and you must use \"valid\" padding. The stride may be set to 1 or more, as we will see shortly. It is able to do so as the output will be the same as a dense network with one small adjustment, the output tensor will contain two scalers that are one. \n",
    "\n",
    "\n",
    "## Book Solution\n",
    "\n",
    "Fully convolutional networks are neural networks composed exclusively of convolutional and pooling layers. FCNs can efficiently process images of any width and height (at least above the minimum size). They are most useful for object detection and semantic segmentation because they only need to look at the image once (instead of having to run a CNN multiple times on different parts of the image). If you have a CNN with some dense layers on top, you can convert these dense layers to convolutional layers to create an FCN: just replace the lowest dense layer with a convolutional layer with a kernel size equal to the layerâ€™s input size, with one filter per neuron in the dense layer, and using \"valid\" padding. Generally the stride should be 1, but you can set it to a higher value if you want. The activation function should be the same as the dense layerâ€™s. The other dense layers should be converted the same way, but using 1 Ã— 1 filters. It is actually possible to convert a trained CNN this way by appropriately reshaping the dense layersâ€™ weight matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73feb3be",
   "metadata": {},
   "source": [
    "# 8. \n",
    "\n",
    "What is the main technical difficulty of semantic segmentation?\n",
    "\n",
    "# My solution \n",
    "\n",
    "Semantic segmentaion loses details of the original image so its classifications are general and it clumps similar items together.\n",
    "\n",
    "# Book Solution\n",
    "\n",
    "The main technical difficulty of semantic segmentation is the fact that a lot of the spatial information gets lost in a CNN as the signal flows through each layer, especially in pooling layers and layers with a stride greater than 1. This spatial information needs to be restored somehow to accurately predict the class of each pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3984e5",
   "metadata": {},
   "source": [
    "# 9. \n",
    "\n",
    "Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST.\n",
    "\n",
    "# My solution \n",
    "\n",
    "To do so I would simply copy an architecture from the book. Since I have terrible hardware, I will skip on this. \n",
    "\n",
    "\n",
    "\n",
    "# Book Solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
