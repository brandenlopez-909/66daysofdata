{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. \n",
    "\n",
    "If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?\n",
    "\n",
    "## My Solution \n",
    "\n",
    "Yes. Ensemble learning with weak or strong models generally produces better results than a single model. We can use a voting classifier to combine the models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. \n",
    "What is the difference between hard and soft voting classifiers?\n",
    "\n",
    "## My solution \n",
    "\n",
    "Hard voting take the classification from each model and uses the majority prediction as the end result. Soft voting is similar but takes in the probability the model used to come to that conclusion. The latter often leads to better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. \n",
    "\n",
    "Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, Random Forests, or stacking ensembles?\n",
    "\n",
    "## My Solution \n",
    "\n",
    "From what I call, bagging is out of core. Out of core algoritms can be ran across multiple servers.\n",
    "\n",
    "Pasting requires the algorithm to remove the training instances after selection, thus it cannot be out of core. Boosting requires us to train the models in a sequence, thus we cannot do it across multiple servers. \n",
    "\n",
    "Random forests use bagging on the many models it trains, since there is no need for the various models to communicate with one another during training, we should be able to distribute it over multiple servers. \n",
    "\n",
    "Stacking ensembles should able to ran across multiple servers. Each model is trained on the first inital set. Then they can communicate predicitions for the second training set. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. \n",
    "\n",
    "What is the benefit of out-of-bag evaluation?\n",
    "\n",
    "## My solution \n",
    "\n",
    "Does not need a validation(testing) set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. \n",
    "\n",
    "What makes Extra-Trees more random than regular Random Forests? How can this extra randomness help? Are Extra-Trees slower or faster than regular Random Forests?\n",
    "\n",
    "## My solution \n",
    "\n",
    "Extra-trees use a random threshold for the decision nodes, while random forests use some expensive algorithm like Gini. The randomness trades in some bias for a lower variance. Extra-trees are faster than random forests. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. \n",
    "\n",
    "If your AdaBoost ensemble underfits the training data, which hyperparameters should you tweak and how?\n",
    "\n",
    "## My solution \n",
    "\n",
    "If your AdaBoost ensemble is underfitting the training set, you can try increasing the number of estimators or reduce regularizing the base estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. \n",
    "\n",
    "If your Gradient Boosting ensemble overfits the training set, should you increase or decrease the learning rate?\n",
    "\n",
    "## My solution \n",
    "\n",
    "Decrease the learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. \n",
    "Load the MNIST data, and split it into a training set, a validation set, and a test set (e.g., use 50,000 instances for training, 10,000 for validation, and 10,000 for testing). Then train various classifiers, such as a Random Forest classifier, an Extra-Trees classifier, and an SVM classifier. Next, try to combine them into an ensemble that outperforms each individual classifier on the validation set, using soft or hard voting. Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fetches MNIST Dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The model training was taking too long. \n",
    "# I decreased the number of instances to speed this up \n",
    "X_train, X_val, X_test = X[:20000],X[20000:25000],X[25000:30000]\n",
    "y_train, y_val, y_test = y[:20000],y[20000:25000],y[25000:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('et',\n",
       "                              ExtraTreesClassifier(bootstrap=False,\n",
       "                                                   ccp_alpha=0.0,\n",
       "                                                   class_weight=None,\n",
       "                                                   criterion='gini',\n",
       "                                                   max_depth=None,\n",
       "                                                   max_features='auto',\n",
       "                                                   max_leaf_nodes=None,\n",
       "                                                   max_samples=None,\n",
       "                                                   min_impurity_decrease=0.0,\n",
       "                                                   min_impurity_split=None,\n",
       "                                                   min_samples_leaf=1,\n",
       "                                                   min_samples_split=2,\n",
       "                                                   min_weight_fraction_leaf=0.0,\n",
       "                                                   n_estimators=100,\n",
       "                                                   n_jobs=None, oob_score=...\n",
       "                                                     oob_score=False,\n",
       "                                                     random_state=None,\n",
       "                                                     verbose=0,\n",
       "                                                     warm_start=False)),\n",
       "                             ('svc',\n",
       "                              SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                                  class_weight=None, coef0=0.0,\n",
       "                                  decision_function_shape='ovr', degree=3,\n",
       "                                  gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                                  probability=False, random_state=None,\n",
       "                                  shrinking=True, tol=0.001, verbose=False))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='hard',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "rf_cl = RandomForestClassifier()\n",
    "SVC = SVC()\n",
    "et_cl = ExtraTreesClassifier()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('et', et_cl), ('rf', rf_cl), ('svc',SVC)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtraTreesClassifier 0.9558\n",
      "RandomForestClassifier 0.9516\n",
      "SVC 0.9654\n",
      "VotingClassifier 0.9578\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (et_cl, rf_cl,  SVC, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. \n",
    "\n",
    "Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions: each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the image’s class. Train a classifier on this new training set. Congratulations, you have just trained a blender, and together with the classifiers it forms a stacking ensemble! Now evaluate the ensemble on the test set. For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble’s predictions. How does it compare to the voting classifier you trained earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "estimators = [et_cl, rf_cl,  SVC, voting_clf]\n",
    "X_val_predictions = np.empty((len(X_val), len(estimators)), dtype=np.float32)\n",
    "\n",
    "\n",
    "for index, estimator in enumerate(estimators):\n",
    "    X_val_predictions[:, index] = estimator.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9672"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_forest_blender = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
    "rnd_forest_blender.fit(X_val_predictions, y_val)\n",
    "rnd_forest_blender.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_predictions = np.empty((len(X_test), len(estimators)), dtype=np.float32)\n",
    "\n",
    "for index, estimator in enumerate(estimators):\n",
    "    X_test_predictions[:, index] = estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rnd_forest_blender.predict(X_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
