{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31813ee9",
   "metadata": {},
   "source": [
    "# 1. \n",
    "Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "\n",
    "## My solution \n",
    "\n",
    "No, all weights having the same value will lead every gradient step being the same. This causes multiple neurons and layers to act as the same thing. Thus, the model will not be able to pick up on complex patterns and features. \n",
    "\n",
    "## Book solution \n",
    "\n",
    "No, all weights should be sampled independently; they should not all have the same initial value. One important goal of sampling weights randomly is to break symmetry: if all the weights have the same initial value, even if that value is not zero, then symmetry is not broken (i.e., all neurons in a given layer are equivalent), and backpropagation will be unable to break it. Concretely, this means that all the neurons in any given layer will always have the same weights. It’s like having just one neuron per layer, and much slower. It is virtually impossible for such a configuration to converge to a good solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadaf980",
   "metadata": {},
   "source": [
    "# 2. \n",
    "Is it OK to initialize the bias terms to 0?\n",
    "\n",
    "## My solution \n",
    "\n",
    "I didn't recall an in-depth exploration of Bias terms this chapter so I decided to search information. This [medium](https://medium.com/deeper-learning/glossary-of-deep-learning-bias-cf49d9c895e2) article does a great job at explaining the bias term interaction with an activation function. More so, it provides our answer to the question. Yes, typucall bias terms are initilized to zero and \"asymetric breaking\" is provided by the weights. \n",
    "\n",
    "\n",
    "## Book Solution \n",
    "\n",
    "It is perfectly fine to initialize the bias terms to zero. Some people like to initialize them just like weights, and that’s okay too; it does not make much difference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e540cd",
   "metadata": {},
   "source": [
    "# 3. \n",
    "\n",
    "Name three advantages of the SELU activation function over ReLU.\n",
    "\n",
    "## My solution \n",
    "\n",
    "SELU:\n",
    "\n",
    "1. allows our network to self normalize, fixing the exploding gradient problems.\n",
    "\n",
    "2. Allows our activation functions to take on negative values, fixing the vanishing gradient. (inherited from ELU)\n",
    "\n",
    "3. It has a non-zero gradient for inputs less than 0. Fixing the dead neuron problems. (inherited from ELU)\n",
    "\n",
    "## Book Solution \n",
    "\n",
    "A few advantages of the SELU function over the ReLU function are:\n",
    "\n",
    "1. It can take on negative values, so the average output of the neurons in any given layer is typically closer to zero than when using the ReLU activation function (which never outputs negative values). This helps alleviate the vanishing gradients problem.\n",
    "\n",
    "\n",
    "2. It always has a nonzero derivative, which avoids the dying units issue that can affect ReLU units.\n",
    "\n",
    "\n",
    "3. When the conditions are right (i.e., if the model is sequential, and the weights are initialized using LeCun initialization, and the inputs are standardized, and there’s no incompatible layer or regularization, such as dropout or ℓ1 regularization), then the SELU activation function ensures the model is self-normalized, which solves the exploding/vanishing gradients problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f716899e",
   "metadata": {},
   "source": [
    "# 4. \n",
    "\n",
    "In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "\n",
    "## My solution \n",
    "\n",
    "- Softmax is useful for classification problems where we want the sum of the outputs to equal one. \n",
    "\n",
    "- Logistic is another usful classification activation function. Used for simple binary classification problems.\n",
    "\n",
    "- Tanh, IDK when you would want to use it. It's rather primative\n",
    "\n",
    "- ReLU is useful fro classification and regression tasks. If is quick to work with but has an issue with dead/vanishing gradients. \n",
    "\n",
    "- Leaky ReLU: Is great if negative gradient are occuring. Gives a slope of values less than 0. Mitigating the dead/vanishing gradient problems. Is fairly quick. Used for classifications and regressions \n",
    "\n",
    "- SELU, is best with sequential models. Mitagates most know problems and outperforms all ReLU variations. Used for classifcations and regressions. \n",
    "\n",
    "## Book Solution \n",
    "\n",
    "The SELU activation function is a good default. If you need the neural network to be as fast as possible, you can use one of the leaky ReLU variants instead (e.g., a simple leaky ReLU using the default hyperparameter value). \n",
    "\n",
    "The simplicity of the ReLU activation function makes it many people’s preferred option, despite the fact that it is generally outperformed by SELU and leaky ReLU. However, the ReLU activation function’s ability to output precisely zero can be useful in some cases (e.g., see Chapter 17). Moreover, it can sometimes benefit from optimized implementation as well as from hardware acceleration. \n",
    "\n",
    "The hyperbolic tangent (tanh) can be useful in the output layer if you need to output a number between –1 and 1, but nowadays it is not used much in hidden layers (except in recurrent nets). The logistic activation function is also useful in the output layer when you need to estimate a probability (e.g., for binary classification), but is rarely used in hidden layers (there are exceptions—for example, for the coding layer of variational autoencoders; see Chapter 17). Finally, the softmax activation function is useful in the output layer to output probabilities for mutually exclusive classes, but it is rarely (if ever) used in hidden layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8b6eb0",
   "metadata": {},
   "source": [
    "# 5. \n",
    "What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?\n",
    "\n",
    "## My solution \n",
    "\n",
    "Using a higer momentum parameter decrease the \"friction\" of the learning rate. Thus it can go much quicker than non-momentum SGD. It can cause the solution to roam around the optimial solution for some time. \n",
    "\n",
    "## Book solution \n",
    "\n",
    "f you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer, then the algorithm will likely pick up a lot of speed, hopefully moving roughly toward the global minimum, but its momentum will carry it right past the minimum. Then it will slow down and come back, accelerate again, overshoot again, and so on. It may oscillate this way many times before converging, so overall it will take much longer to converge than with a smaller momentum value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dccab15",
   "metadata": {},
   "source": [
    "# 6.\n",
    "\n",
    "Name three ways you can produce a sparse model.\n",
    "\n",
    "## My solution \n",
    "\n",
    "By using strong l1 regularization, using dropout, or max-norm regularization. (got this one 2/3 wrong.)\n",
    "\n",
    "# Book solution \n",
    "\n",
    "One way to produce a sparse model (i.e., with most weights equal to zero) is to train the model normally, then zero out tiny weights. For more sparsity, you can apply ℓ1 regularization during training, which pushes the optimizer toward sparsity. A third option is to use the TensorFlow Model Optimization Toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8750282b",
   "metadata": {},
   "source": [
    "# 7. \n",
    "\n",
    "Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?\n",
    "\n",
    "## My solution\n",
    "\n",
    "Standard dropout does slow down training. At the end of training, it does not use a subset of neurons. Instead it uses the entire model trained from dropout, so no. It dropout does not slow down inference. \n",
    "\n",
    "MC dropout is performed after training a model. Thus, it does not slow down training. Since MC is an average of dropout predictions, it should infact slow predictions. (I was wrong, it is active during training.)\n",
    "\n",
    "## Book Solution \n",
    "\n",
    "Yes, dropout does slow down training, in general roughly by a factor of two. However, it has no impact on inference speed since it is only turned on during training. MC Dropout is exactly like dropout during training, but it is still active during inference, so each inference is slowed down slightly. More importantly, when using MC Dropout you generally want to run inference 10 times or more to get better predictions. This means that making predictions is slowed down by a factor of 10 or more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
