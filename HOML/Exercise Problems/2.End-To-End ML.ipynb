{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the data \n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "import pandas as pd\n",
    "#Where we download the data from\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "#Puts hosuing in the datasets folder\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "#\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "#Now when you call fetch_housing_data(), it creates a datasets/housing directory in your workspace,\n",
    "#downloads the housing.tgz file, and extracts the housing.csv file from it in this directory.\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    #Makes a new directory. \n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    # Join one or more path components intelligently. The return value is the concatenation of path \n",
    "    #and any members of *paths with exactly one directory separator following each non-empty part \n",
    "    #except the last, meaning that the result will only end in a separator if the last part is empty.\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "    \n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "fetch_housing_data()\n",
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x22634796f98>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we need to handle the income category \n",
    "#The following code uses the pd.cut()\n",
    "#function to create an income category attribute with five categories (labeled from 1 to 5):\n",
    "import numpy as np\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "housing[\"income_cat\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we make sure we stratify our data \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    0.350533\n",
       "2    0.318798\n",
       "4    0.176357\n",
       "5    0.114583\n",
       "1    0.039729\n",
       "Name: income_cat, dtype: float64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let’s see if this worked as expected. You can start by looking at the income category proportions in the test set:\n",
    "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)\n",
    "#Since this matches our graph we should be fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you should remove the income_cat attribute so the data is back to its original state:\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The attribute combinations used in the notes \n",
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#median house value is the response variable so we get rid of it\n",
    "# strat_train_set was declared when we stratified the data.\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "\n",
    "#Copy the dats the originals set is fine\n",
    "#More so, housing_labels is our response\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "              missing_values=nan, strategy='median', verbose=0)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Neatly takes care of missing values \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Since the median can only be computed on numerical attributes, \n",
    "# you need to create a copy of the data without the text attribute ocean_proximity:\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now you can use this “trained” imputer to transform \n",
    "#the training set by replacing missing values with the learned medians:\n",
    "X = imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to put it back into a pandas DataFrame, it’s simple:\n",
    "#create a new data fram with the information\n",
    "#.columns gets the columns of housing_num\n",
    "#.index gets the index\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
    "                          index=housing_num.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding attributes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17606</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18632</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14650</th>\n",
       "      <td>NEAR OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3555</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19480</th>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8879</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13685</th>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4937</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861</th>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ocean_proximity\n",
       "17606       <1H OCEAN\n",
       "18632       <1H OCEAN\n",
       "14650      NEAR OCEAN\n",
       "3230           INLAND\n",
       "3555        <1H OCEAN\n",
       "19480          INLAND\n",
       "8879        <1H OCEAN\n",
       "13685          INLAND\n",
       "4937        <1H OCEAN\n",
       "4861        <1H OCEAN"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can add extra categories from ordinal (categorical) data\n",
    "#First we grab only the categroies as a DF\n",
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "housing_cat.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [4.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ordinal encoder translates categorical variables \n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded[:10]\n",
    "#Ordinal encoder went ahead and mapped the categories to some number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<16512x5 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 16512 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Likewise we can chose to use a binary method. This will make len(DF[\"category_column\"].values) new columns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transformers \n",
    "\n",
    "In the book we used a custom transformer to add a few attributes that might enhance our modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17606</th>\n",
       "      <td>-121.89</td>\n",
       "      <td>37.29</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1568.0</td>\n",
       "      <td>351.0</td>\n",
       "      <td>710.0</td>\n",
       "      <td>339.0</td>\n",
       "      <td>2.7042</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18632</th>\n",
       "      <td>-121.93</td>\n",
       "      <td>37.05</td>\n",
       "      <td>14.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>6.4214</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14650</th>\n",
       "      <td>-117.20</td>\n",
       "      <td>32.77</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1952.0</td>\n",
       "      <td>471.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>462.0</td>\n",
       "      <td>2.8621</td>\n",
       "      <td>NEAR OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>-119.61</td>\n",
       "      <td>36.31</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1847.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>1460.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>1.8839</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3555</th>\n",
       "      <td>-118.59</td>\n",
       "      <td>34.23</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6592.0</td>\n",
       "      <td>1525.0</td>\n",
       "      <td>4459.0</td>\n",
       "      <td>1463.0</td>\n",
       "      <td>3.0347</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6563</th>\n",
       "      <td>-118.13</td>\n",
       "      <td>34.20</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1271.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>573.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>4.9312</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12053</th>\n",
       "      <td>-117.56</td>\n",
       "      <td>33.88</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1196.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>1052.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>2.0682</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13908</th>\n",
       "      <td>-116.40</td>\n",
       "      <td>34.09</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4855.0</td>\n",
       "      <td>872.0</td>\n",
       "      <td>2098.0</td>\n",
       "      <td>765.0</td>\n",
       "      <td>3.2723</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11159</th>\n",
       "      <td>-118.01</td>\n",
       "      <td>33.82</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>1356.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>4.0625</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15775</th>\n",
       "      <td>-122.45</td>\n",
       "      <td>37.77</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3095.0</td>\n",
       "      <td>682.0</td>\n",
       "      <td>1269.0</td>\n",
       "      <td>639.0</td>\n",
       "      <td>3.5750</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16512 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "17606    -121.89     37.29                38.0       1568.0           351.0   \n",
       "18632    -121.93     37.05                14.0        679.0           108.0   \n",
       "14650    -117.20     32.77                31.0       1952.0           471.0   \n",
       "3230     -119.61     36.31                25.0       1847.0           371.0   \n",
       "3555     -118.59     34.23                17.0       6592.0          1525.0   \n",
       "...          ...       ...                 ...          ...             ...   \n",
       "6563     -118.13     34.20                46.0       1271.0           236.0   \n",
       "12053    -117.56     33.88                40.0       1196.0           294.0   \n",
       "13908    -116.40     34.09                 9.0       4855.0           872.0   \n",
       "11159    -118.01     33.82                31.0       1960.0           380.0   \n",
       "15775    -122.45     37.77                52.0       3095.0           682.0   \n",
       "\n",
       "       population  households  median_income ocean_proximity  \n",
       "17606       710.0       339.0         2.7042       <1H OCEAN  \n",
       "18632       306.0       113.0         6.4214       <1H OCEAN  \n",
       "14650       936.0       462.0         2.8621      NEAR OCEAN  \n",
       "3230       1460.0       353.0         1.8839          INLAND  \n",
       "3555       4459.0      1463.0         3.0347       <1H OCEAN  \n",
       "...           ...         ...            ...             ...  \n",
       "6563        573.0       210.0         4.9312          INLAND  \n",
       "12053      1052.0       258.0         2.0682          INLAND  \n",
       "13908      2098.0       765.0         3.2723          INLAND  \n",
       "11159      1356.0       356.0         4.0625       <1H OCEAN  \n",
       "15775      1269.0       639.0         3.5750        NEAR BAY  \n",
       "\n",
       "[16512 rows x 9 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In python is a class both the class and ctr. \n",
    "#How do we overload this in Python?\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6 #Saves th index location of certain columns in the housing DF\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    #Would we overload the constructor here?\n",
    "    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room # is a boolean \n",
    "    def fit(self, X, y=None):\n",
    "        return self  #Since we are only transforming, we do not need to fill in the fit method\n",
    "    def transform(self, X):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix] #Gets all rows of the column_index\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room: #if true\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room] # numpy.c_ :Translates slice objects to concatenation along the second axis. \n",
    "\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "Rather than do everystep in individual boxes, we can use a pipeline for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        #First we use the imputer to get rid of NAns\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        #Next we use the custom transformer to enchance the data set. \n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        #Next we scale the data as some algorithms do not work well when \n",
    "        #the input numerical attributes have very different scales\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "#Pipelines make it easy to implement the code. Of course the above code was to develop this. \n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also pipeline a pipeline. \n",
    "#Since the num_pipeline only handles numerical values, we need to handle the categorical variables from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#Sklearn has a module to pipeline all columns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#When working with this pipeline we need the column names\n",
    "\n",
    "#This is a convient way to get the columns.\n",
    "#Housing_num contains to categorical columns\n",
    "num_attribs = list(housing_num)\n",
    "\n",
    "#The only categorical column or orginal DF should have\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "            #Name the pipeline, the method/transformer/pipeline to use, what to use it on\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "#Apply it ot the housing dataset \n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale',\n",
       "    kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR(kernel=\"linear\")\n",
    "#housing_prepared is the, cleaned and enchanced DF.\n",
    "#housing_labels is our response column\n",
    "svr.fit(housing_prepared, housing_labels)\n",
    "#This trains the model which can make predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [175421.01071251 190268.74555128 175653.26689489 165556.9789596\n",
      " 183911.42788418]\n",
      "Labels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\n"
     ]
    }
   ],
   "source": [
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "print(\"Predictions:\", svr.predict(some_data_prepared))\n",
    "print(\"Labels:\", list(some_labels))\n",
    "#This is terribly off to begin with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111094.6308539982"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Measuring the error for the entire set\n",
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = svr.predict(housing_prepared)\n",
    "svr_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "svr_rmse = np.sqrt(svr_mse)\n",
    "svr_rmse\n",
    "#Significantly off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118580.68301157995"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rbf is the default\n",
    "svr = SVR(kernel=\"rbf\")\n",
    "#housing_prepared is the, cleaned and enchanced DF.\n",
    "#housing_labels is our response column\n",
    "svr.fit(housing_prepared, housing_labels)\n",
    "#This trains the model which can make predictions \n",
    "housing_predictions = svr.predict(housing_prepared)\n",
    "svr_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "svr_rmse = np.sqrt(svr_mse)\n",
    "svr_rmse\n",
    "#Significantly off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118577.43356412371"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Messing with the gamma\n",
    "\n",
    "\n",
    "#rbf is the default\n",
    "svr = SVR(kernel=\"rbf\", gamma='auto')\n",
    "#housing_prepared is the, cleaned and enchanced DF.\n",
    "#housing_labels is our response column\n",
    "svr.fit(housing_prepared, housing_labels)\n",
    "#This trains the model which can make predictions \n",
    "housing_predictions = svr.predict(housing_prepared)\n",
    "svr_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "svr_rmse = np.sqrt(svr_mse)\n",
    "svr_rmse\n",
    "#Significantly off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118226.98577896513"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Messing with the C\n",
    "\n",
    "\n",
    "#rbf is the default\n",
    "#Scale is defualt\n",
    "# C =1 is defauly]t\n",
    "svr = SVR(kernel=\"rbf\", gamma='scale', C = 2.0)\n",
    "#housing_prepared is the, cleaned and enchanced DF.\n",
    "#housing_labels is our response column\n",
    "svr.fit(housing_prepared, housing_labels)\n",
    "#This trains the model which can make predictions \n",
    "housing_predictions = svr.predict(housing_prepared)\n",
    "svr_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "svr_rmse = np.sqrt(svr_mse)\n",
    "svr_rmse\n",
    "#Significantly off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "[CV] C=10.0, kernel=linear ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ C=10.0, kernel=linear, total=   8.8s\n",
      "[CV] C=10.0, kernel=linear ...........................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ............................ C=10.0, kernel=linear, total=   8.1s\n",
      "[CV] C=10.0, kernel=linear ...........................................\n",
      "[CV] ............................ C=10.0, kernel=linear, total=   7.8s\n",
      "[CV] C=10.0, kernel=linear ...........................................\n",
      "[CV] ............................ C=10.0, kernel=linear, total=   9.0s\n",
      "[CV] C=10.0, kernel=linear ...........................................\n",
      "[CV] ............................ C=10.0, kernel=linear, total=  10.2s\n",
      "[CV] C=30.0, kernel=linear ...........................................\n",
      "[CV] ............................ C=30.0, kernel=linear, total=   8.8s\n",
      "[CV] C=30.0, kernel=linear ...........................................\n",
      "[CV] ............................ C=30.0, kernel=linear, total=   9.9s\n",
      "[CV] C=30.0, kernel=linear ...........................................\n",
      "[CV] ............................ C=30.0, kernel=linear, total=   9.3s\n",
      "[CV] C=30.0, kernel=linear ...........................................\n",
      "[CV] ............................ C=30.0, kernel=linear, total=  10.4s\n",
      "[CV] C=30.0, kernel=linear ...........................................\n",
      "[CV] ............................ C=30.0, kernel=linear, total=  10.0s\n",
      "[CV] C=100.0, kernel=linear ..........................................\n",
      "[CV] ........................... C=100.0, kernel=linear, total=   9.6s\n",
      "[CV] C=100.0, kernel=linear ..........................................\n",
      "[CV] ........................... C=100.0, kernel=linear, total=  10.1s\n",
      "[CV] C=100.0, kernel=linear ..........................................\n",
      "[CV] ........................... C=100.0, kernel=linear, total=   9.6s\n",
      "[CV] C=100.0, kernel=linear ..........................................\n",
      "[CV] ........................... C=100.0, kernel=linear, total=  10.1s\n",
      "[CV] C=100.0, kernel=linear ..........................................\n",
      "[CV] ........................... C=100.0, kernel=linear, total=  10.1s\n",
      "[CV] C=300.0, kernel=linear ..........................................\n",
      "[CV] ........................... C=300.0, kernel=linear, total=   8.3s\n",
      "[CV] C=300.0, kernel=linear ..........................................\n",
      "[CV] ........................... C=300.0, kernel=linear, total=   8.8s\n",
      "[CV] C=300.0, kernel=linear ..........................................\n",
      "[CV] ........................... C=300.0, kernel=linear, total=   8.8s\n",
      "[CV] C=300.0, kernel=linear ..........................................\n",
      "[CV] ........................... C=300.0, kernel=linear, total=   9.3s\n",
      "[CV] C=300.0, kernel=linear ..........................................\n",
      "[CV] ........................... C=300.0, kernel=linear, total=   8.7s\n",
      "[CV] C=1000.0, kernel=linear .........................................\n",
      "[CV] .......................... C=1000.0, kernel=linear, total=  10.4s\n",
      "[CV] C=1000.0, kernel=linear .........................................\n",
      "[CV] .......................... C=1000.0, kernel=linear, total=  10.8s\n",
      "[CV] C=1000.0, kernel=linear .........................................\n",
      "[CV] .......................... C=1000.0, kernel=linear, total=   8.8s\n",
      "[CV] C=1000.0, kernel=linear .........................................\n",
      "[CV] .......................... C=1000.0, kernel=linear, total=   9.7s\n",
      "[CV] C=1000.0, kernel=linear .........................................\n",
      "[CV] .......................... C=1000.0, kernel=linear, total=   8.5s\n",
      "[CV] C=3000.0, kernel=linear .........................................\n",
      "[CV] .......................... C=3000.0, kernel=linear, total=   9.0s\n",
      "[CV] C=3000.0, kernel=linear .........................................\n",
      "[CV] .......................... C=3000.0, kernel=linear, total=   9.5s\n",
      "[CV] C=3000.0, kernel=linear .........................................\n",
      "[CV] .......................... C=3000.0, kernel=linear, total=  11.1s\n",
      "[CV] C=3000.0, kernel=linear .........................................\n",
      "[CV] .......................... C=3000.0, kernel=linear, total=  10.5s\n",
      "[CV] C=3000.0, kernel=linear .........................................\n",
      "[CV] .......................... C=3000.0, kernel=linear, total=   8.6s\n",
      "[CV] C=10000.0, kernel=linear ........................................\n",
      "[CV] ......................... C=10000.0, kernel=linear, total=  11.7s\n",
      "[CV] C=10000.0, kernel=linear ........................................\n",
      "[CV] ......................... C=10000.0, kernel=linear, total=  12.0s\n",
      "[CV] C=10000.0, kernel=linear ........................................\n",
      "[CV] ......................... C=10000.0, kernel=linear, total=  12.6s\n",
      "[CV] C=10000.0, kernel=linear ........................................\n",
      "[CV] ......................... C=10000.0, kernel=linear, total=  18.2s\n",
      "[CV] C=10000.0, kernel=linear ........................................\n",
      "[CV] ......................... C=10000.0, kernel=linear, total=  14.2s\n",
      "[CV] C=30000.0, kernel=linear ........................................\n",
      "[CV] ......................... C=30000.0, kernel=linear, total=  19.8s\n",
      "[CV] C=30000.0, kernel=linear ........................................\n",
      "[CV] ......................... C=30000.0, kernel=linear, total=  22.4s\n",
      "[CV] C=30000.0, kernel=linear ........................................\n",
      "[CV] ......................... C=30000.0, kernel=linear, total=  21.1s\n",
      "[CV] C=30000.0, kernel=linear ........................................\n",
      "[CV] ......................... C=30000.0, kernel=linear, total=  19.6s\n",
      "[CV] C=30000.0, kernel=linear ........................................\n",
      "[CV] ......................... C=30000.0, kernel=linear, total=  18.1s\n",
      "[CV] C=1.0, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] .................... C=1.0, gamma=0.01, kernel=rbf, total=  13.7s\n",
      "[CV] C=1.0, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] .................... C=1.0, gamma=0.01, kernel=rbf, total=  15.7s\n",
      "[CV] C=1.0, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] .................... C=1.0, gamma=0.01, kernel=rbf, total=  14.6s\n",
      "[CV] C=1.0, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] .................... C=1.0, gamma=0.01, kernel=rbf, total=  15.6s\n",
      "[CV] C=1.0, gamma=0.01, kernel=rbf ...................................\n",
      "[CV] .................... C=1.0, gamma=0.01, kernel=rbf, total=  15.8s\n",
      "[CV] C=1.0, gamma=0.03, kernel=rbf ...................................\n",
      "[CV] .................... C=1.0, gamma=0.03, kernel=rbf, total=  13.7s\n",
      "[CV] C=1.0, gamma=0.03, kernel=rbf ...................................\n",
      "[CV] .................... C=1.0, gamma=0.03, kernel=rbf, total=  14.8s\n",
      "[CV] C=1.0, gamma=0.03, kernel=rbf ...................................\n",
      "[CV] .................... C=1.0, gamma=0.03, kernel=rbf, total=  15.9s\n",
      "[CV] C=1.0, gamma=0.03, kernel=rbf ...................................\n",
      "[CV] .................... C=1.0, gamma=0.03, kernel=rbf, total=  15.0s\n",
      "[CV] C=1.0, gamma=0.03, kernel=rbf ...................................\n",
      "[CV] .................... C=1.0, gamma=0.03, kernel=rbf, total=  17.6s\n",
      "[CV] C=1.0, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ..................... C=1.0, gamma=0.1, kernel=rbf, total=  15.0s\n",
      "[CV] C=1.0, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ..................... C=1.0, gamma=0.1, kernel=rbf, total=  14.5s\n",
      "[CV] C=1.0, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ..................... C=1.0, gamma=0.1, kernel=rbf, total=  14.6s\n",
      "[CV] C=1.0, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ..................... C=1.0, gamma=0.1, kernel=rbf, total=  16.5s\n",
      "[CV] C=1.0, gamma=0.1, kernel=rbf ....................................\n",
      "[CV] ..................... C=1.0, gamma=0.1, kernel=rbf, total=  15.7s\n",
      "[CV] C=1.0, gamma=0.3, kernel=rbf ....................................\n",
      "[CV] ..................... C=1.0, gamma=0.3, kernel=rbf, total=  12.7s\n",
      "[CV] C=1.0, gamma=0.3, kernel=rbf ....................................\n",
      "[CV] ..................... C=1.0, gamma=0.3, kernel=rbf, total=  17.2s\n",
      "[CV] C=1.0, gamma=0.3, kernel=rbf ....................................\n",
      "[CV] ..................... C=1.0, gamma=0.3, kernel=rbf, total=  17.8s\n",
      "[CV] C=1.0, gamma=0.3, kernel=rbf ....................................\n",
      "[CV] ..................... C=1.0, gamma=0.3, kernel=rbf, total=  14.5s\n",
      "[CV] C=1.0, gamma=0.3, kernel=rbf ....................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................... C=1.0, gamma=0.3, kernel=rbf, total=  15.0s\n",
      "[CV] C=1.0, gamma=1.0, kernel=rbf ....................................\n",
      "[CV] ..................... C=1.0, gamma=1.0, kernel=rbf, total=  14.3s\n",
      "[CV] C=1.0, gamma=1.0, kernel=rbf ....................................\n",
      "[CV] ..................... C=1.0, gamma=1.0, kernel=rbf, total=  13.6s\n",
      "[CV] C=1.0, gamma=1.0, kernel=rbf ....................................\n",
      "[CV] ..................... C=1.0, gamma=1.0, kernel=rbf, total=  13.3s\n",
      "[CV] C=1.0, gamma=1.0, kernel=rbf ....................................\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-06fa27972580>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0msvm_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvm_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'neg_mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhousing_prepared\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhousing_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    708\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1151\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    687\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 689\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    833\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    256\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Solution from book\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "        {'kernel': ['linear'], 'C': [10., 30., 100., 300., 1000., 3000., 10000., 30000.0]},\n",
    "        {'kernel': ['rbf'], 'C': [1.0, 3.0, 10., 30., 100., 300., 1000.0],\n",
    "         'gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},\n",
    "    ]\n",
    "\n",
    "svm_reg = SVR()\n",
    "grid_search = GridSearchCV(svm_reg, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2)\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is much easier with grid search\n",
    "negative_mse = grid_search.best_score_\n",
    "rmse = np.sqrt(-negative_mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernal =Linear worked best. We cannot mess with gamma in the linear case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 Try replacing GridSearchCV with RandomizedSearchCV.\n",
    "\n",
    "For trying different combinations of hyper parameters, all you need to do is tell it which hyperparameters you want it to experiment with and what values to try out, and it will use cross-validation to evaluate all the possible combinations of hyperparameter values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18660.507950144554"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First they tried to use another model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "housing_predictions = forest_reg.predict(housing_prepared)\n",
    "forest_reg_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "forest_reg_rmse = np.sqrt(forest_reg_mse)\n",
    "forest_reg_rmse\n",
    "#Much better than an SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=RandomForestRegressor(bootstrap=True, ccp_alpha=0.0,\n",
       "                                             criterion='mse', max_depth=None,\n",
       "                                             max_features='auto',\n",
       "                                             max_leaf_nodes=None,\n",
       "                                             max_samples=None,\n",
       "                                             min_impurity_decrease=0.0,\n",
       "                                             min_impurity_split=None,\n",
       "                                             min_samples_leaf=1,\n",
       "                                             min_samples_split=2,\n",
       "                                             min_weight_fraction_leaf=0.0,\n",
       "                                             n_estimators=100, n_jobs=None,\n",
       "                                             oob_score=False, random_state=None,\n",
       "                                             verbose=0, warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid=[{'max_features': [2, 4, 6, 8],\n",
       "                          'n_estimators': [3, 10, 30]},\n",
       "                         {'bootstrap': [False], 'max_features': [2, 3, 4],\n",
       "                          'n_estimators': [3, 10]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=8, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=30, n_jobs=None, oob_score=False,\n",
       "                      random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Then we can get the best model\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64393.898418595985 {'max_features': 2, 'n_estimators': 3}\n",
      "55544.30776869036 {'max_features': 2, 'n_estimators': 10}\n",
      "52926.884471275574 {'max_features': 2, 'n_estimators': 30}\n",
      "59385.984636021516 {'max_features': 4, 'n_estimators': 3}\n",
      "53181.674170222745 {'max_features': 4, 'n_estimators': 10}\n",
      "50711.46957844601 {'max_features': 4, 'n_estimators': 30}\n",
      "58337.90570720766 {'max_features': 6, 'n_estimators': 3}\n",
      "52388.4993668803 {'max_features': 6, 'n_estimators': 10}\n",
      "50238.539274394854 {'max_features': 6, 'n_estimators': 30}\n",
      "58909.88347281292 {'max_features': 8, 'n_estimators': 3}\n",
      "51866.484392218095 {'max_features': 8, 'n_estimators': 10}\n",
      "50038.526261904124 {'max_features': 8, 'n_estimators': 30}\n",
      "62653.561155716285 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n",
      "54363.97557758518 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n",
      "61072.181118138 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n",
      "52741.61345900205 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n",
      "58369.99726215795 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n",
      "51850.66225910162 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "#Next we need to see the RMSE for the models \n",
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score=nan,\n",
       "                   estimator=RandomForestRegressor(bootstrap=True,\n",
       "                                                   ccp_alpha=0.0,\n",
       "                                                   criterion='mse',\n",
       "                                                   max_depth=None,\n",
       "                                                   max_features='auto',\n",
       "                                                   max_leaf_nodes=None,\n",
       "                                                   max_samples=None,\n",
       "                                                   min_impurity_decrease=0.0,\n",
       "                                                   min_impurity_split=None,\n",
       "                                                   min_samples_leaf=1,\n",
       "                                                   min_samples_split=2,\n",
       "                                                   min_weight_fraction_leaf=0.0,\n",
       "                                                   n_estimators=100,\n",
       "                                                   n_jobs=None, oob_score=False,\n",
       "                                                   random_state=None, verbose=0,\n",
       "                                                   warm_start=False),\n",
       "                   iid='deprecated', n_iter=10, n_jobs=None,\n",
       "                   param_distributions=[{'max_features': [2, 4, 6, 8],\n",
       "                                         'n_estimators': [3, 10, 30]},\n",
       "                                        {'bootstrap': [False],\n",
       "                                         'max_features': [2, 3, 4],\n",
       "                                         'n_estimators': [3, 10]}],\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=True, scoring='neg_mean_squared_error',\n",
       "                   verbose=0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "random_search = RandomizedSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "\n",
    "random_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                      max_depth=None, max_features=8, max_leaf_nodes=None,\n",
       "                      max_samples=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      n_estimators=30, n_jobs=None, oob_score=False,\n",
       "                      random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 30, 'max_features': 8}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52446.363555246826 {'n_estimators': 10, 'max_features': 8}\n",
      "50255.752209280734 {'n_estimators': 30, 'max_features': 6}\n",
      "50111.06131725529 {'n_estimators': 30, 'max_features': 8}\n",
      "59052.461808108725 {'n_estimators': 3, 'max_features': 4, 'bootstrap': False}\n",
      "52477.975204757044 {'n_estimators': 10, 'max_features': 4, 'bootstrap': False}\n",
      "53020.33475491836 {'n_estimators': 30, 'max_features': 2}\n",
      "51504.72324927583 {'n_estimators': 10, 'max_features': 6}\n",
      "58793.20439649452 {'n_estimators': 3, 'max_features': 8}\n",
      "55702.30177306523 {'n_estimators': 10, 'max_features': 2}\n",
      "50909.97654544978 {'n_estimators': 30, 'max_features': 4}\n"
     ]
    }
   ],
   "source": [
    "#Next we need to see the RMSE for the models \n",
    "rmses = random_search.cv_results_\n",
    "for mean_score, params in zip(rmses[\"mean_test_score\"], rmses[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)\n",
    "#It managed to find a better model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3528826866529892, 'median_income'),\n",
       " (0.16222607925956448, 'INLAND'),\n",
       " (0.11412153068915319, 'pop_per_hhold'),\n",
       " (0.07045794544674386, 'longitude'),\n",
       " (0.06801698424450403, 'latitude'),\n",
       " (0.06426612178491156, 'bedrooms_per_room'),\n",
       " (0.057341764178743385, 'rooms_per_hhold'),\n",
       " (0.04142214828149238, 'housing_median_age'),\n",
       " (0.014961186886030172, 'total_bedrooms'),\n",
       " (0.014568307434873608, 'population'),\n",
       " (0.014473591492776324, 'total_rooms'),\n",
       " (0.01413325121571032, 'households'),\n",
       " (0.0061086586717591755, '<1H OCEAN'),\n",
       " (0.002755878185539272, 'NEAR OCEAN'),\n",
       " (0.00213924054095453, 'NEAR BAY'),\n",
       " (0.00012462503425450996, 'ISLAND')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since we have our best models we can look at features that are important \n",
    "\n",
    "#Grid_search \n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.35329948315106624, 'median_income'),\n",
       " (0.16903538612487937, 'INLAND'),\n",
       " (0.11435858611448066, 'pop_per_hhold'),\n",
       " (0.06962348283488266, 'bedrooms_per_room'),\n",
       " (0.06674186876635067, 'longitude'),\n",
       " (0.06443232854782972, 'latitude'),\n",
       " (0.05213680132899528, 'rooms_per_hhold'),\n",
       " (0.041287177414318164, 'housing_median_age'),\n",
       " (0.014924176056648192, 'population'),\n",
       " (0.01488079666825658, 'total_rooms'),\n",
       " (0.014563895999280754, 'households'),\n",
       " (0.01456301139087125, 'total_bedrooms'),\n",
       " (0.004780827247313626, '<1H OCEAN'),\n",
       " (0.0035316495260256783, 'NEAR OCEAN'),\n",
       " (0.0017494045716299452, 'NEAR BAY'),\n",
       " (9.112425717113587e-05, 'ISLAND')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since we have our best models we can look at features that are important \n",
    "\n",
    "#random_search \n",
    "feature_importances = random_search.best_estimator_.feature_importances_\n",
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_importances, attributes), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Github Solution\n",
    "#Do not run it will take a long assss time\n",
    "'''\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import expon, reciprocal\n",
    "\n",
    "# see https://docs.scipy.org/doc/scipy/reference/stats.html\n",
    "# for `expon()` and `reciprocal()` documentation and more probability distribution functions.\n",
    "\n",
    "# Note: gamma is ignored when kernel is \"linear\"\n",
    "param_distribs = {\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'C': reciprocal(20, 200000),\n",
    "        'gamma': expon(scale=1.0),\n",
    "    }\n",
    "\n",
    "svm_reg = SVR()\n",
    "rnd_search = RandomizedSearchCV(svm_reg, param_distributions=param_distribs,\n",
    "                                n_iter=50, cv=5, scoring='neg_mean_squared_error',\n",
    "                                verbose=2, random_state=42)\n",
    "rnd_search.fit(housing_prepared, housing_labels)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importance tells us that only one categorical predictor really matters. That is Inland, we can drop the other categoricals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 Try adding a transformer in the preparation pipeline to select only the most important attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-121.89, -121.93, -117.2, ..., -116.4, -118.01, -122.45],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the values method allows us to easily grab certain columns and their values\n",
    "housing.values[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
       "       'total_bedrooms', 'population', 'households', 'median_income',\n",
       "       'ocean_proximity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.columns\n",
    "#starts with the natural numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[0] allows us to manpulate the array\n",
    "cat_encoder.categories_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<1H OCEAN', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'], dtype=object)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This does the trick\n",
    "np.delete(cat_encoder.categories_[0], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<1H OCEAN', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This does the trick\n",
    "list(np.delete(cat_encoder.categories_[0], 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#My attempt didn't work. Past this is the github solution\n",
    "\n",
    "#We need the most important features. \n",
    "#Idea have it do thru the zip above if the value is less than 4, we remove it from the data set\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6 #Saves th index location of certain columns in the housing DF\n",
    "class removeAttributes(BaseEstimator, TransformerMixin):\n",
    "    #Would we overload the constructor here?\n",
    "    def __init__(self, good_attributes=True): # no *args or **kargs\n",
    "        self.good_attributes = good_attributes # is a boolean \n",
    "    def fit(self, X, y=None):\n",
    "        return self  #Since we are only transforming, we do not need to fill in the fit method\n",
    "    def transform(self, X):\n",
    "        if(self.good_attributes):\n",
    "            to_drop = list(np.delete(cat_encoder.categories_[0], 1, 0))\n",
    "            #I think X is an numpy array so this will not work \n",
    "            return X.drop(to_drop, axis =1)\n",
    "\n",
    "\n",
    "#First we make a pipeline that will do all the prior steps and drop a few columns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    \n",
    "    \n",
    "        #We use the imputer to get rid of NAns   \n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        #Next we use the custom transformer to enchance the data set. \n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        #Next we scale the data as some algorithms do not work well when \n",
    "        #the input numerical attributes have very different scales\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "#Pipelines make it easy to implement the code. Of course the above code was to develop this. \n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we go back to Pipelines \n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "#what I wanted to do was remove any value less that 4. \n",
    "#He selects the first k feautres\n",
    "def indices_of_top_k(arr, k):\n",
    "    #Perform an indirect partition along the given axis using the algorithm specified by the kind keyword.\n",
    "    #It returns an array of indices of the same shape as a that index data along the given axis in partitioned order.\n",
    "    #np.array(arr) is the thing we sort. \n",
    "    #Anything after k is what we get rid of. \n",
    "    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n",
    "\n",
    "class TopFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_importances, k):\n",
    "        self.feature_importances = feature_importances\n",
    "        self.k = k\n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[:, self.feature_indices_]\n",
    "#Note: this feature selector assumes that you have already computed the feature importances\n",
    "# You may be tempted to compute them directly in the TopFeatureSelector's fit() method,\n",
    "#however this would likely slow down grid/randomized search since the feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  7,  9, 10, 12], dtype=int64)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Speficify the steps we want to keep\n",
    "k = 5\n",
    "#feature_importances = grid_search.best_estimator_.feature_importances_ \n",
    "#It was called before the zip to display model results\n",
    "#Sorts on a numerical value\n",
    "top_k_feature_indices = indices_of_top_k(feature_importances, k)\n",
    "top_k_feature_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['longitude', 'median_income', 'pop_per_hhold', 'bedrooms_per_room',\n",
       "       'INLAND'], dtype='<U18')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#attributes is defined in the model result section\n",
    "#attributes was sorted on the top indices \n",
    "np.array(attributes)[top_k_feature_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.35329948315106624, 'median_income'),\n",
       " (0.16903538612487937, 'INLAND'),\n",
       " (0.11435858611448066, 'pop_per_hhold'),\n",
       " (0.06962348283488266, 'bedrooms_per_room'),\n",
       " (0.06674186876635067, 'longitude')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sorted(zip(feature_importances, attributes), reverse=True)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we can manipulte the materials lets do the pipeline\n",
    "preparation_and_feature_selection_pipeline = Pipeline([\n",
    "    #The full pipeline cleans, adds and more.\n",
    "    ('preparation', full_pipeline),\n",
    "    #We just need to keep the \"good\" features\n",
    "    ('feature_selection', TopFeatureSelector(feature_importances, k))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If there is an issue with this step go back to the notes and re-run the pipelines\n",
    "#I had an issue and it was due to me running what I tried for this exercise\n",
    "housing_prepared_top_k_features = preparation_and_feature_selection_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.15604281, -0.61493744, -0.08649871,  0.15531753,  0.        ],\n",
       "       [-1.17602483,  1.33645936, -0.03353391, -0.83628902,  0.        ],\n",
       "       [ 1.18684903, -0.5320456 , -0.09240499,  0.4222004 ,  0.        ]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_prepared_top_k_features[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.15604281, -0.61493744, -0.08649871,  0.15531753,  0.        ],\n",
       "       [-1.17602483,  1.33645936, -0.03353391, -0.83628902,  0.        ],\n",
       "       [ 1.18684903, -0.5320456 , -0.09240499,  0.4222004 ,  0.        ]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "housing_prepared[0:3, top_k_feature_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Try creating a single pipeline that does the full data preparation plus the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-139-834ba155239d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[1;34m'feature_selection'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTopFeatureSelector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_importances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#('svm_reg', SVR(**rnd_search.best_params_)) #He ran his with th random_search. We will not as it took too long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[1;34m'svm_reg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSVR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m ])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "prepare_select_and_predict_pipeline = Pipeline([\n",
    "    ('preparation', full_pipeline),\n",
    "    ('feature_selection', TopFeatureSelector(feature_importances, k)),\n",
    "    #('svm_reg', SVR(**rnd_search.best_params_)) #He ran his with th random_search. We will not as it took too long\n",
    "    ('svm_reg', SVR(**grid_search.best_params_)) #Our take forever too! This would word but we didn't finish out method.\n",
    "])\n",
    "\n",
    "\n",
    "prepare_select_and_predict_pipeline.fit(housing, housing_labels)\n",
    "\n",
    "some_data = housing.iloc[:4]\n",
    "some_labels = housing_labels.iloc[:4]\n",
    "\n",
    "print(\"Predictions:\\t\", prepare_select_and_predict_pipeline.predict(some_data))\n",
    "print(\"Labels:\\t\\t\", list(some_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty much it, it would lower the result significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Automatically explore some preparation options using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{\n",
    "    'preparation__num__imputer__strategy': ['mean', 'median', 'most_frequent'],\n",
    "    'feature_selection__k': list(range(1, len(feature_importances) + 1))\n",
    "}]\n",
    "\n",
    "grid_search_prep = GridSearchCV(prepare_select_and_predict_pipeline, param_grid, cv=5,\n",
    "                                scoring='neg_mean_squared_error', verbose=2)\n",
    "grid_search_prep.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid_search_prep.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
