{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The brain’s architecture was inspiration on how to build an intelligent machine. This is what sparked artificial neural networks (ANNs)\n",
    "\n",
    "- **ANN**: A Machine Learning model inspired by the networks of biological neurons found in our brains.\n",
    "\n",
    "ANNs are at the very core of Deep Learning. \n",
    "\n",
    "They are used for Googles image recognization, apple's siri, recomendation systems, or learning games. \n",
    "\n",
    "# From Biological to Artificial Neurons\n",
    "\n",
    "- **Connectionism** :the study of neural network\n",
    "\n",
    "- ANNs frequently outperform other ML techniques on very large and complex problems.\n",
    "\n",
    "- Increases in computing power have made training possible. \n",
    "- Anns rarely get stuck at a local optima\n",
    "\n",
    "# Biological Neurons\n",
    "\n",
    "- **Cell body**: containing the nucleus and most of the cell’s complex components\n",
    "- **Dendrites**: branching extensions\n",
    "- **Axon**: One very long extension called of the cell body\n",
    "- **Telodendria**:  The axon splits off into many branches near its extremity. \n",
    "More bio stuff, don't have to remember? \n",
    "\n",
    "Individual biological neurons seem to behave in a rather simple way, but they are organized in a vast network of billions.\n",
    "\n",
    "# Logical Computations with Neurons\n",
    "\n",
    "- **Artificial neuron**: it has one or more binary (on/off) inputs and one binary output. \n",
    "- essentially a simple if statement\n",
    "\n",
    "Artificial neurons activates its output when more than a certain number of its inputs are active\n",
    "\n",
    "Figure 10-3 Artifical neurons operations \n",
    "![](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1003.png)\n",
    "\n",
    "- The first network on the left is the identity function: if neuron A is activated, then neuron C gets activated as well (since it receives two input signals from neuron A); but if neuron A is off, then neuron C is off as well.\n",
    "\n",
    "- The second network performs a logical AND: neuron C is activated only when both neurons A and B are activated (a single input signal is not enough to activate neuron C).\n",
    "\n",
    "- The third network performs a logical OR: neuron C gets activated if either neuron A or neuron B is activated (or both).\n",
    "\n",
    "- Finally, if we suppose that an input connection can inhibit the neuron’s activity (which is the case with biological neurons), then the fourth network computes a slightly more complex logical proposition: neuron C is activated only if neuron A is active and neuron B is off. If neuron A is active all the time, then you get a logical NOT: neuron C is active when neuron B is off, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Perceptron\n",
    "\n",
    "- **Perceptron**: a slightly different artificial neuron where the input and output are numbers. And  each input connection is associated with a weight. \n",
    "\n",
    "- **Linear threshold unit**: computes a weighted sum of its inputs  $\n",
    "\\left(z=w_{1} x_{1}+w_{2} x_{2}+\\cdots+w_{n} x_{n}=\\mathbf{x}^{\\top} \\mathbf{w}\\right)\n",
    "$ then applies a step function to that sum and outputs the result $\n",
    "h_{\\mathbf{w}}(\\mathbf{x})=\\operatorname{step}(z), \\text { where } z=\\mathbf{x}^{\\top} \\mathbf{w}\n",
    "$ \n",
    "\n",
    "![](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1004.png)\n",
    "\n",
    "The most common step function used in Perceptrons is the Heaviside step function\n",
    "Sometimes the sign function is used instead.\n",
    "\n",
    "Equation 10-1. Common step functions used in Perceptrons (assuming threshold = 0)\n",
    "$$\n",
    "\\text { heaviside }(z)=\\left\\{\\begin{array}{ll}\n",
    "0 & \\text { if } z<0 \\\\\n",
    "1 & \\text { if } z \\geq 0\n",
    "\\end{array} \\quad \\operatorname{sgn}(z)=\\left\\{\\begin{array}{ll}\n",
    "-1 & \\text { if } z<0 \\\\\n",
    "0 & \\text { if } z=0 \\\\\n",
    "+1 & \\text { if } z>0\n",
    "\\end{array}\\right.\\right.\n",
    "$$\n",
    "\n",
    "- A single TLU can be used for simple linear binary classification\n",
    "\n",
    "EX: Ue a single TLU to classify iris flowers based on petal length and width \n",
    "\n",
    "- Training a TLU in this case means finding the right values for $w_0$, $w_1$, and $w_2$\n",
    "\n",
    "- A Perceptron is simply composed of a single layer of TLUs, with each TLU connected to all the inputs. \n",
    "\n",
    "-  **fully connected layer**, or a **dense layer**: When all the neurons in a layer are connected to every neuron in the previous layer.\n",
    "\n",
    "- **input neurons**: they output whatever input they are fed.\n",
    "\n",
    "- **Input layer**: All of the input neurons\n",
    "\n",
    "- **bias neuron**: Represents a bias feature. outputs 1 all the time\n",
    "\n",
    " A Perceptron with two inputs and three outputs is represented in Figure 10-5 below. This Perceptron can classify instances simultaneously into three different binary classes, which makes it a multilabel classifier.\n",
    " \n",
    " ![](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1005.png)\n",
    " \n",
    " \n",
    " We can compute the output of a layer of neurons all at once. \n",
    " \n",
    "Eqn 10-2:  Computing the outputs of a fully connected layer \n",
    " $$\n",
    "h_{\\mathbf{W}, \\mathbf{b}}(\\mathbf{X})=\\phi(\\mathbf{X} \\mathbf{W}+\\mathbf{b})\n",
    "$$\n",
    "\n",
    "- X represents the matrix of input features. It has one row per instance and one column per feature.\n",
    "\n",
    "- The weight matrix W contains all the connection weights except for the ones from the bias neuron. It has one row per input neuron and one column per artificial neuron in the layer.\n",
    "\n",
    "- The bias vector b contains all the connection weights between the bias neuron and the artificial neurons. It has one bias term per artificial neuron.\n",
    "\n",
    "- The function ϕ is called the activation function: when the artificial neurons are TLUs, it is a step function.\n",
    "\n",
    "-  **Hebb’s rule** (or Hebbian learning):the connection weight between two neurons tends to increase when they fire simultaneously. \n",
    "\n",
    "This Perceptron learning rule reinforces connections that help reduce the error.\n",
    "\n",
    "More specifically, the Perceptron is fed one training instance at a time, and for each instance it makes its predictions. For every output neuron that produced a wrong prediction, it reinforces the connection weights from the inputs that would have contributed to the correct prediction. The rule is shown in Equation 10-3 below.\n",
    "\n",
    "$$\n",
    "w_{i, j}^{(\\text {next step })}=w_{i, j}+\\eta\\left(y_{j}-\\hat{y}_{j}\\right) x_{i}\n",
    "$$\n",
    "\n",
    "- $w_{i, j}$ is the connection weight between the ith input neuron and the jth output neuron.\n",
    "\n",
    "- $x_i$ is the ith input value of the current training instance.\n",
    "\n",
    "- $\\hat{y}_j$ is the output of the jth output neuron for the current training instance.\n",
    "\n",
    "- $y_j$ is the target output of the jth output neuron for the current training instance.\n",
    "\n",
    "- η is the learning rate.\n",
    "\n",
    "\n",
    "The decision boundary of each output neuron is linear, so Perceptrons are incapable of learning complex patterns \n",
    "\n",
    "- **Perceptron convergence theorem.**  If the training instances are linearly separable,this algorithm would converge to a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]  # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int)  # Iris setosa?\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perceptron learning algorithm strongly resembles Stochastic Gradient Descent.\n",
    "\n",
    "Scikit-Learn’s Perceptron class is equivalent to using an SGDClassifier with the following hyperparameters: loss=\"perceptron\", learning_rate=\"constant\", eta0=1 (the learning rate), and penalty=None (no regularization).\n",
    "\n",
    "- Perceptrons do not output a class probability. This is one reason to prefer Logistic Regression over Perceptrons.\n",
    "\n",
    "limitations of Perceptrons can be eliminated by stacking multiple Perceptrons.\n",
    "\n",
    "- **Multilayer Perceptron**: Multiple Stacked Perceptrons \n",
    "\n",
    "# The Multilayer Perceptron and Backpropagation\n",
    "\n",
    "- An MLP is composed of one input layer, one or more layers of TLUs(hidden layers)  and one final layer of TLUs called the output layer. \n",
    "\n",
    "- **lower layers**: layers close to the input layer\n",
    "\n",
    "Every layer except the output layer includes a bias neuron and is fully connected to the next layer.\n",
    "\n",
    "- **deep neural network (DNN)**: When an ANN contains a deep stack of hidden layers\n",
    "\n",
    "\n",
    "The trainging algorithms for MLP is \n",
    "\n",
    "- **backpropagation training algorithm**: In short, it is Gradient Descent using an efficient technique for computing the gradients automatically. Determines how to tweak the weights to reduce error. \n",
    "\n",
    "- **NOTE**:  Automatically computing gradients is called automatic differentiation, or autodiff. There are various autodiff techniques, with different pros and cons. The one used by backpropagation is called reverse-mode autodiff. It is fast and precise, and is well suited when the function to differentiate has many variables (e.g., connection weights) and few outputs (e.g., one loss). If you want to learn more about autodiff, check out Appendix D in the book.\n",
    "\n",
    "Let’s run through this algorithm in a bit more detail:\n",
    "\n",
    "- It handles one mini-batch at a time (for example, containing 32 instances each), and it goes through the full training set multiple times. Each pass is called an epoch.\n",
    "\n",
    "- Each mini-batch is passed to the network’s input layer, which sends it to the first hidden layer. The algorithm then computes the output of all the neurons in this layer (for every instance in the mini-batch). The result is passed on to the next layer, its output is computed and passed to the next layer, and so on until we get the output of the last layer, the output layer. This is the forward pass: it is exactly like making predictions, except all intermediate results are preserved since they are needed for the backward pass.\n",
    "\n",
    "- Next, the algorithm measures the network’s output error (i.e., it uses a loss function that compares the desired output and the actual output of the network, and returns some measure of the error).\n",
    "\n",
    "- Then it computes how much each output connection contributed to the error. This is done analytically by applying the chain rule (perhaps the most fundamental rule in calculus), which makes this step fast and precise.\n",
    "\n",
    "- The algorithm then measures how much of these error contributions came from each connection in the layer below, again using the chain rule, working backward until the algorithm reaches the input layer. As explained earlier, this reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward through the network (hence the name of the algorithm).\n",
    "\n",
    "- Finally, the algorithm performs a Gradient Descent step to tweak all the connection weights in the network, using the error gradients it just computed.\n",
    "\n",
    "Summarzing this : for each training instance, the backpropagation algorithm first makes a prediction (forward pass) and measures the error, then goes through each layer in reverse to measure the error contribution from each connection (reverse pass), and finally tweaks the connection weights to reduce the error (Gradient Descent step).\n",
    "\n",
    "- **WARNING** : It is important to initialize all the hidden layers’ connection weights randomly, or else training will fail. For example, if you initialize all weights and biases to zero, then all neurons in a given layer will be perfectly identical, and thus backpropagation will affect them in exactly the same way, so they will remain identical. In other words, despite having hundreds of neurons per layer, your model will act as if it had only one neuron per layer: it won’t be too smart. If instead you randomly initialize the weights, you break the symmetry and allow backpropagation to train a diverse team of neurons.\n",
    "\n",
    "- In order for this algorithm to work properly the step function is the logistic (sigmoid) function $\\sigma(z)=1 /(1+\\exp (-z))$. This adds a hill(a gradient can then be used )\n",
    "\n",
    "\n",
    "### Other Popular Step Functions \n",
    "\n",
    "The hyperbolic tangent function: tanh(z) = 2σ(2z) – 1\n",
    "\n",
    "    Just like the logistic function, this activation function is S-shaped, continuous, and differentiable, but its output value ranges from –1 to 1 (instead of 0 to 1 in the case of the logistic function). That range tends to make each layer’s output more or less centered around 0 at the beginning of training, which often helps speed up convergence.\n",
    "    \n",
    "The Rectified Linear Unit function: ReLU(z) = max(0, z)\n",
    "\n",
    "     The ReLU function is continuous but unfortunately not differentiable at z = 0 (the slope changes abruptly, which can make Gradient Descent bounce around), and its derivative is 0 for z < 0. In practice, however, it works very well and has the advantage of being fast to compute, so it has become the default.13 Most importantly, the fact that it does not have a maximum output value helps reduce some issues during Gradient Descent).\n",
    "     \n",
    "- **activation functions**: A step function \n",
    "\n",
    "If we don't use activation functions then each layer will be considered a single layer. This is because they all solve a problem with the same complexity. \n",
    "\n",
    "![](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1008.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression MLPs\n",
    "\n",
    "- MLPs can be used for regression tasks\n",
    "\n",
    "-  If you want to predict a single value you just need a single output neuron.\n",
    "\n",
    "- For multivariate regression, you need one output neuron per output dimension\n",
    "\n",
    "- For MLP for regression, you do not want to use any activation function for the output neurons\n",
    "\n",
    "-  To guarantee that the output will always be positive, then you can use the ReLU activation function in the output layer.  \n",
    "\n",
    "- **Softplus activation function**: A smooth variant of ReLU, softplus(z) = log(1 + exp(z))\n",
    "\n",
    "- If you want to guarantee that the predictions will fall within a given range of values, then you can use the logistic function or the hyperbolic tangent, then scale the labels to the appropriate range. \n",
    "\n",
    "Typical Loss function \n",
    "\n",
    "- mean squared error\n",
    "\n",
    "- if you have a lot of outliers in the training set, you may prefer to use the mean absolute error instead\n",
    "\n",
    "Huber loss, which is a combination of both.\n",
    "\n",
    "**TIP**: The Huber loss is quadratic when the error is smaller than a threshold δ (typically 1) but linear when the error is larger than δ. The linear part makes it less sensitive to outliers than the mean squared error, and the quadratic part allows it to converge faster and be more precise than the mean absolute error.\n",
    "\n",
    "\n",
    "### Typical  architecture of a regression MLP\n",
    "\n",
    "| Hyperparameter\t|Typical value |\n",
    "|-------------------|--------------|\n",
    "| # input neurons   | One per input feature (e.g., 28 x 28 = 784 for MNIST) | \n",
    "| # hidden layers   | Depends on the problem, but typically 1 to 5  |\n",
    "|# neurons per hidden layer| Depends on the problem, but typically 10 to 100|\n",
    "|# output neurons    | 1 per prediction dimension |\n",
    "| Hidden activation  | ReLU (or SELU, see Chapter 11) | \n",
    "| Output activation  | None, or ReLU/softplus (if positive outputs) or logistic/tanh (if bounded outputs) |\n",
    "| Loss function    | MSE or MAE/Huber (if outliers) | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification MLPs\n",
    "\n",
    "- For a binary classification problem, you just need a single output neuron using the logistic activation function\n",
    "\n",
    "- the output will be a number between 0 and 1, which you can interpret as the estimated probability of the positive class\n",
    "\n",
    "- The estimated probability of the negative class is equal to one minus that number.\n",
    "\n",
    "Multilabel binary classification \n",
    "\n",
    "- Dedicate one output neuron for each positive class\n",
    "\n",
    "- If each instance can belong only to a single class (eg a single digit from 0 thru 9) you need to have one output neuron per class,.\n",
    "\n",
    "- Use a softmax activation function for the whole output layer. \n",
    "\n",
    "- **softmax function**: ensures that all the estimated probabilities are between 0 and 1 and that they add up to 1 \n",
    "\n",
    "Loss Function \n",
    "\n",
    "- Cross-entropy loss (also called the log loss) is generally a good choice. \n",
    "\n",
    "\n",
    "Typical architecture of a classification MLP.\n",
    "\n",
    "|Hyperparameter\t         | Binary classification\t|Multilabel binary classification | Multiclass classification |\n",
    "|------------------------|--------------------------|---------------------------------|---------------------------|\n",
    "|Input and hidden layers | Same as regression       |Same as regression               | Same as regression        |\n",
    "| # output neurons       | 1                        |1 per label                      |       1 per class         |\n",
    "|Output layer activation |Logistic                  | Logistic                        | Softmax                   |\n",
    "| Loss function          |Cross entropy             |Cross entropy                    |Cross entropy              |\n",
    "\n",
    "\n",
    "You have all the concepts you need to start implementing MLPs with Keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing MLPs with Keras\n",
    "\n",
    "\n",
    "**Multibackend Keras**\n",
    "\n",
    "-  To perform the heavy computations required by neural networks it  relies on a computation backend. \n",
    "\n",
    "- you can choose from three popular open source Deep Learning libraries: TensorFlow, Microsoft Cognitive Toolkit (CNTK), and Theano. other implementations have been released JavaScript or TypeScript (to run Keras code in a web browser), and PlaidML (which can run on all sorts of GPU devices, not just Nvidia) and many more. \n",
    "\n",
    "- TensorFlow itself now comes bundled with its own Keras implementation, tf.keras. This allows us to use Tensorflow Apis' such as TF Data API.  \n",
    "\n",
    "![](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1010.png)\n",
    "\n",
    "- The most popular Deep Learning library, after Keras and TensorFlow, is Facebook’s PyTorch library.\n",
    "\n",
    "- Once you know Keras, it is not difficult to switch to PyTorch. They were inspired by sklearn and chainer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__\n",
    "#the version of the Keras API implemented by tf.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Image Classifier Using the Sequential API\n",
    "\n",
    "-  We will use Fashion MNIST\n",
    "\n",
    "- 70,000 grayscale images of 28 × 28 pixels each, with 10 classes\n",
    "\n",
    "- the images represent fashion items rather than handwritten digits\n",
    "\n",
    "- Thus the problem turns out to be significantly more challenging than MNIST.\n",
    "\n",
    "- A simple linear model reaches about 92% accuracy on MNIST, but only about 83% on Fashion MNIST.\n",
    "\n",
    "## USING KERAS TO LOAD THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras provides some utility functions to fetch and load common datasets\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t every image is represented as a 28 × 28 array rather than a 1D array of size 784\n",
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no validation set, so we’ll create one now. since we are going to train the neural network using Gradient Descent, we must scale the input features. For simplicity, we’ll scale the pixel intensities down to the 0–1 range by dividing them by 255.0 .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Fashion MNIST we need the list of class names to know what we are dealing with:\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "# the first image in the training set represents a coat:\n",
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples from the Fashion MSNT Dataset \n",
    "\n",
    "![](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1011.png)\n",
    "\n",
    "# CREATING THE MODEL USING THE SEQUENTIAL API\n",
    "\n",
    "Here is a classification MLP with two hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s go through this code line by line:\n",
    "\n",
    "- The first line creates a Sequential model. This is the simplest kind of Keras model for neural networks that are just composed of a single stack of layers connected sequentially. This is called the Sequential API.\n",
    "\n",
    "- Next, we build the first layer and add it to the model. It is a Flatten layer whose role is to convert each input image into a 1D array: if it receives input data X, it computes X.reshape(-1, 28*28). This layer does not have any parameters; it is just there to do some simple preprocessing. Since it is the first layer in the model, you should specify the input_shape, which doesn’t include the batch size, only the shape of the instances. Alternatively, you could add a keras.layers.InputLayer as the first layer, setting input_shape=[28,28].\n",
    "\n",
    "- Next we add a Dense hidden layer with 300 neurons. It will use the ReLU activation function. Each Dense layer manages its own weight matrix, containing all the connection weights between the neurons and their inputs. It also manages a vector of bias terms (one per neuron). When it receives some input data, it computes Equation 10-2.\n",
    "\n",
    "- Then we add a second Dense hidden layer with 100 neurons, also using the ReLU activation function.\n",
    "\n",
    "- Finally, we add a Dense output layer with 10 neurons (one per class), using the softmax activation function (because the classes are exclusive).\n",
    "\n",
    "\n",
    "- **TIP**: Specifying activation=\"relu\" is equivalent to specifying activation=keras.activations.relu. Other activation functions are available in the keras.activations package, we will use many of them in this book. See https://keras.io/activations/ for the full list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of adding the layers one by one as we just did, \n",
    "# you can pass a list of layers when creating the Sequential model:\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING CODE EXAMPLES FROM KERAS.IO\n",
    "\n",
    "Code examples documented on keras.io will work fine with tf.keras, but you need to change the imports. For example, consider this keras.io code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "# output_layer = Dense(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You must change the imports like this:\n",
    "from tensorflow.keras.layers import Dense\n",
    "output_layer = Dense(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is more verbose, but I use it in this book so you can easily see which packages to use, and to avoid confusion between standard classes and custom classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model’s summary() method displays all the model’s layers \n",
    "\n",
    "The summary includes:\n",
    "\n",
    "- Each layer’s name\n",
    "- its output shape\n",
    "- number of parameters\n",
    "\n",
    "- The summary ends with the total number of parameters, including trainable and non-trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Dense layers** often have a lot of parameters. \n",
    "\n",
    "- For example, the first hidden layer has 784 × 300 connection weights, plus 300 bias terms, which adds up to 235,500 parameters\n",
    "\n",
    "- This gives the model quite a lot of flexibility to fit the training data, but it also means that the model runs the risk of overfitting especially when you do not have a lot of training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Flatten at 0x1c3a2b0c4f0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1c3a2b0cc70>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1c3a9850070>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1c3a98502e0>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can easily get a model’s list of layers, to fetch a layer by its index, or you can fetch it by name:\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_3'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('dense_3') is hidden1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.1464750e-03, -3.2529876e-02, -1.5262183e-02, ...,\n",
       "        -1.3105795e-03,  8.5406899e-03, -3.6444444e-02],\n",
       "       [-6.3532025e-02,  6.0944483e-02,  6.5623716e-02, ...,\n",
       "         5.2042976e-02, -8.5994154e-03,  7.0309341e-03],\n",
       "       [ 6.4758435e-02,  3.1512059e-02,  3.3124819e-02, ...,\n",
       "         1.2559891e-02, -2.0416554e-02,  5.4857239e-02],\n",
       "       ...,\n",
       "       [ 5.5092201e-03,  5.4104820e-02, -8.4963441e-03, ...,\n",
       "        -1.4903169e-02,  5.2396208e-02,  1.7420314e-02],\n",
       "       [ 2.2592530e-02,  6.8063006e-02, -9.8086894e-05, ...,\n",
       "        -3.6417391e-02, -2.2300880e-02,  3.0775055e-02],\n",
       "       [-6.5863028e-02,  1.1268631e-02,  2.7589194e-02, ...,\n",
       "         3.5672411e-03, -4.0185284e-02,  2.6057616e-02]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access parameters with get_weights() and set_weights()\n",
    "weights, biases = hidden1.get_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dense layer initialized the connection weights randomly( is needed to break symmetry) and  biases were initialized to zeros, which is fine.\n",
    "\n",
    "- For a different initialization method, you can set kernel_initializer (kernel is another name for the matrix of connection weights) or bias_initializer when creating the layer.\n",
    "\n",
    "-  initialization method will be further dicussed in chapter 11\n",
    "\n",
    "\n",
    "- NOTE:  The shape of the weight matrix depends on the number of inputs. This is why it is recommended to specify the input_shape when creating the first layer in a Sequential model. However, if you do not specify the input shape, it’s OK: Keras will simply wait until it knows the input shape before it actually builds the model. This will happen either when you feed it actual data (e.g., during training), or when you call its build() method. Until the model is really built, the layers will not have any weights, and you will not be able to do certain things (such as print the model summary or save the model). So, if you know the input shape when creating the model, it is best to specify it.\n",
    "\n",
    "\n",
    "# COMPILING THE MODEL\n",
    "\n",
    "After a model is created, you must call its compile() method to specify the loss function and the optimizer to use. Optionally, you can specify a list of extra metrics to compute during training and evaluation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Using loss=\"sparse_categorical_crossentropy\" is equivalent to using loss=keras.losses.sparse_categorical_crossentropy. Similarly, specifying optimizer=\"sgd\" is equivalent to specifying optimizer=keras.optimizers.SGD(), and metrics=[\"accuracy\"] is equivalent to metrics=[keras.metrics.sparse_categorical_accuracy] (when using this loss). We will use many other losses, optimizers, and metrics in this book; for the full lists,\n",
    "\n",
    "\n",
    "- We use the \"sparse_categorical_crossentropy\" loss because we have sparse labels (i.e., for each instance, there is just a target class index, from 0 to 9 in this case), and the classes are exclusive.\n",
    "\n",
    "\n",
    "- If instead we had one target probability per class for each instance (such as one-hot vectors to represent class 3), then we would need to use the \"categorical_crossentropy\" loss instead.\n",
    "\n",
    "\n",
    "- were doing binary classification or multilabel binary classification, then we would use the \"sigmoid\"  activation function in the output layer instead of the \"softmax\" activation function, and we would use the \"binary_crossentropy\" loss.\n",
    "\n",
    "\n",
    "- **TIP**: If you want to convert sparse labels (i.e., class indices) to one-hot vector labels, use the keras.utils.to_categorical() function. To go the other way round, use the np.argmax() function with axis=1.\n",
    "\n",
    "Regarding the optimizer, \"sgd\" means that we will train the model using simple Stochastic Gradient Descent. In other words, Keras will perform the backpropagation algorithm described earlier (i.e., reverse-mode autodiff plus Gradient Descent). We will discuss more efficient optimizers in Chapter 11 (they improve the Gradient Descent part, not the autodiff).\n",
    "\n",
    "\n",
    "\n",
    "- NOTE: When using the SGD optimizer, it is important to tune the learning rate. So, you will generally want to use optimizer=keras.optimizers.SGD(lr=???) to set the learning rate, rather than optimizer=\"sgd\", which defaults to lr=0.01.\n",
    "\n",
    " # TRAINING AND EVALUATING THE MODEL\n",
    " \n",
    "Now the model is ready to be trained. For this we simply need to call its fit() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.7094 - accuracy: 0.7661 - val_loss: 0.5023 - val_accuracy: 0.8312\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4868 - accuracy: 0.8303 - val_loss: 0.4544 - val_accuracy: 0.8452\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4423 - accuracy: 0.8457 - val_loss: 0.4451 - val_accuracy: 0.8446\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4155 - accuracy: 0.8532 - val_loss: 0.4308 - val_accuracy: 0.8472\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3964 - accuracy: 0.8610 - val_loss: 0.3974 - val_accuracy: 0.8626\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3790 - accuracy: 0.8675 - val_loss: 0.3650 - val_accuracy: 0.8740\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3656 - accuracy: 0.8709 - val_loss: 0.3703 - val_accuracy: 0.8710\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3549 - accuracy: 0.8738 - val_loss: 0.3492 - val_accuracy: 0.8822\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3435 - accuracy: 0.8786 - val_loss: 0.3507 - val_accuracy: 0.8782\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3340 - accuracy: 0.8815 - val_loss: 0.3510 - val_accuracy: 0.8762\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3261 - accuracy: 0.8839 - val_loss: 0.3534 - val_accuracy: 0.8712\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3180 - accuracy: 0.8854 - val_loss: 0.3398 - val_accuracy: 0.8790\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3108 - accuracy: 0.8878 - val_loss: 0.3338 - val_accuracy: 0.8846\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3032 - accuracy: 0.8908 - val_loss: 0.3337 - val_accuracy: 0.8798\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2970 - accuracy: 0.8933 - val_loss: 0.3249 - val_accuracy: 0.8824\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2915 - accuracy: 0.8959 - val_loss: 0.3210 - val_accuracy: 0.8862\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2842 - accuracy: 0.8974 - val_loss: 0.3124 - val_accuracy: 0.8894\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2786 - accuracy: 0.8996 - val_loss: 0.3255 - val_accuracy: 0.8816\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2742 - accuracy: 0.9012 - val_loss: 0.3138 - val_accuracy: 0.8886\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2683 - accuracy: 0.9036 - val_loss: 0.3076 - val_accuracy: 0.8900\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2636 - accuracy: 0.9045 - val_loss: 0.3102 - val_accuracy: 0.8874\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2590 - accuracy: 0.9063 - val_loss: 0.3123 - val_accuracy: 0.8828\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2532 - accuracy: 0.9093 - val_loss: 0.3129 - val_accuracy: 0.8874\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2499 - accuracy: 0.9095 - val_loss: 0.3125 - val_accuracy: 0.8914\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2450 - accuracy: 0.9127 - val_loss: 0.3023 - val_accuracy: 0.8908\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2414 - accuracy: 0.9129 - val_loss: 0.3766 - val_accuracy: 0.8584\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2375 - accuracy: 0.9139 - val_loss: 0.3041 - val_accuracy: 0.8884\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2345 - accuracy: 0.9157 - val_loss: 0.3062 - val_accuracy: 0.8900\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2292 - accuracy: 0.9174 - val_loss: 0.3095 - val_accuracy: 0.8918\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2254 - accuracy: 0.9192 - val_loss: 0.3084 - val_accuracy: 0.8902\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We pass it the input features (X_train) and the target classes (y_train), as well as the number of epochs to train (or else it would default to just 1). \n",
    "\n",
    "-  We also pass a validation set (this is optional)\n",
    "\n",
    "-  Keras will measure the loss and the extra metrics on this set at the end of each epoch, which is useful to see how the model really performs \n",
    "\n",
    "- If the performance on the training set is better than on the validation set. The model is probably overfitting the training set. \n",
    "\n",
    "- **TIP**: Instead of passing a validation set using the validation_data argument, you could set validation_split to the ratio of the training set that you want Keras to use for validation. For example, validation_split=0.1 tells Keras to use the last 10% of the data (before shuffling) for validation.\n",
    "\n",
    "- If the training set was very skewed(specific class with higher representaion),it would be useful to set the class_weight argument when calling the fit() method, which would give a larger weight to underrepresented classes and a lower weight to overrepresented classes.\n",
    "\n",
    "- If you need per-instance weights, set the sample_weight argument (if both class_weight and sample_weight are provided, Keras multiplies them). Per-instance weights could be useful if some instances were labeled by experts while others were labeled using a crowdsourcing platform: you might want to give more weight to the former. You can also provide sample weights (but not class weights) for the validation set by adding them as a third item in the validation_data tuple.\n",
    "\n",
    "- The fit() method returns a **History** object containing the training parameters(history.params), the list of epochs it went through(history.epoch), and most importantly a dictionary(history.history). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.7094171643257141,\n",
       "  0.48683422803878784,\n",
       "  0.44231462478637695,\n",
       "  0.4154796600341797,\n",
       "  0.3963824212551117,\n",
       "  0.3789786994457245,\n",
       "  0.365550696849823,\n",
       "  0.3548777401447296,\n",
       "  0.3434709310531616,\n",
       "  0.3339502811431885,\n",
       "  0.3261406719684601,\n",
       "  0.3179630935192108,\n",
       "  0.310824990272522,\n",
       "  0.3031958341598511,\n",
       "  0.29700279235839844,\n",
       "  0.29147017002105713,\n",
       "  0.28419044613838196,\n",
       "  0.2786494791507721,\n",
       "  0.27419376373291016,\n",
       "  0.2682740092277527,\n",
       "  0.26361826062202454,\n",
       "  0.25903797149658203,\n",
       "  0.25324445962905884,\n",
       "  0.2498549520969391,\n",
       "  0.24502216279506683,\n",
       "  0.24144811928272247,\n",
       "  0.23752665519714355,\n",
       "  0.23446249961853027,\n",
       "  0.22917698323726654,\n",
       "  0.2254130095243454],\n",
       " 'accuracy': [0.7660727500915527,\n",
       "  0.8303454518318176,\n",
       "  0.845727264881134,\n",
       "  0.8531818389892578,\n",
       "  0.8610363602638245,\n",
       "  0.8674545288085938,\n",
       "  0.8709454536437988,\n",
       "  0.8737999796867371,\n",
       "  0.878600001335144,\n",
       "  0.8815272450447083,\n",
       "  0.8839272856712341,\n",
       "  0.885418176651001,\n",
       "  0.8877636194229126,\n",
       "  0.8907999992370605,\n",
       "  0.8933091163635254,\n",
       "  0.8959090709686279,\n",
       "  0.8973818421363831,\n",
       "  0.8995636105537415,\n",
       "  0.901199996471405,\n",
       "  0.9035817980766296,\n",
       "  0.9044545292854309,\n",
       "  0.9063454270362854,\n",
       "  0.9092727303504944,\n",
       "  0.9095273017883301,\n",
       "  0.9126726984977722,\n",
       "  0.9129454493522644,\n",
       "  0.9138908982276917,\n",
       "  0.9156727194786072,\n",
       "  0.9174181818962097,\n",
       "  0.9192000031471252],\n",
       " 'val_loss': [0.5022676587104797,\n",
       "  0.45444750785827637,\n",
       "  0.4451088011264801,\n",
       "  0.4308033585548401,\n",
       "  0.3974446654319763,\n",
       "  0.36501264572143555,\n",
       "  0.37028881907463074,\n",
       "  0.3491823077201843,\n",
       "  0.35074782371520996,\n",
       "  0.35104966163635254,\n",
       "  0.35337305068969727,\n",
       "  0.3397827744483948,\n",
       "  0.33380985260009766,\n",
       "  0.33366143703460693,\n",
       "  0.3249349892139435,\n",
       "  0.32096850872039795,\n",
       "  0.3123718202114105,\n",
       "  0.32546693086624146,\n",
       "  0.3138315975666046,\n",
       "  0.30757835507392883,\n",
       "  0.3102366626262665,\n",
       "  0.312310129404068,\n",
       "  0.3128533959388733,\n",
       "  0.3124881982803345,\n",
       "  0.30226075649261475,\n",
       "  0.3765799403190613,\n",
       "  0.3041410744190216,\n",
       "  0.30618494749069214,\n",
       "  0.30953851342201233,\n",
       "  0.3084092438220978],\n",
       " 'val_accuracy': [0.8312000036239624,\n",
       "  0.8452000021934509,\n",
       "  0.8446000218391418,\n",
       "  0.8471999764442444,\n",
       "  0.8626000285148621,\n",
       "  0.8740000128746033,\n",
       "  0.8709999918937683,\n",
       "  0.8822000026702881,\n",
       "  0.8781999945640564,\n",
       "  0.8762000203132629,\n",
       "  0.8712000250816345,\n",
       "  0.8790000081062317,\n",
       "  0.8845999836921692,\n",
       "  0.879800021648407,\n",
       "  0.8823999762535095,\n",
       "  0.8862000107765198,\n",
       "  0.8894000053405762,\n",
       "  0.881600022315979,\n",
       "  0.8885999917984009,\n",
       "  0.8899999856948853,\n",
       "  0.8873999714851379,\n",
       "  0.8827999830245972,\n",
       "  0.8873999714851379,\n",
       "  0.8913999795913696,\n",
       "  0.8907999992370605,\n",
       "  0.8583999872207642,\n",
       "  0.8884000182151794,\n",
       "  0.8899999856948853,\n",
       "  0.8917999863624573,\n",
       "  0.8902000188827515]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The training accuracy and the validation accuracy steadily increase during training, while the training loss and the validation loss decrease. Good! \n",
    "\n",
    "- the validation curves are close to the training curves, which means that there is not too much overfitting.\n",
    "\n",
    "- The validation error is computed at the end of each epoch, while the training error is computed using a running mean during each epoch. Which is why validation accuracy starts off higher. \n",
    "\n",
    "- **TIP**: When plotting the training curve, it should be shifted by half an epoch to the left.\n",
    "\n",
    "\n",
    "- You can tell that the model has not quite converged yet, as the validation loss is still going down, so you should probably continue training.It’s as simple as calling the fit() method again, since Keras just continues training where it left off \n",
    "\n",
    "### Hyperparmeter Tuning \n",
    "\n",
    "- If you are not satisfied with the performance of your model, you should go back and tune the hyperparameters\n",
    "\n",
    "- first check is the learning rate.\n",
    "\n",
    "- try another optimizer(always retune the learning rate after changing any hyperparameter)\n",
    "\n",
    "If still unstatisfied\n",
    "\n",
    "- Try tuning model hyperparameters such as the number of layers\n",
    "\n",
    "- number of neurons per layer\n",
    "\n",
    "-  the types of activation functions to use for each hidden layer\n",
    "\n",
    "- the batch size (it can be set in the fit() method using the batch_size argument)\n",
    "\n",
    "Once you are satisfied with your model’s validation accuracy, you should evaluate it on the test set to estimate the generalization error before you deploy the model to production\n",
    "\n",
    "- You can easily do this using the evaluate() method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3380 - accuracy: 0.8785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3379684388637543, 0.8784999847412109]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it is common to get slightly lower performance on the test set than on the validation set,not the test set. \n",
    "\n",
    "-  resist the temptation to tweak the hyperparameters on the test set, or else your estimate of the generalization error will be too optimistic.\n",
    "\n",
    "# USING THE MODEL TO MAKE PREDICTIONS\n",
    "\n",
    "- use the model’s predict() method to make predictions on new instances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.01, 0.  , 0.98],\n",
       "       [0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- each instance the model estimates one probability per class, from class 0 to class 9.\n",
    "\n",
    "-  If you only care about the class with the highest estimated probability (even if that probability is quite low), then you can use the predict_classes() method instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JungleBook\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Another way of doing it, they have been changing how the api works. \n",
    "np.argmax(model.predict(X_new), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Regression MLP Using the Sequential API\n",
    "\n",
    "Let’s switch to the California housing problem and tackle it using a regression neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target) #Load data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full) #Cool way of splitting the full training/test into validatoin sets \n",
    "\n",
    "scaler = StandardScaler() #Going to use a sgd method, requires normalization \n",
    "X_train = scaler.fit_transform(X_train) \n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The main differences(b/w classification and regression) are the fact that the output layer has a single neuron (since we only want to predict a single value) and uses no activation function, and the loss function is the mean squared error.\n",
    "\n",
    "- Since the dataset is quite noisy, we just use a single hidden layer with fewer neurons than before, to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.1046 - val_loss: 1.0134\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7778 - val_loss: 0.7846\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7242 - val_loss: 0.4767\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4306 - val_loss: 0.4491\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4150 - val_loss: 0.4345\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4043 - val_loss: 0.4307\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3970 - val_loss: 0.4218\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3985 - val_loss: 0.4266\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3931 - val_loss: 0.4128\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3853 - val_loss: 0.4099\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3812 - val_loss: 0.4088\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3799 - val_loss: 0.4064\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3759 - val_loss: 0.3999\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3736 - val_loss: 0.3987\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3708 - val_loss: 0.3999\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3700 - val_loss: 0.3935\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3686 - val_loss: 0.3947\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3790 - val_loss: 0.3950\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3698 - val_loss: 0.3924\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3662 - val_loss: 0.3919\n",
      "162/162 [==============================] - 0s 935us/step - loss: 0.3577\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]), #One layer using the relu, that standard \n",
    "    keras.layers.Dense(1) #A single output\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\") #Specify hyperparameters useed for training \n",
    "history = model.fit(X_train, y_train, epochs=20,  #Train\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)  \n",
    "X_new = X_test[:3] # pretend these are new instances\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential API is quite easy to use, it is sometimes useful to build neural networks with more complex topologies, or with multiple inputs or outputs. For this purpose, Keras offers the Functional API.\n",
    "\n",
    "# Building Complex Models Using the Functional API\n",
    "\n",
    "One example of a nonsequential neural network is a Wide & Deep neural network. \n",
    "\n",
    "- It connects all or part of the inputs directly to the output layer\n",
    "\n",
    "- This architecture makes it possible for the neural network to learn both deep patterns (using the deep path) and simple rules (through the short path)\n",
    "\n",
    "In contrast, a regular MLP forces all the data to flow through the full stack of layers; thus, simple patterns in the data may end up being distorted by this sequence of transformations.\n",
    "\n",
    "![](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1014.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:]) \n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create an Input object. This is a specification of the kind of input the model will get, including its shape and dtype. A model may actually have multiple inputs, as we will see shortly.\n",
    "\n",
    "Next, we create a Dense layer with 30 neurons, using the ReLU activation function. As soon as it is created, notice that we call it like a function, passing it the input. This is why this is called the Functional API. Note that we are just telling Keras how it should connect the layers together; no actual data is being processed yet.\n",
    "\n",
    "We then create a second hidden layer, and again we use it as a function. Note that we pass it the output of the first hidden layer.\n",
    "\n",
    "Next, we create a Concatenate layer, and once again we immediately use it like a function, to concatenate the input and the output of the second hidden layer. You may prefer the keras.layers.concatenate() function, which creates a Concatenate layer and immediately calls it with the given inputs.\n",
    "\n",
    "Then we create the output layer, with a single neuron and no activation function, and we call it like a function, passing it the result of the concatenation.\n",
    "\n",
    "Lastly, we create a Keras Model, specifying which inputs and outputs to use.\n",
    "\n",
    "- Once you have built the Keras model, everything is exactly like earlier, so there’s no need to repeat it here: you must compile the model, train it, evaluate it, and use it to make predictions.\n",
    "\n",
    "But what if you want to send a subset of the features through the wide path and a different subset (possibly overlapping) through the deep path\n",
    "\n",
    "-  In this case, one solution is to use multiple inputs. For example, suppose we want to send five features through the wide path (features 0 to 4), and six features through the deep path (features 2 to 7):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1015.png)\n",
    "\n",
    "- You should name at least the most important layers, especially when the model gets a bit complex like this. \n",
    "\n",
    "Note that we specified inputs=[input_A, input_B] when creating the model. Now we can compile the model as usual, but when we call the fit() method, instead of passing a single input matrix X_train, we must pass a pair of matrices (X_train_A, X_train_B): one per input. The same is true for X_valid, and also for X_test and X_new when you call evaluate() or predict():\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JungleBook\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 1.6671 - val_loss: 0.8663\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7176 - val_loss: 0.7232\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6438 - val_loss: 0.6805\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6147 - val_loss: 0.6444\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5842 - val_loss: 0.6316\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5757 - val_loss: 0.6133\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5586 - val_loss: 0.6191\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5617 - val_loss: 0.6123\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5525 - val_loss: 0.6096\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5566 - val_loss: 0.5613\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5168 - val_loss: 0.5546\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5092 - val_loss: 0.5493\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5047 - val_loss: 0.5433\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5003 - val_loss: 0.5401\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4969 - val_loss: 0.5358\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4934 - val_loss: 0.5325\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4905 - val_loss: 0.5287\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4873 - val_loss: 0.5267\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4847 - val_loss: 0.5229\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4821 - val_loss: 0.5200\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.4604\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3)) \n",
    "#set hyperparameters for regression\n",
    "\n",
    "#We can select the features we wish to pass thru the wide later, \n",
    "# and the deep later\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "\n",
    "#Fit it with the required dimensions \n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    "                    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "# Eval\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "#Predict\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many use cases in which you may want to have multiple outputs:\n",
    "\n",
    "- The task may demand it. For instance, you may want to locate and classify the main object in a picture. This is both a regression task (finding the coordinates of the object’s center, as well as its width and height) and a classification task.\n",
    "\n",
    "- Similarly, you may have multiple independent tasks based on the same data. Sure, you could train one neural network per task, but in many cases you will get better results on all tasks by training a single neural network with one output per task. This is because the neural network can learn features in the data that are useful across tasks. For example, you could perform multitask classification on pictures of faces, using one output to classify the person’s facial expression (smiling, surprised, etc.) and another output to identify whether they are wearing glasses or not.\n",
    "\n",
    "- Another use case is as a regularization technique (i.e., a training constraint whose objective is to reduce overfitting and thus improve the model’s ability to generalize). For example, you may want to add some auxiliary outputs in a neural network architecture (see Figure 10-16) to ensure that the underlying part of the network learns something useful on its own, without relying on the rest of the network.\n",
    "\n",
    "![](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_1016.png)\n",
    "\n",
    "Adding extra outputs is quite easy: just connect them to the appropriate layers and add them to your model’s list of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each output will need its own loss function. Therefore, when we compile the model, we should pass a list of losses\n",
    "(if we pass a single loss, Keras will assume that the same loss must be used for all outputs)\n",
    "\n",
    "We care much more about the main output than about the auxiliary output (as it is just used for regularization)  so we want to give the main output’s loss a much greater weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we train the model, we need to provide labels for each output. In this example, the main output and the auxiliary output should try to predict the same thing, so they should use the same labels. So instead of passing y_train, we need to pass (y_train, y_train) (and the same goes for y_valid and y_test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 3ms/step - loss: 1.0184 - main_output_loss: 0.9683 - aux_output_loss: 1.4686 - val_loss: 0.6125 - val_main_output_loss: 0.5557 - val_aux_output_loss: 1.1243\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5525 - main_output_loss: 0.4967 - aux_output_loss: 1.0553 - val_loss: 0.5882 - val_main_output_loss: 0.5461 - val_aux_output_loss: 0.9670\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5481 - main_output_loss: 0.5084 - aux_output_loss: 0.9051 - val_loss: 0.5279 - val_main_output_loss: 0.4928 - val_aux_output_loss: 0.8434\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4872 - main_output_loss: 0.4520 - aux_output_loss: 0.8044 - val_loss: 0.4998 - val_main_output_loss: 0.4682 - val_aux_output_loss: 0.7843\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4641 - main_output_loss: 0.4336 - aux_output_loss: 0.7391 - val_loss: 0.4829 - val_main_output_loss: 0.4569 - val_aux_output_loss: 0.7167\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4476 - main_output_loss: 0.4213 - aux_output_loss: 0.6841 - val_loss: 0.4698 - val_main_output_loss: 0.4470 - val_aux_output_loss: 0.6752\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4369 - main_output_loss: 0.4133 - aux_output_loss: 0.6490 - val_loss: 0.4564 - val_main_output_loss: 0.4357 - val_aux_output_loss: 0.6433\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4244 - main_output_loss: 0.4032 - aux_output_loss: 0.6150 - val_loss: 0.4520 - val_main_output_loss: 0.4343 - val_aux_output_loss: 0.6108\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4718 - main_output_loss: 0.4541 - aux_output_loss: 0.6316 - val_loss: 0.4464 - val_main_output_loss: 0.4302 - val_aux_output_loss: 0.5922\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4173 - main_output_loss: 0.4005 - aux_output_loss: 0.5684 - val_loss: 0.4320 - val_main_output_loss: 0.4172 - val_aux_output_loss: 0.5646\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4070 - main_output_loss: 0.3910 - aux_output_loss: 0.5509 - val_loss: 0.4224 - val_main_output_loss: 0.4084 - val_aux_output_loss: 0.5480\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3946 - main_output_loss: 0.3789 - aux_output_loss: 0.5360 - val_loss: 0.4244 - val_main_output_loss: 0.4106 - val_aux_output_loss: 0.5492\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3946 - main_output_loss: 0.3796 - aux_output_loss: 0.5294 - val_loss: 0.4134 - val_main_output_loss: 0.4004 - val_aux_output_loss: 0.5304\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3836 - main_output_loss: 0.3693 - aux_output_loss: 0.5131 - val_loss: 0.4058 - val_main_output_loss: 0.3935 - val_aux_output_loss: 0.5166\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3808 - main_output_loss: 0.3667 - aux_output_loss: 0.5070 - val_loss: 0.4029 - val_main_output_loss: 0.3905 - val_aux_output_loss: 0.5145\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3929 - main_output_loss: 0.3802 - aux_output_loss: 0.5071 - val_loss: 0.4033 - val_main_output_loss: 0.3917 - val_aux_output_loss: 0.5079\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3722 - main_output_loss: 0.3596 - aux_output_loss: 0.4862 - val_loss: 0.3892 - val_main_output_loss: 0.3782 - val_aux_output_loss: 0.4886\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3762 - main_output_loss: 0.3643 - aux_output_loss: 0.4830 - val_loss: 0.3904 - val_main_output_loss: 0.3796 - val_aux_output_loss: 0.4874\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3714 - main_output_loss: 0.3598 - aux_output_loss: 0.4752 - val_loss: 0.3794 - val_main_output_loss: 0.3687 - val_aux_output_loss: 0.4763\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3619 - main_output_loss: 0.3499 - aux_output_loss: 0.4700 - val_loss: 0.3791 - val_main_output_loss: 0.3687 - val_aux_output_loss: 0.4729\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 2ms/step - loss: 0.3494 - main_output_loss: 0.3370 - aux_output_loss: 0.4606\n"
     ]
    }
   ],
   "source": [
    "# When we evaluate the model, Keras will return the total loss, as well as all the individual losses:\n",
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    [X_test_A, X_test_B], [y_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, the predict() method will return predictions for each output:\n",
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Subclassing API to Build Dynamic Models\n",
    "\n",
    "Both the Sequential API and the Functional API are declarative: you start by declaring which layers you want to use and how they should be connected, and only then can you start feeding the model some data for training or inference. This has many advantages:\n",
    "\n",
    "- he model can easily be saved. cloned and shared. \n",
    "\n",
    "- Its structure can be displayed and analyzed; the framework can infer shapes and check types, so errors can be caught early\n",
    "\n",
    "- It’s also fairly easy to debug, since the whole model is a static graph of layers\n",
    "\n",
    "But the flip side is just that: it's static  \n",
    "\n",
    "-  Some models involve loops, varying shapes, conditional branching, and other dynamic behaviors.\n",
    "\n",
    "- For such cases, or simply if you prefer a more imperative programming style, the Subclassing API is for you.\n",
    "\n",
    "Simply subclass the Model class, create the layers you need in the constructor, and use them to perform the computations you want in the call() method. For example, creating an instance of the following WideAndDeepModel class gives us an equivalent model to the one we just built with the Functional API. You can then compile it, evaluate it, and use it to make predictions, exactly like we just did:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs) # handles standard args (e.g., name)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 2s 3ms/step - loss: 2.5310 - output_1_loss: 2.2683 - output_2_loss: 4.8952 - val_loss: 1.3834 - val_output_1_loss: 1.0892 - val_output_2_loss: 4.0311\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.1728 - output_1_loss: 0.9357 - output_2_loss: 3.3062 - val_loss: 0.9831 - val_output_1_loss: 0.7888 - val_output_2_loss: 2.7316\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8717 - output_1_loss: 0.7118 - output_2_loss: 2.3102 - val_loss: 0.8633 - val_output_1_loss: 0.7344 - val_output_2_loss: 2.0235\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7859 - output_1_loss: 0.6721 - output_2_loss: 1.8098 - val_loss: 0.7987 - val_output_1_loss: 0.7012 - val_output_2_loss: 1.6756\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7366 - output_1_loss: 0.6446 - output_2_loss: 1.5641 - val_loss: 0.7577 - val_output_1_loss: 0.6753 - val_output_2_loss: 1.4998\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7025 - output_1_loss: 0.6211 - output_2_loss: 1.4349 - val_loss: 0.7285 - val_output_1_loss: 0.6533 - val_output_2_loss: 1.4056\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6759 - output_1_loss: 0.6000 - output_2_loss: 1.3590 - val_loss: 0.7046 - val_output_1_loss: 0.6337 - val_output_2_loss: 1.3431\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6549 - output_1_loss: 0.5826 - output_2_loss: 1.3054 - val_loss: 0.6841 - val_output_1_loss: 0.6153 - val_output_2_loss: 1.3036\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6373 - output_1_loss: 0.5671 - output_2_loss: 1.2689 - val_loss: 0.6667 - val_output_1_loss: 0.5999 - val_output_2_loss: 1.2682\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6215 - output_1_loss: 0.5532 - output_2_loss: 1.2367 - val_loss: 0.6516 - val_output_1_loss: 0.5864 - val_output_2_loss: 1.2391\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.5893 - output_1_loss: 0.5176 - output_2_loss: 1.2351\n",
      "WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C3A81708B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "history = model.fit((X_train_A, X_train_B), (y_train, y_train), epochs=10,\n",
    "                    validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)))\n",
    "total_loss, main_loss, aux_loss = model.evaluate((X_test_A, X_test_B), (y_test, y_test))\n",
    "y_pred_main, y_pred_aux = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example looks very much like the Functional API, except we do not need to create the inputs we just use the input argument to the call() methods and we separate the creation of the layers in the constructor from their usage in the call() method.\n",
    "\n",
    "\n",
    "- The big difference is that you can do pretty much anything you want in the call() method: for loops, if statements, low-level TensorFlow operations\n",
    "\n",
    "- This extra flexibility does come at a cost: your model’s architecture is hidden within the call() method, so Keras cannot easily inspect it\n",
    "\n",
    "- it cannot save or clone it\n",
    "\n",
    "- when you call the summary() method, you only get a list of layers, without any information on how they are connected to each other\n",
    "\n",
    "- Moreover, Keras cannot check types and shapes ahead of time, and it is easier to make mistakes.\n",
    "\n",
    "So unless you really need that extra flexibility, you should probably stick to the Sequential API or the Functional API.\n",
    "\n",
    "- **Tip**: Keras models can be used just like regular layers, so you can easily combine them to build complex architectures.\n",
    "\n",
    "# Saving and Restoring a Model\n",
    "\n",
    "When using the Sequential API or the Functional API, saving a trained Keras model is as simple as it gets:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.9206 - val_loss: 0.8762\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.7019 - val_loss: 0.6965\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6136 - val_loss: 0.6548\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5785 - val_loss: 0.6191\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5504 - val_loss: 0.5968\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5315 - val_loss: 0.5749\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5139 - val_loss: 0.5619\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5023 - val_loss: 0.5472\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4909 - val_loss: 0.5355\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4821 - val_loss: 0.5277\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.4586\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras will use the HDF5 format to save both the model’s architecture (including every layer’s hyperparameters) and the values of all the model parameters for every layer \n",
    "\n",
    "It also saves the optimizer \n",
    "\n",
    "You will typically have a script that trains a model and saves it, and one or more scripts (or web services) that load the model and use it to make predictions. Loading the model is just as easy:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **WARNING**: This will work when using the Sequential API or the Functional API, but unfortunately not when using model subclassing. You can use save_weights() and load_weights() to at least save and restore the model parameters, but you will need to save and restore everything else yourself.\n",
    "\n",
    "\n",
    "But what if training lasts several hours?\n",
    "\n",
    "\n",
    "In this case, you should not only save your model at the end of training, but also save checkpoints at regular intervals during training, to avoid losing everything if your computer crashes.\n",
    "\n",
    "How can you tell the fit() method to save checkpoints? Use callbacks.\n",
    "\n",
    "# Using Callbacks\n",
    "\n",
    "The fit() method accepts a callbacks argument that lets you specify a list of objects that Keras will call at the start and end of training, at the start and end of each epoch, and even before and after processing each batch. For example, the ModelCheckpoint callback saves checkpoints of your model at regular intervals during training, by default at the end of each epoch:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.9206 - val_loss: 0.8762\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7019 - val_loss: 0.6965\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6136 - val_loss: 0.6548\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5785 - val_loss: 0.6191\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5504 - val_loss: 0.5968\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5315 - val_loss: 0.5749\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5139 - val_loss: 0.5619\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5023 - val_loss: 0.5472\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4909 - val_loss: 0.5355\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4821 - val_loss: 0.5277\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.4586\n"
     ]
    }
   ],
   "source": [
    "# build and compile the model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model(\"my_keras_model.h5\") # rollback to best model\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 2s 3ms/step - loss: 0.4747 - val_loss: 0.5204\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4687 - val_loss: 0.5151\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4634 - val_loss: 0.5111\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4593 - val_loss: 0.5060\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4553 - val_loss: 0.5032\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4526 - val_loss: 0.4980\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4488 - val_loss: 0.4988\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4480 - val_loss: 0.4916\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4432 - val_loss: 0.4884\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4408 - val_loss: 0.4859\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4384 - val_loss: 0.4835\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4361 - val_loss: 0.4815\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4341 - val_loss: 0.4791\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4322 - val_loss: 0.4772\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4305 - val_loss: 0.4765\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4290 - val_loss: 0.4737\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4268 - val_loss: 0.4721\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4254 - val_loss: 0.4697\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4239 - val_loss: 0.4736\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4248 - val_loss: 0.4674\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4232 - val_loss: 0.4751\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4243 - val_loss: 0.4773\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4266 - val_loss: 0.4762\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4258 - val_loss: 0.4635\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4199 - val_loss: 0.4667\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4177 - val_loss: 0.4586\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4141 - val_loss: 0.4608\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4135 - val_loss: 0.4557\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4113 - val_loss: 0.4561\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4103 - val_loss: 0.4542\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4091 - val_loss: 0.4527\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4080 - val_loss: 0.4512\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4067 - val_loss: 0.4508\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4061 - val_loss: 0.4493\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4048 - val_loss: 0.4486\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4036 - val_loss: 0.4459\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4027 - val_loss: 0.4473\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4023 - val_loss: 0.4431\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4006 - val_loss: 0.4445\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3999 - val_loss: 0.4406\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3984 - val_loss: 0.4434\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3986 - val_loss: 0.4382\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3973 - val_loss: 0.4413\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3961 - val_loss: 0.4385\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3949 - val_loss: 0.4375\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3941 - val_loss: 0.4346\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3939 - val_loss: 0.4427\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3941 - val_loss: 0.4335\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3937 - val_loss: 0.4409\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3960 - val_loss: 0.4320\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3932 - val_loss: 0.4508\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3969 - val_loss: 0.4389\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4006 - val_loss: 0.4558\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4010 - val_loss: 0.4600\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4090 - val_loss: 0.4622\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4107 - val_loss: 0.4416\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4029 - val_loss: 0.4543\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4015 - val_loss: 0.4340\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3955 - val_loss: 0.4410\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3923 - val_loss: 0.4249\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3888 - val_loss: 0.4307\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3861 - val_loss: 0.4211\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3824 - val_loss: 0.4257\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3810 - val_loss: 0.4184\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3797 - val_loss: 0.4204\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3784 - val_loss: 0.4153\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3767 - val_loss: 0.4176\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3759 - val_loss: 0.4136\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3747 - val_loss: 0.4144\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3741 - val_loss: 0.4111\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3732 - val_loss: 0.4125\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3728 - val_loss: 0.4090\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3716 - val_loss: 0.4107\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3709 - val_loss: 0.4081\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3699 - val_loss: 0.4087\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3695 - val_loss: 0.4059\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3686 - val_loss: 0.4086\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3683 - val_loss: 0.4042\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3675 - val_loss: 0.4061\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3667 - val_loss: 0.4030\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3661 - val_loss: 0.4054\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3657 - val_loss: 0.4032\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3670 - val_loss: 0.4099\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3677 - val_loss: 0.4011\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3663 - val_loss: 0.4111\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3659 - val_loss: 0.4020\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3675 - val_loss: 0.4144\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3691 - val_loss: 0.4023\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3694 - val_loss: 0.4119\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3676 - val_loss: 0.4011\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3673 - val_loss: 0.4154\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3668 - val_loss: 0.4023\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3678 - val_loss: 0.4173\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3708 - val_loss: 0.4017\n",
      "162/162 [==============================] - 0s 991us/step - loss: 0.3664\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                  restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, if you use a validation set during training, you can set save_best_only=True when creating the ModelCheckpoint. In this case, it will only save your model when its performance on the validation set is the best so far. \n",
    "\n",
    "This way, you do not need to worry about training for too long and overfitting the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3664 - val_loss: 0.4098\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3676 - val_loss: 0.4017\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3680 - val_loss: 0.4145\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3705 - val_loss: 0.4025\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3702 - val_loss: 0.4185\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3701 - val_loss: 0.4045\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3735 - val_loss: 0.4209\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3746 - val_loss: 0.4028\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3688 - val_loss: 0.4170\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3670 - val_loss: 0.4005\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",\n",
    "                                                save_best_only=True)\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model(\"my_keras_model.h5\") # roll back to best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to implement early stopping is to simply use the EarlyStopping callback.\n",
    "\n",
    "t will interrupt training when it measures no progress on the validation set for a number of epochs (defined by the patience argument), and it will optionally roll back to the best model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of epochs can be set to a large value since training will stop automatically when there is no more progress. In this case, there is no need to restore the best model saved because the EarlyStopping callback will keep track of the best weights and restore them for you at the end of training.\n",
    "\n",
    "If you need extra control, you can easily write your own custom callbacks. As an example of how to do that, the following custom callback will display the ratio between the validation loss and the training loss during training (e.g., to detect overfitting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might expect, you can implement on_train_begin(), on_train_end(), on_epoch_begin(), on_epoch_end(), on_batch_begin(), and on_batch_end(). Callbacks can also be used during evaluation and predictions, should you ever need them (e.g., for debugging). For evaluation, you should implement on_test_begin(), on_test_end(), on_test_batch_begin(), or on_test_batch_end() (called by evaluate()), and for prediction you should implement on_predict_begin(), on_predict_end(), on_predict_batch_begin(), or on_predict_batch_end() (called by predict()).\n",
    "\n",
    "Now let’s take a look at one more tool you should definitely have in your toolbox when using tf.keras: TensorBoard.\n",
    "\n",
    "# Using TensorBoard for Visualization\n",
    "\n",
    "TensorBoard is a great interactive visualization tool that you can use to view the learning curves during training, compare learning curves between multiple runs, visualize the computation graph, analyze training statistics, view images generated by your model, visualize complex multidimensional data projected down to 3D and automatically clustered for you, and more! This tool is installed automatically when you install TensorFlow, so you already have it.\n",
    "\n",
    "To use it, you must modify your program so that it outputs the data you want to visualize to special binary log files called event files.\n",
    "\n",
    "Each binary data record is called a summary.\n",
    " \n",
    "The TensorBoard server will monitor the log directory, and it will automatically pick up the changes and update the visualizations: this allows you to visualize live data\n",
    "\n",
    "In general, you want to point the TensorBoard server to a root log directory and configure your program so that it writes to a different subdirectory every time it runs. This way, the same TensorBoard server instance will allow you to visualize and compare data from multiple runs of your program, without getting everything mixed up.\n",
    "\n",
    "Let’s start by defining the root log directory we will use for our TensorBoard logs, plus a small function that will generate a subdirectory path based on the current date and time so that it’s different at every run. You may want to include extra information in the log directory name, such as hyperparameter values that you are testing, to make it easier to know what you are looking at in TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir() # e.g., './my_logs/run_2019_06_07-15_15_22'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.9206 - val_loss: 0.8762\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7019 - val_loss: 0.6965\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.6136 - val_loss: 0.6548\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5785 - val_loss: 0.6191\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5504 - val_loss: 0.5968\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5315 - val_loss: 0.5749\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5139 - val_loss: 0.5619\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5023 - val_loss: 0.5472\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4909 - val_loss: 0.5355\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4821 - val_loss: 0.5277\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4748 - val_loss: 0.5201\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4685 - val_loss: 0.5150\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4635 - val_loss: 0.5089\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4591 - val_loss: 0.5054\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4554 - val_loss: 0.5018\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4520 - val_loss: 0.4984\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4488 - val_loss: 0.4940\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4459 - val_loss: 0.4914\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4429 - val_loss: 0.4914\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4420 - val_loss: 0.4856\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4388 - val_loss: 0.4887\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4386 - val_loss: 0.4867\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4379 - val_loss: 0.4875\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4375 - val_loss: 0.4776\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4323 - val_loss: 0.4799\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4309 - val_loss: 0.4732\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4273 - val_loss: 0.4744\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4263 - val_loss: 0.4697\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4237 - val_loss: 0.4693\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4224 - val_loss: 0.4675\n"
     ]
    }
   ],
   "source": [
    "# The good news is that Keras provides a nice TensorBoard() callback:\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    \n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that’s all there is to it! It could hardly be easier to use. If you run this code, the TensorBoard() callback will take care of creating the log directory for you. \n",
    "\n",
    "There’s one directory per run, each containing one subdirectory for training logs and one for validation logs. Both contain event files, but the training logs also include profiling traces: this allows TensorBoard to show you exactly how much time the model spent on each part of your model, across all your devices, which is great for locating performance bottlenecks.\n",
    "\n",
    "Next you need to start the TensorBoard server. One way to do this is by running a command in a terminal.We can also do it via Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4f8b1c83fc2ce3d5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4f8b1c83fc2ce3d5\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./my_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\my_logs\\\\run_2021_07_19-17_24_41'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we make some more sweet data \n",
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "run_logdir2 = get_run_logdir()\n",
    "run_logdir2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5766 - val_loss: 1.2938\n",
      "Epoch 2/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4695 - val_loss: 0.4236\n",
      "Epoch 3/30\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4033 - val_loss: 0.5830\n",
      "Epoch 4/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5368 - val_loss: 0.4407\n",
      "Epoch 5/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4049 - val_loss: 0.3977\n",
      "Epoch 6/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3581 - val_loss: 0.3695\n",
      "Epoch 7/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3457 - val_loss: 0.3732\n",
      "Epoch 8/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3480 - val_loss: 0.3619\n",
      "Epoch 9/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3524 - val_loss: 0.3911\n",
      "Epoch 10/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3322 - val_loss: 0.3611\n",
      "Epoch 11/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3288 - val_loss: 0.3600\n",
      "Epoch 12/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3253 - val_loss: 0.3537\n",
      "Epoch 13/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3130 - val_loss: 0.3366\n",
      "Epoch 14/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3102 - val_loss: 0.5271\n",
      "Epoch 15/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 17.6763 - val_loss: 1.3736\n",
      "Epoch 16/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 1.5728 - val_loss: 1.3455\n",
      "Epoch 17/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.3501 - val_loss: 1.3441\n",
      "Epoch 18/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.3320 - val_loss: 1.3250\n",
      "Epoch 19/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.3407 - val_loss: 1.3494\n",
      "Epoch 20/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 1.3394 - val_loss: 1.3512\n",
      "Epoch 21/30\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 1.3449 - val_loss: 1.3497\n",
      "Epoch 22/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.3460 - val_loss: 1.3497\n",
      "Epoch 23/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.3438 - val_loss: 1.3455\n",
      "Epoch 24/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.3528 - val_loss: 1.3509\n",
      "Epoch 25/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.3447 - val_loss: 1.3540\n",
      "Epoch 26/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.3451 - val_loss: 1.3499\n",
      "Epoch 27/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.3446 - val_loss: 1.3524\n",
      "Epoch 28/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.3446 - val_loss: 1.3500\n",
      "Epoch 29/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.3442 - val_loss: 1.3497\n",
      "Epoch 30/30\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.3441 - val_loss: 1.3545\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    \n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=0.05))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir2)\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can also visualize the whole graph, the learned weights (projected to 3D), or the profiling traces. The TensorBoard() callback has options to log extra data too, such as embeddings \n",
    "\n",
    "TensorFlow offers a lower-level API in the tf.summary package\n",
    "\n",
    "The following code creates a SummaryWriter using the create_file_writer() function, and it uses this writer as a context to log scalars, histograms, images, audio, and text, all of which can then be visualized using TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logdir = get_run_logdir()\n",
    "writer = tf.summary.create_file_writer(test_logdir)\n",
    "with writer.as_default():\n",
    "    for step in range(1, 1000 + 1):\n",
    "        tf.summary.scalar(\"my_scalar\", np.sin(step / 10), step=step)\n",
    "        data = (np.random.randn(100) + 2) * step / 100 # some random data\n",
    "        tf.summary.histogram(\"my_hist\", data, buckets=50, step=step)\n",
    "        images = np.random.rand(2, 32, 32, 3) # random 32×32 RGB images\n",
    "        tf.summary.image(\"my_images\", images * step / 1000, step=step)\n",
    "        texts = [\"The step is \" + str(step), \"Its square is \" + str(step**2)]\n",
    "        tf.summary.text(\"my_text\", texts, step=step)\n",
    "        sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)\n",
    "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\n",
    "        tf.summary.audio(\"my_audio\", audio, sample_rate=48000, step=step)\n",
    "# This is actually a useful visualization tool to have, even beyond TensorFlow or Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Neural Network Hyperparameters\n",
    "\n",
    "- The flexibility of neural networks is also one of their main drawbacks: there are many hyperparameters to tweak\n",
    "\n",
    " How do you know what combination of hyperparameters is the best for your task?\n",
    " \n",
    "- One option is to simply try many combinations of hyperparameters and see which one works best on the validation set (or use K-fold cross-validation).\n",
    " \n",
    "- we can use GridSearchCV or RandomizedSearchCV to explore the hyperparameter space\n",
    "\n",
    "To do this, we need to wrap our Keras models in objects that mimic regular Scikit-Learn regressors. The first step is to create a function that will build and compile a Keras model, given a set of hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate) # compiles it using an SGD optimizer \n",
    "    #configured with the specified learning rate.\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model\n",
    "# This function creates a simple Sequential model for univariate regression (only one output neuron),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, let’s create a KerasRegressor based on this build_model() function:\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KerasRegressor object is a thin wrapper around the Keras model built using build_model(). Since we did not specify any hyperparameters when creating it, it will use the default hyperparameters we defined in build_model()\n",
    "\n",
    "Now we can use this object like a regular Scikit-Learn regressor: we can train it using its fit() method, then evaluate it using its score() method, and use it to make predictions using its predict() method, as you can see in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.2039 - val_loss: 1.0548\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8834 - val_loss: 0.6566\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5769 - val_loss: 0.6048\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5399 - val_loss: 0.5707\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5145 - val_loss: 0.5464\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4966 - val_loss: 0.5310\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4833 - val_loss: 0.5178\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4733 - val_loss: 0.5067\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4645 - val_loss: 0.4986\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4581 - val_loss: 0.4913\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4525 - val_loss: 0.4873\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4483 - val_loss: 0.4827\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4442 - val_loss: 0.4779\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4411 - val_loss: 0.4753\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4381 - val_loss: 0.4723\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4350 - val_loss: 0.4690\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4320 - val_loss: 0.4666\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4291 - val_loss: 0.4643\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4271 - val_loss: 0.4610\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4246 - val_loss: 0.4598\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4227 - val_loss: 0.4566\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4205 - val_loss: 0.4545\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4187 - val_loss: 0.4522\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4163 - val_loss: 0.4511\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4146 - val_loss: 0.4488\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4126 - val_loss: 0.4469\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4109 - val_loss: 0.4444\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4095 - val_loss: 0.4429\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4076 - val_loss: 0.4432\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4063 - val_loss: 0.4399\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4048 - val_loss: 0.4389\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4032 - val_loss: 0.4369\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4015 - val_loss: 0.4349\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4004 - val_loss: 0.4348\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3993 - val_loss: 0.4329\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3975 - val_loss: 0.4323\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3968 - val_loss: 0.4296\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3957 - val_loss: 0.4279\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3941 - val_loss: 0.4279\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3928 - val_loss: 0.4262\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3920 - val_loss: 0.4257\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3914 - val_loss: 0.4226\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3900 - val_loss: 0.4223\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3887 - val_loss: 0.4241\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3879 - val_loss: 0.4199\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3871 - val_loss: 0.4192\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3859 - val_loss: 0.4200\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3846 - val_loss: 0.4180\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3843 - val_loss: 0.4151\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3833 - val_loss: 0.4154\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3827 - val_loss: 0.4159\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3819 - val_loss: 0.4131\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3810 - val_loss: 0.4132\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3798 - val_loss: 0.4124\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3795 - val_loss: 0.4107\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3793 - val_loss: 0.4091\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3778 - val_loss: 0.4099\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3766 - val_loss: 0.4098\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3770 - val_loss: 0.4091\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3760 - val_loss: 0.4064\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3760 - val_loss: 0.4063\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3743 - val_loss: 0.4068\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3745 - val_loss: 0.4051\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3727 - val_loss: 0.4064\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3731 - val_loss: 0.4030\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3720 - val_loss: 0.4034\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3717 - val_loss: 0.4029\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3715 - val_loss: 0.4022\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3699 - val_loss: 0.4026\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3703 - val_loss: 0.3993\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3688 - val_loss: 0.3985\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - ETA: 0s - loss: 0.369 - 1s 2ms/step - loss: 0.3693 - val_loss: 0.3980\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3688 - val_loss: 0.3990\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3674 - val_loss: 0.3981\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3669 - val_loss: 0.3965\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3674 - val_loss: 0.3959\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3660 - val_loss: 0.3992\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3656 - val_loss: 0.3950\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3659 - val_loss: 0.3929\n",
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3646 - val_loss: 0.3938\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3640 - val_loss: 0.3935\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3633 - val_loss: 0.3951\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3629 - val_loss: 0.3939\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3628 - val_loss: 0.3915\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3637 - val_loss: 0.3913\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3605 - val_loss: 0.3930\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3603 - val_loss: 0.3923\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3622 - val_loss: 0.3896\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3600 - val_loss: 0.3902\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3598 - val_loss: 0.3876\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3581 - val_loss: 0.3869\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3574 - val_loss: 0.3869\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3575 - val_loss: 0.3859\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3561 - val_loss: 0.3863\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3558 - val_loss: 0.3858\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3545 - val_loss: 0.3850\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3541 - val_loss: 0.3842\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3558 - val_loss: 0.3909\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3532 - val_loss: 0.3820\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3550 - val_loss: 0.3823\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3507\n",
      "WARNING:tensorflow:6 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001C3A6EACDC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100,\n",
    "              validation_data=(X_valid, y_valid),\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that any extra parameter you pass to the fit() method will get passed to the underlying Keras model. Also note that the score will be the opposite of the MSE because Scikit-Learn wants scores, not losses \n",
    "\n",
    "- We don’t want to train and evaluate a single model like this, though we want to train hundreds of variants and see which one performs best on the validation set. Since there are many hyperparameters, it is preferable to use a randomized search rather than grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that RandomizedSearchCV uses K-fold cross-validation, so it does not use X_valid and y_valid, which are only used for early stopping.\n",
    "\n",
    "The exploration may last many hours, depending on the hardware, the size of the dataset, the complexity of the model, and the values of n_iter and cv. When it’s over, you can access the best parameters found, the best score, and the trained Keras model like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rnd_search_cv.best_estimator_.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using randomized search is not too hard, and it works well for many fairly simple problems. When training is slow, however (e.g., for more complex problems with larger datasets), this approach will only explore a tiny portion of the hyperparameter space.\n",
    "\n",
    "You can partially alleviate this problem by assisting the search process manually: first run a quick random search using wide ranges of hyperparameter values, then run another search using smaller ranges of values centered on the best ones found during the first run, and so on. \n",
    "\n",
    "There are many techniques to explore a search space much more efficiently than randomly. Their core idea is simple: when a region of the space turns out to be good, it should be explored more. \n",
    "\n",
    "- Such techniques take care of the “zooming” process for you and lead to much better solutions in much less time. Here are some Python libraries you can use to optimize hyperparameters:\n",
    "\n",
    "Hyperopt\n",
    "\n",
    "    A popular library for optimizing over all sorts of complex search spaces (including real values, such as the learning rate, and discrete values, such as the number of layers).\n",
    "\n",
    "Hyperas, kopt, or Talos\n",
    "\n",
    "    Useful libraries for optimizing hyperparameters for Keras models (the first two are based on Hyperopt).\n",
    "\n",
    "Keras Tuner\n",
    "\n",
    "    An easy-to-use hyperparameter optimization library by Google for Keras models, with a hosted service for visualization and analysis.\n",
    "\n",
    "Scikit-Optimize (skopt)\n",
    "\n",
    "    A general-purpose optimization library. The BayesSearchCV class performs Bayesian optimization using an interface similar to GridSearchCV.\n",
    "\n",
    "Spearmint\n",
    "\n",
    "    A Bayesian optimization library.\n",
    "\n",
    "Hyperband\n",
    "\n",
    "    A fast hyperparameter tuning library based on the recent Hyperband paper by Lisha Li et al.\n",
    "\n",
    "Sklearn-Deap\n",
    "\n",
    "    A hyperparameter optimization library based on evolutionary algorithms, with a GridSearchCV-like interface.\n",
    "    \n",
    "Moreover, many companies offer services for hyperparameter optimization. We’ll discuss Google Cloud AI Platform’s hyperparameter tuning service in chapter 19.\n",
    "\n",
    "- Hyperparameter tuning is still an active area of research, and evolutionary algorithms are making a comeback.\n",
    "\n",
    "Google has also used an evolutionary approach, not just to search for hyperparameters but also to look for the best neural network architecture for the problem; their AutoML suite is available. Perhaps the days of building neural networks manually will soon be over?\n",
    "\n",
    "In fact, evolutionary algorithms have been used successfully to train individual neural networks, replacing the ubiquitous Gradient Descent!\n",
    "\n",
    "But despite all this exciting progress and all these tools and services, it still helps to have an idea of what values are reasonable for each hyperparameter so that you can build a quick prototype and restrict the search space. The following sections provide guidelines for choosing the number of hidden layers and neurons in an MLP and for selecting good values for some of the main hyperparameters.\n",
    "\n",
    "## Number of Hidden Layers\n",
    "\n",
    "For many problems, you can begin with a single hidden layer and get reasonable results. An MLP with just one hidden layer can theoretically model even the most complex functions, provided it has enough neurons. \n",
    "\n",
    "But for complex problems, deep networks have a much higher parameter efficiency than shallow ones: they can model complex functions using exponentially fewer neurons than shallow nets, allowing them to reach much better performance with the same amount of training data.\n",
    "\n",
    "Real-world data is often structured in such a hierarchical way, and deep neural networks automatically take advantage of this fact: lower hidden layers model low-level structures (e.g., line segments of various shapes and orientations), intermediate hidden layers combine these low-level structures to model intermediate-level structures (e.g., squares, circles), and the highest hidden layers and the output layer combine these intermediate structures to model high-level structures (e.g., faces).\n",
    "\n",
    "- Not only does this hierarchical architecture help DNNs converge faster to a good solution, but it also improves their ability to generalize to new datasets.\n",
    "\n",
    "For example, if you have already trained a model to recognize faces in pictures and you now want to train a new neural network to recognize hairstyles, you can kickstart the training by reusing the lower layers of the first network. Instead of randomly initializing the weights and biases of the first few layers of the new neural network, you can initialize them to the values of the weights and biases of the lower layers of the first network. This way the network will not have to learn from scratch all the low-level structures that occur in most pictures; it will only have to learn the higher-level structures (e.g., hairstyles). This is called **transfer learning**.\n",
    "\n",
    "\n",
    "In summary, for many problems you can start with just one or two hidden layers and the neural network will work just fine. For instance, you can easily reach above 97% accuracy on the MNIST dataset using just one hidden layer with a few hundred neurons, and above 98% accuracy using two hidden layers with the same total number of neurons, in roughly the same amount of training time.\n",
    "\n",
    "Very complex tasks, such as large image classification or speech recognition, typically require networks with dozens of layers (or even hundreds, but not fully connected ones, as we will see in Chapter 14), and they need a huge amount of training data. \n",
    "\n",
    "## Number of Neurons per Hidden Layer\n",
    "\n",
    "The number of neurons in the input and output layers is determined by the type of input and output your task requires. For example, the MNIST task requires 28 × 28 = 784 input neurons and 10 output neurons.\n",
    "\n",
    "As for the hidden layers, it used to be common to size them to form a pyramid, with fewer and fewer neurons at each layer—the rationale being that many low-level features can coalesce into far fewer high-level features. A typical neural network for MNIST might have 3 hidden layers, the first with 300 neurons, the second with 200, and the third with 100. \n",
    "\n",
    "However, this practice has been largely abandoned because it seems that using the same number of neurons in all hidden layers performs just as well in most cases, or even better; plus, there is only one hyperparameter to tune, instead of one per layer. That said, depending on the dataset, it can sometimes help to make the first hidden layer bigger than the others.\n",
    "\n",
    "Just like the number of layers, you can try increasing the number of neurons gradually until the network starts overfitting. But in practice, it’s often simpler and more efficient to pick a model with more layers and neurons than you actually need, then use early stopping and other regularization techniques to prevent it from overfitting.\n",
    "\n",
    "- **TIP**: In general you will get more bang for your buck by increasing the number of layers instead of the number of neurons per layer.\n",
    "\n",
    "## Learning Rate, Batch Size, and Other Hyperparameters\n",
    "\n",
    "The numbers of hidden layers and neurons are not the only hyperparameters you can tweak in an MLP. Here are some of the most important ones, as well as tips on how to set them:\n",
    "\n",
    "Learning rate\n",
    "\n",
    "    The learning rate is arguably the most important hyperparameter. In general, the optimal learning rate is about half of the maximum learning rate (i.e., the learning rate above which the training algorithm diverges, as we saw in Chapter 4). One way to find a good learning rate is to train the model for a few hundred iterations, starting with a very low learning rate (e.g., 10-5) and gradually increasing it up to a very large value (e.g., 10). This is done by multiplying the learning rate by a constant factor at each iteration (e.g., by exp(log(106)/500) to go from 10-5 to 10 in 500 iterations). If you plot the loss as a function of the learning rate (using a log scale for the learning rate), you should see it dropping at first. But after a while, the learning rate will be too large, so the loss will shoot back up: the optimal learning rate will be a bit lower than the point at which the loss starts to climb (typically about 10 times lower than the turning point). You can then reinitialize your model and train it normally using this good learning rate. We will look at more learning rate techniques in Chapter 11.\n",
    "\n",
    "Optimizer\n",
    "\n",
    "    Choosing a better optimizer than plain old Mini-batch Gradient Descent (and tuning its hyperparameters) is also quite important. We will see several advanced optimizers in Chapter 11.\n",
    "\n",
    "Batch size\n",
    "\n",
    "    The batch size can have a significant impact on your model’s performance and training time. The main benefit of using large batch sizes is that hardware accelerators like GPUs can process them efficiently (see Chapter 19), so the training algorithm will see more instances per second. Therefore, many researchers and practitioners recommend using the largest batch size that can fit in GPU RAM. There’s a catch, though: in practice, large batch sizes often lead to training instabilities, especially at the beginning of training, and the resulting model may not generalize as well as a model trained with a small batch size. In April 2018, Yann LeCun even tweeted “Friends don’t let friends use mini-batches larger than 32,” citing a 2018 paper by Dominic Masters and Carlo Luschi which concluded that using small batches (from 2 to 32) was preferable because small batches led to better models in less training time. Other papers point in the opposite direction, however; in 2017, papers by Elad Hoffer et al.26 and Priya Goyal et al.27 showed that it was possible to use very large batch sizes (up to 8,192) using various techniques such as warming up the learning rate (i.e., starting training with a small learning rate, then ramping it up, as we will see in Chapter 11). This led to a very short training time, without any generalization gap. So, one strategy is to try to use a large batch size, using learning rate warmup, and if training is unstable or the final performance is disappointing, then try using a small batch size instead.\n",
    "\n",
    "Activation function\n",
    "\n",
    "    We discussed how to choose the activation function earlier in this chapter: in general, the ReLU activation function will be a good default for all hidden layers. For the output layer, it really depends on your task.\n",
    "\n",
    "Number of iterations\n",
    "\n",
    "    In most cases, the number of training iterations does not actually need to be tweaked: just use early stopping instead.\n",
    "    \n",
    "- **TIP**: The optimal learning rate depends on the other hyperparameters—especially the batch size—so if you modify any hyperparameter, make sure to update the learning rate as well.\n",
    "\n",
    "For more best practices regarding tuning neural network hyperparameters, check out the excellent 2018 [paper](https://arxiv.org/abs/1803.09820) by Leslie Smith.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
