{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a01feb71",
   "metadata": {},
   "source": [
    "Convolutional neural networks (CNNs) emerged from the study of the brain’s visual cortex.  They power image search services, self-driving cars, automatic video classification systems, and more. Moreover, CNNs are not restricted to visual perception: they are also successful at many other tasks, such as voice recognition and natural language processing. However, we will focus on visual applications for now.\n",
    "\n",
    "- **NOTE**: Why not simply use a deep neural network with fully connected layers for image recognition tasks? Unfortunately, although this works fine for small images (e.g., MNIST), it breaks down for larger images because of the huge number of parameters it requires. For example, a 100 × 100–pixel image has 10,000 pixels, and if the first layer has just 1,000 neurons (which already severely restricts the amount of information transmitted to the next layer), this means a total of 10 million connections. And that’s just the first layer. CNNs solve this problem using partially connected layers and weight sharing.\n",
    "\n",
    "# Convolutional Layers\n",
    "\n",
    "The most important building block of a CNN is the convolutional layer: \n",
    "\n",
    "- neurons in the first convolutional layer are not connected to every single pixel in the input image (like they were in the layers discussed in previous chapters), but only to pixels in their receptive fields\n",
    "\n",
    "- In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer.\n",
    "\n",
    "This architecture allows the network to concentrate on small low-level features in the first hidden layer, then assemble them into larger higher-level features in the next hidden layer, and so on.\n",
    "\n",
    "This hierarchical structure is common in real-world images, which is one of the reasons why CNNs work so well for image recognition.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1402.png)\n",
    "\n",
    "- **NOTE**: All the multilayer neural networks we’ve looked at so far had layers composed of a long line of neurons, and we had to flatten input images to 1D before feeding them to the neural network. In a CNN each layer is represented in 2D, which makes it easier to match neurons with their corresponding inputs.\n",
    "\n",
    "A neuron located in row $i$, column $j$ of a given layer is connected to the outputs of the neurons in the previous layer located in rows $i$ to $i + f_h – 1$, columns $j$ to $j + f_w – 1$, \n",
    "\n",
    "- where fh and fw are the height and width of the receptive field \n",
    "\n",
    "- In order for a layer to have the same height and width as the previous layer, it is common to add zeros around the inputs, as shown in the diagram. This is called **zero padding**.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1403.png) \n",
    "\n",
    "\n",
    "It is also possible to connect a large input layer to a much smaller layer by spacing out the receptive field (shown below). This dramatically reduces the model’s computational complexity.\n",
    "\n",
    "- The shift from one receptive field to the next is called the **stride**\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1404.png)\n",
    "\n",
    "In the diagram, a 5 × 7 input layer (plus zero padding) is connected to a 3 × 4 layer, using 3 × 3 receptive fields and a stride of 2 (in this example the stride is the same in both directions, but it does not have to be so). A neuron located in row $i$, column $j$ in the upper layer is connected to the outputs of the neurons in the previous layer located in rows $i × s_h$ to $i × s_h + f_h – 1$, columns $j × s_w$ to $j × s_w + f_w – 1$, \n",
    "\n",
    "- where $s_h$ and $s_w$ are the vertical and horizontal strides.\n",
    "\n",
    "# Filters\n",
    "\n",
    "- A neuron’s weights can be represented as a small image the size of the receptive field. These sets of weights, are called **filters** (or convolution kernels).\n",
    "\n",
    "- a layer full of neurons using the same filter outputs a **feature map**, which highlights the areas in an image that activate the filter the most.\n",
    "\n",
    "Each filter will focus on a field. A field is for 2-d is commonly horizontal or vertical. \n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1405.png)\n",
    "\n",
    "- a layer full of neurons using the same filter outputs a feature map, which highlights the areas in an image that activate the filter the most. \n",
    "\n",
    "# Stacking Multiple Feature Maps\n",
    "\n",
    "So far we have represented CNNs with 2-d layers, in reality they have multiple filters, outputs one feature map per filter.\n",
    "\n",
    "- It has one neuron per pixel in each feature map, and all neurons within a given feature map share the same parameters (i.e., the same weights and bias term). Neurons in different feature maps use different parameters\n",
    "\n",
    "A neuron’s receptive field is the same as described earlier, but it extends across all the previous layers’ feature maps. In short, a convolutional layer simultaneously applies multiple trainable filters to its inputs, making it capable of detecting multiple features anywhere in its inputs.\n",
    "\n",
    "- **NOTE**: The fact that all neurons in a feature map share the same parameters dramatically reduces the number of parameters in the model. Once the CNN has learned to recognize a pattern in one location, it can recognize it in any other location. In contrast, once a regular DNN has learned to recognize a pattern in one location, it can recognize it only in that particular location.\n",
    "\n",
    "Input images are also composed of multiple sublayers: one per color channel. There are typically three: red, green, and blue (RGB). Grayscale images have just one channel, but some images may have much more—for example, satellite images that capture extra light frequencies (such as infrared).\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1406.png)\n",
    "\n",
    "\n",
    "Specifically, a neuron located in row $i$, column $j$ of the feature map $k$ in a given convolutional layer $l$ is connected to the outputs of the neurons in the previous layer $l – 1$, located in rows $i × s_h$ to $i × s_h + f_h – 1$ and columns $j × s_w$ to $j × s_w + f_w – 1$, across all feature maps (in layer $l – 1$). Note that all neurons located in the same row $i$ and column $j$ but in different feature maps are connected to the outputs of the exact same neurons in the previous layer.\n",
    "\n",
    "With the information we have covered we can now compute the output of a convolutional layer, in the equation below. All it does is calculate the weighted sum of all the inputs, plus the bias term.\n",
    "\n",
    "**Equation 14-1. Computing the output of a neuron in a convolutional layer**\n",
    "\n",
    "$$\n",
    "z_{i, j, k}=b_{k}+\\sum_{u=0}^{f_{h}-1} \\sum_{v=0}^{f_{w}-1} \\sum_{k l=0}^{f_{n^{\\prime}}-1} x_{i^{\\prime}, j^{\\prime}, k^{\\prime}} \\times w_{u, v, k^{\\prime}, k} \\quad \\text { with }\\left\\{\\begin{array}{l}\n",
    "i /=i \\times s_{h}+u \\\\\n",
    "j^{\\prime}=j \\times s_{w}+v\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "\n",
    "In this equation:\n",
    "\n",
    "- $z_{i, j, k}$ is the output of the neuron located in row $i$, column $j$ in feature map $k$ of the convolutional layer (layer $l$).\n",
    "\n",
    "- As explained earlier, $s_h$ and $s_w$ are the vertical and horizontal strides, $f_h$ and $f_w$ are the height and width of the receptive field, and $f_{n′}$ is the number of feature maps in the previous layer (layer $l – 1$).\n",
    "\n",
    "- $x_{i′, j′, k′}$ is the output of the neuron located in layer $l – 1$, row $i′$, column $j′$, feature map $k′$ (or channel $k′$ if the previous layer is the input layer).\n",
    "\n",
    "- $b_k$ is the bias term for feature map $k$ (in layer $l$). You can think of it as a knob that tweaks the overall brightness of the feature map $k$.\n",
    "\n",
    "- $w_{u, v, k′ ,k}$ is the connection weight between any neuron in feature map $k$ of the layer $l$ and its input located at row $u$, column $v$ (relative to the neuron’s receptive field), and feature map $k′$.\n",
    "\n",
    "# TensorFlow Implementation\n",
    "\n",
    "In TensorFlow, each input image is typically represented as a 3D tensor of shape [height, width, channels]. A mini-batch is represented as a 4D tensor of shape [mini-batch size, height, width, channels]. The weights of a convolutional layer are represented as a 4D tensor of shape [$f_h, f_w, f_n′, f_n$]. The bias terms of a convolutional layer are simply represented as a 1D tensor of shape [$f_n$].\n",
    "\n",
    "Let’s look at a simple example. It loads two images, creates two filters and applies them to both images, and displays one of the feature maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1030430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_sample_image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load sample images\n",
    "china = load_sample_image(\"china.jpg\") / 255\n",
    "flower = load_sample_image(\"flower.jpg\") / 255\n",
    "images = np.array([china, flower])\n",
    "batch_size, height, width, channels = images.shape\n",
    "\n",
    "# Create 2 filters\n",
    "filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)\n",
    "filters[:, 3, :, 0] = 1  # vertical line\n",
    "filters[3, :, :, 1] = 1  # horizontal line\n",
    "\n",
    "outputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\")\n",
    "\n",
    "plt.imshow(outputs[0, :, :, 1], cmap=\"gray\") # plot 1st image's 2nd feature map\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d9711e",
   "metadata": {},
   "source": [
    "Let’s go through this code:\n",
    "\n",
    "- The pixel intensity for each color channel is represented as a byte from 0 to 255, so we scale these features simply by dividing by 255, to get floats ranging from 0 to 1.\n",
    "\n",
    "- Then we create two 7 × 7 filters (one with a vertical white line in the middle, and the other with a horizontal white line in the middle).\n",
    "\n",
    "- We apply them to both images using the tf.nn.conv2d() function, which is part of TensorFlow’s low-level Deep Learning API. In this example, we use zero padding (padding=\"SAME\") and a stride of 1.\n",
    "\n",
    "- Finally, we plot one of the resulting feature maps.\n",
    "\n",
    "\n",
    "tf.nn.conv2d() line deserves a bit more explanation:\n",
    "\n",
    "- images is the input mini-batch (a 4D tensor, as explained earlier).\n",
    "\n",
    "- filters is the set of filters to apply (also a 4D tensor, as explained earlier).\n",
    "\n",
    "- strides is equal to 1, but it could also be a 1D array with four elements, where the two central elements are the vertical and horizontal strides (sh and sw). The first and last elements must currently be equal to 1. They may one day be used to specify a batch stride (to skip some instances) and a channel stride (to skip some of the previous layer’s feature maps or channels).\n",
    "\n",
    "- padding must be either \"SAME\" or \"VALID\":\n",
    "\n",
    " - If set to \"SAME\", the convolutional layer uses zero padding if necessary. The output size is set to the number of input neurons divided by the stride, rounded up. For example, if the input size is 13 and the stride is 5 (see Figure below), then the output size is 3 (i.e., 13 / 5 = 2.6, rounded up to 3). Then zeros are added as evenly as possible around the inputs, as needed. When strides=1, the layer’s outputs will have the same spatial dimensions (width and height) as its inputs, hence the name same.\n",
    "\n",
    " - If set to \"VALID\", the convolutional layer does not use zero padding and may ignore some rows and columns at the bottom and right of the input image, depending on the stride, as shown below (for simplicity, only the horizontal dimension is shown here, but of course the same logic applies to the vertical dimension). This means that every neuron’s receptive field lies strictly within valid positions inside the input (it does not go out of bounds), hence the name valid.\n",
    " \n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1407.png)\n",
    "\n",
    "In this example we manually defined the filters, but in a real CNN you would normally define filters as trainable variables so the neural net can learn which filters work best, as explained earlier. Instead of manually creating the variables, use the keras.layers.Conv2D layer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba353c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "conv = keras.layers.Conv2D(filters=32, kernel_size=3, strides=1,\n",
    "                           padding=\"same\", activation=\"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2ed461",
   "metadata": {},
   "source": [
    "This code creates a Conv2D layer with 32 filters, each 3 × 3, using a stride of 1 (both horizontally and vertically) and \"same\" padding, and applying the ReLU activation function to its outputs. \n",
    "\n",
    "convolutional layers have quite a few hyperparameters: you must choose the number of filters, their height and width, the strides, and the padding type. We can use cross-validation to find the right hyperparameters, or we can follow a guide of \"best\" practices (discussed later). \n",
    "\n",
    "# Memory Requirements\n",
    "\n",
    "Another problem with CNNs is that the convolutional layers require a huge amount of RAM. This is especially true during training, because the reverse pass of backpropagation requires all the intermediate values computed during the forward pass.\n",
    "\n",
    "For example, consider a convolutional layer with 5 × 5 filters, outputting 200 feature maps of size 150 × 100, with stride 1 and \"same\" padding. If the input is a 150 × 100 RGB image (three channels), then the number of parameters is (5 × 5 × 3 + 1) × 200 = 15,200 (the + 1 corresponds to the bias terms) , which is fairly small compared to a fully connected layer.  However, each of the 200 feature maps contains 150 × 100 neurons, and each of these neurons needs to compute a weighted sum of its 5 × 5 × 3 = 75 inputs: that’s a total of 225 million float multiplications.\n",
    "\n",
    "Not as bad as a fully connected layer, but still quite computationally intensive. Moreover, if the feature maps are represented using 32-bit floats, then the convolutional layer’s output will occupy 200 × 150 × 100 × 32 = 96 million bits (12 MB) of RAM. And that’s just for one instance. If a training batch contains 100 instances, then this layer will use up 1.2 GB of RAM!\n",
    "\n",
    "During inference (i.e., when making a prediction for a new instance) the RAM occupied by one layer can be released as soon as the next layer has been computed, so you only need as much RAM as required by two consecutive layers. But **during training** everything computed during the forward pass needs to be preserved for the reverse pass, so the amount of RAM needed is (at least) the total amount of RAM required by all layers.\n",
    "\n",
    "- **TIP** If training crashes because of an out-of-memory error, you can try reducing the mini-batch size. Alternatively, you can try reducing dimensionality using a stride, or removing a few layers. Or you can try using 16-bit floats instead of 32-bit floats. Or you could distribute the CNN across multiple devices.\n",
    "\n",
    "# Pooling Layers\n",
    "\n",
    "Once you understand how convolutional layers work, the pooling layers are quite easy to grasp. Their goal is to subsample (i.e., shrink) the input image in order to reduce the computational load, the memory usage, and the number of parameters (thereby limiting the risk of overfitting).\n",
    "\n",
    "- Each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before.\n",
    "\n",
    "- A pooling neuron has no weights; all it does is aggregate the inputs using an aggregation function such as the max or mean.\n",
    "\n",
    "\n",
    "Below we illustrate a max pooling layer.  In this example, we use a 2 × 2 pooling kernel, with a stride of 2 and no padding. Only the max input value in each receptive field makes it to the next layer, while the other inputs are dropped.\n",
    "\n",
    "For example, in the lower-left receptive field in the Figure below, the input values are 1, 5, 3, 2, so only the max value, 5, is propagated to the next layer. Because of the stride of 2, the output image has half the height and half the width of the input image (rounded down since we use no padding).\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1408.png)\n",
    "\n",
    "- **NOTE**: A pooling layer typically works on every input channel independently, so the output depth is the same as the input depth.\n",
    "\n",
    "- Pooling layers add some level of invariance to small translations\n",
    "\n",
    "We illustrate the invariance below.  \n",
    "\n",
    "Here we assume that the bright pixels have a lower value than dark pixels, and we consider three images (A, B, C) going through a max pooling layer with a 2 × 2 kernel and stride 2. Images B and C are the same as image A, but shifted by one and two pixels to the right. As you can see, the outputs of the max pooling layer for images A and B are identical. This is what translation invariance means. For image C, the output is different: it is shifted one pixel to the right (but there is still 50% invariance). \n",
    "\n",
    "By inserting a max pooling layer every few layers in a CNN, it is possible to get some level of translation invariance at a larger scale. Moreover, max pooling offers a small amount of rotational invariance and a slight scale invariance. Such invariance (even if it is limited) can be useful in cases where the prediction should not depend on these details, such as in classification tasks.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1409.png)\n",
    "\n",
    "However, max pooling has some downsides too. Firstly, it is obviously very destructive: even with a tiny 2 × 2 kernel and a stride of 2, the output will be two times smaller in both directions (so its area will be four times smaller), simply dropping 75% of the input values. And in some applications, invariance is not desirable. \n",
    "\n",
    "Take semantic segmentation (the task of classifying each pixel in an image according to the object that pixel belongs to, which we’ll explore later in this chapter): obviously, if the input image is translated by one pixel to the right, the output should also be translated by one pixel to the right. The goal in this case is equivariance, not invariance: a small change to the inputs should lead to a corresponding small change in the output.\n",
    "\n",
    "# TensorFlow Implementation\n",
    "\n",
    "Implementing a max pooling layer in TensorFlow is quite easy. The following code creates a max pooling layer using a 2 × 2 kernel. The strides default to the kernel size, so this layer will use a stride of 2 (both horizontally and vertically). By default, it uses \"valid\" padding (i.e., no padding at all):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e92f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool = keras.layers.MaxPool2D(pool_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8ef6b1",
   "metadata": {},
   "source": [
    "To create an average pooling layer, just use AvgPool2D instead of MaxPool2D. As you might expect, it works exactly like a max pooling layer, except it computes the mean rather than the max. Average pooling layers used to be very popular, but people mostly _use max pooling layers now, as they generally perform better_. \n",
    "\n",
    "This may seem surprising, since computing the mean generally loses less information than computing the max. But on the other hand, max pooling preserves only the strongest features, getting rid of all the meaningless ones, so the next layers get a cleaner signal to work with. Moreover, max pooling offers stronger translation invariance than average pooling, and it requires slightly less compute.\n",
    "\n",
    "- Note that max pooling and average pooling can be performed along the depth dimension rather than the spatial dimensions, although this is not as common. This can allow the CNN to learn to be invariant to various features.\n",
    "\n",
    "For example, it could learn multiple filters, each detecting a different rotation of the same pattern (such as hand-written digits; see Figure 14-10), and the depthwise max pooling layer would ensure that the output is the same regardless of the rotation. The CNN could similarly learn to be invariant to anything else: thickness, brightness, skew, color, and so on.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1410.png)\n",
    "\n",
    "Keras does not include a depthwise max pooling layer, but TensorFlow’s low-level Deep Learning API does: just use the tf.nn.max_pool() function, and specify the kernel size and strides as 4-tuples (i.e., tuples of size 4).\n",
    "\n",
    "The first three values of each should be 1: this indicates that the kernel size and stride along the batch, height, and width dimensions should be 1. The last value should be whatever kernel size and stride you want along the depth dimension—for example, 3 (this must be a divisor of the input depth; it will not work if the previous layer outputs 20 feature maps, since 20 is not a multiple of 3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df7b76a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.nn.max_pool(images,\n",
    "                        ksize=(1, 1, 1, 3),\n",
    "                        strides=(1, 1, 1, 3),\n",
    "                        padding=\"VALID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6d9b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to include this as a layer in your Keras models, \n",
    "# wrap it in a Lambda layer (or create a custom Keras layer):\n",
    "depth_pool = keras.layers.Lambda(\n",
    "    lambda X: tf.nn.max_pool(X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3),\n",
    "                             padding=\"VALID\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d93fc5",
   "metadata": {},
   "source": [
    "- One last type of pooling layer that you will often see in modern architectures is the global average pooling layer. It works very differently: \n",
    "\n",
    "all it does is compute the mean of each entire feature map (it’s like an average pooling layer using a pooling kernel with the same spatial dimensions as the inputs). This means that it just outputs a single number per feature map and per instance. Although this is of course extremely destructive (most of the information in the feature map is lost), it can be useful as the output layer, as we will see later in this chapter. To create such a layer, simply use the keras.layers.GlobalAvgPool2D class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80f62aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_avg_pool = keras.layers.GlobalAvgPool2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ce2c8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It’s equivalent to this simple Lambda layer, which computes the mean over the spatial dimensions (height and width):\n",
    "global_avg_pool = keras.layers.Lambda(lambda X: tf.reduce_mean(X, axis=[1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9877187f",
   "metadata": {},
   "source": [
    "# CNN Architectures\n",
    "\n",
    "Typical CNN architectures stack a few convolutional layers (each one generally followed by a ReLU layer), then a pooling layer, then another few convolutional layers (+ReLU), then another pooling layer, and so on.\n",
    "\n",
    "Images get smaller and smaller but the this means it gets deeper and deeper. \n",
    "\n",
    "At the top of the stack, a regular feedforward neural network is added, composed of a few fully connected layers (+ReLUs), and the final layer outputs the prediction (e.g., a softmax layer that outputs estimated class probabilities).\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1411.png)\n",
    "\n",
    "- **TIP**:  A common mistake is to use convolution kernels that are too large. For example, instead of using a convolutional layer with a 5 × 5 kernel, stack two layers with 3 × 3 kernels: it will use fewer parameters and require fewer computations, and it will usually perform better. One exception is for the first convolutional layer: it can typically have a large kernel (e.g., 5 × 5), usually with a stride of 2 or more: this will reduce the spatial dimension of the image without losing too much information, and since the input image only has three channels in general, it will not be too costly.\n",
    "\n",
    "Here is how you can implement a simple CNN to tackle the Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42b619fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(64, 7, activation=\"relu\", padding=\"same\",\n",
    "                        input_shape=[28, 28, 1]),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(64, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fee50f7",
   "metadata": {},
   "source": [
    "Let’s go through this model:\n",
    "\n",
    "- The first layer uses 64 fairly large filters (7 × 7) but only stride 1 because the input images are not very large. It also sets input_shape=[28, 28, 1], because the images are 28 × 28 pixels, with a single color channel (i.e., grayscale).\n",
    "\n",
    "- Next we have a max pooling layer which uses a pool size of 2, so it divides each spatial dimension by a factor of 2.\n",
    "\n",
    "- Then we repeat the same structure twice: two convolutional layers followed by a max pooling layer. For larger images, we could repeat this structure several more times (the number of repetitions is a hyperparameter you can tune).\n",
    "\n",
    "- Note that the number of filters grows as we climb up the CNN toward the output layer (it is initially 64, then 128, then 256): it makes sense for it to grow, since the number of low-level features is often fairly low (e.g., small circles, horizontal lines), but there are many different ways to combine them into higher-level features. It is a common practice to double the number of filters after each pooling layer: since a pooling layer divides each spatial dimension by a factor of 2, we can afford to double the number of feature maps in the next layer without fear of exploding the number of parameters, memory usage, or computational load.\n",
    "\n",
    "- Next is the fully connected network, composed of two hidden dense layers and a dense output layer. Note that we must flatten its inputs, since a dense network expects a 1D array of features for each instance. We also add two dropout layers, with a dropout rate of 50% each, to reduce overfitting.\n",
    "\n",
    "Recall that dropout is a regularization technique that removes neurons for a few iterations. This decrease dependency and improves accuracy. \n",
    "\n",
    "In the book, this model gets 92% acccuracy. \n",
    "\n",
    "Next we observe the architexture of CNNs.\n",
    "\n",
    "# LeNet-5\n",
    "\n",
    "This is the most widely known CNN architecture, created by LeCunn and has been widely used to hand-digit regonizition. \n",
    "\n",
    "See the book for the architecture or give it a google search. \n",
    "\n",
    "- MNIST images are 28 × 28 pixels, but they are zero-padded to 32 × 32 pixels and normalized before being fed to the network. The rest of the network does not use any padding, which is why the size keeps shrinking as the image progresses through the network.\n",
    "\n",
    "- The average pooling layers are slightly more complex than usual: each neuron computes the mean of its inputs, then multiplies the result by a learnable coefficient (one per map) and adds a learnable bias term (again, one per map), then finally applies the activation function.\n",
    "\n",
    "- Most neurons in C3 maps are connected to neurons in only three or four S2 maps (instead of all six S2 maps). See table 1 (page 8) in the original paper for details.\n",
    "\n",
    "- The output layer is a bit special: instead of computing the matrix multiplication of the inputs and the weight vector, each neuron outputs the square of the Euclidian distance between its input vector and its weight vector. Each output measures how much the image belongs to a particular digit class. The cross-entropy cost function is now preferred, as it penalizes bad predictions much more, producing larger gradients and converging faster.\n",
    "\n",
    "Please visit LeCunn's [website](http://yann.lecun.com/exdb/lenet/index.html) for more. \n",
    "\n",
    "# AlexNet\n",
    "\n",
    "This won the 2012 ImageNet ILSVRC challenge by a large margin: with a top five error rate of 17%. \n",
    "\n",
    "It is similar to LeNet-5, only much larger and deeper, and it was the first to stack convolutional layers directly on top of one another, instead of stacking a pooling layer on top of each convolutional layer.\n",
    "\n",
    "Please see the book for the architecture table. \n",
    "\n",
    "To reduce overfitting, the authors used two regularization techniques. First, they applied dropout with a 50% dropout rate during training to the outputs of layers F9 and F10. Second, they performed data augmentation by randomly shifting the training images by various offsets, flipping them horizontally, and changing the lighting conditions.\n",
    "\n",
    "___\n",
    "\n",
    "## DATA AUGMENTATION\n",
    "\n",
    "- Data augmentation artificially increases the size of the training set by generating many realistic variants of each training instance. This reduces overfitting, making this a regularization technique. \n",
    "\n",
    "The generated instances should be as realistic as possible: ideally, given an image from the augmented training set, a human should not be able to tell whether it was augmented or not. Simply adding white noise will not help; the modifications should be learnable (white noise is not).\n",
    "\n",
    "For example, you can slightly shift, rotate, and resize every picture in the training set by various amounts and add the resulting pictures to the training set\n",
    "\n",
    "This forces the model to be more tolerant to variations in the position, orientation, and size of the objects in the pictures. For a model that’s more tolerant of different lighting conditions, you can similarly generate many images with various contrasts. In general, you can also flip the pictures horizontally (except for text, and other asymmetrical objects). By combining these transformations, you can greatly increase the size of your training set.\n",
    "\n",
    "\n",
    "___ \n",
    "\n",
    "AlexNet also uses a competitive normalization step immediately after the ReLU step of layers C1 and C3, called **local response normalization (LRN)**: the most strongly activated neurons inhibit other neurons located at the same position in neighboring feature maps (such competitive activation has been observed in biological neurons).\n",
    "\n",
    "his encourages different feature maps to specialize, pushing them apart and forcing them to explore a wider range of features, ultimately improving generalization. Equation 14-2 shows how to apply LRN.\n",
    "\n",
    "**Equation 14-2. Local response normalization (LRN)**\n",
    "\n",
    "$$\n",
    "b_{i}=a_{i}\\left(k+\\alpha \\sum_{j=j_{\\text {low }}}^{j_{\\text {high }}} a_{j}{ }^{2}\\right)^{-\\beta} \\text { with }\\left\\{\\begin{array}{l}\n",
    "j_{\\text {high }}=\\min \\left(i+\\frac{r}{2}, f_{n}-1\\right) \\\\\n",
    "j_{\\text {low }}=\\max \\left(0, i-\\frac{r}{2}\\right)\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "- $b_i$ is the normalized output of the neuron located in feature map $i$, at some row $u$ and column $v$ (note that in this equation we consider only neurons located at this row and column, so u and v are not shown).\n",
    "\n",
    "- $a_i$ is the activation of that neuron after the ReLU step, but before normalization.\n",
    "\n",
    "- $k$, $α$, $β$, and $r$ are hyperparameters. $k$ is called the bias, and $r$ is called the depth radius.\n",
    "\n",
    "- $f_n$ is the number of feature maps.\n",
    "\n",
    "\n",
    "For example, if r = 2 and a neuron has a strong activation, it will inhibit the activation of the neurons located in the feature maps immediately above and below its own.\n",
    "\n",
    "In AlexNet, the hyperparameters are set as follows: r = 5, α = 0.0001, β = 0.75, and k = 2. This step can be implemented using the tf.nn.local_response_normalization() function (which you can wrap in a Lambda layer if you want to use it in a Keras model).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f4296f",
   "metadata": {},
   "source": [
    "\n",
    "# GoogLeNet\n",
    "\n",
    "This won in 2014 and is much deeper than prior CNNs. It accomplishes this with _inception modules_.\n",
    "\n",
    "Inception module architecture will be looked at in the figure below. \n",
    "\n",
    "- Notataion: The figure below will describe the layers in a unqiue way. The notation “3 × 3 + 1(S)” means that the layer uses a 3 × 3 kernel, stride 1, and \"same\" padding.\n",
    "\n",
    "- Note: that the second set of convolutional layers uses different kernel sizes (1 × 1, 3 × 3, and 5 × 5), allowing them to capture patterns at different scales.\n",
    "\n",
    "- Note: that every single layer uses a stride of 1 and \"same\" padding (even the max pooling layer), so their outputs all have the same height and width as their inputs. This makes it possible to concatenate all the outputs along the depth dimension in the final _depth concatenation layer_ (i.e., stack the feature maps from all four top convolutional layers).\n",
    "\n",
    "This concatenation layer can be implemented in TensorFlow using the tf.concat() operation, with axis=3 (the axis is the depth).\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1413.png)\n",
    "\n",
    "What's up with the 1 by 1 kernels? Looking at a single pixel cannot serve any purspose, right? Wrong! \n",
    "\n",
    "- Although they cannot capture spatial patterns, they can capture patterns along the depth dimension.\n",
    "\n",
    "- They are configured to output fewer feature maps than their inputs, so they serve as bottleneck layers, meaning they reduce dimensionality. This cuts the computational cost and the number of parameters, speeding up training and improving generalization.\n",
    "- Each pair of convolutional layers ([1 × 1, 3 × 3] and [1 × 1, 5 × 5]) acts like a single powerful convolutional layer, capable of capturing more complex patterns. Indeed, instead of sweeping a simple linear classifier across the image (as a single convolutional layer does), this pair of convolutional layers sweeps a two-layer neural network across the image.\n",
    "\n",
    "\n",
    "- **Warning**:The number of convolutional kernels for each convolutional layer is a hyperparameter. Unfortunately, this means that you have six more hyperparameters to tweak for every inception layer you add.\n",
    "  \n",
    "Now we look at the GoogLeNet architecture.\n",
    "\n",
    "- The architecture is so deep that it has to be represented in three columns, but GoogLeNet is actually one tall stack, including nine inception modules\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1414.png)\n",
    "\n",
    "Let’s go through this network:\n",
    "\n",
    "- The first two layers divide the image’s height and width by 4 (so its area is divided by 16), to reduce the computational load. The first layer uses a large kernel size so that much of the information is preserved.\n",
    "\n",
    "- Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier).\n",
    "\n",
    "- Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional layer.\n",
    "\n",
    "- Again, a local response normalization layer ensures that the previous layers capture a wide variety of patterns.\n",
    "\n",
    "- Next, a max pooling layer reduces the image height and width by 2, again to speed up computations.\n",
    "\n",
    "- Then comes the tall stack of nine inception modules, interleaved with a couple max pooling layers to reduce dimensionality and speed up the net.\n",
    "\n",
    "- Next, the global average pooling layer outputs the mean of each feature map: this drops any remaining spatial information, which is fine because there was not much spatial information left at that point. Indeed, GoogLeNet input images are typically expected to be 224 × 224 pixels, so after 5 max pooling layers, each dividing the height and width by 2, the feature maps are down to 7 × 7. Moreover, it is a classification task, not localization, so it does not matter where the object is. Thanks to the dimensionality reduction brought by this layer, there is no need to have several fully connected layers at the top of the CNN (like in AlexNet), and this considerably reduces the number of parameters in the network and limits the risk of overfitting.\n",
    "\n",
    "- The last layers are self-explanatory: dropout for regularization, then a fully connected layer with 1,000 units (since there are 1,000 classes) and a softmax activation function to output estimated class probabilities.\n",
    "\n",
    "\n",
    "This diagram is slightly simplified: the original GoogLeNet architecture also included two auxiliary classifiers plugged on top of the third and sixth inception modules. They were both composed of one average pooling layer, one convolutional layer, two fully connected layers, and a softmax activation layer. During training, their loss (scaled down by 70%) was added to the overall loss. The goal was to fight the vanishing gradients problem and regularize the network. However, it was later shown that their effect was relatively minor.\n",
    "\n",
    "Google researches went on, publishing a few more architectures. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ac12d9",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "\n",
    "**Residual Network** won in 2015 with a top five error rate of 3.6%. This architecture confirmed the trend that networks are becoming deeper and deeper, with fewer parameters. It accomplished it's depth by using skip connections. \n",
    "\n",
    "- **Skip connections** (also called shortcut connections): the signal feeding into a layer is also added to the output of a layer located a bit higher up the stack.\n",
    "\n",
    "Let's cover why it is useful.\n",
    "\n",
    "When training a neural network, the goal is to make it model a target function h(x). If you add the input x to the output of the network(you add a skip connection)  then the network will be forced to model f(x) = h(x) – x rather than h(x). This is called **residual learning**. \n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1415.png)\n",
    "\n",
    "When you initialize a regular neural network, its weights are close to zero, so the network just outputs values close to zero. \n",
    "\n",
    "- If you add a skip connection, the resulting network just outputs a copy of its inputs; in other words, it initially models the identity function. If the target function is fairly close to the identity function (which is often the case), this will speed up training considerably.\n",
    "\n",
    "Moreover, if you add many skip connections, the network can start making progress even if several layers have not started learning yet. The deep residual network can be seen as a stack of residual units (RUs), where each residual unit is a small neural network with a skip connection.\n",
    "\n",
    "Lets look at the difference between a regular network and a network with skip connections: \n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1416.png)\n",
    "\n",
    "Now lets look at a ResNet architecture. It's similar to GoogLeNet with no drop layer and a simple very deep stack of residual units. \n",
    "\n",
    "\n",
    "- Each residual unit is composed of two convolutional layers (and no pooling layer!), with Batch Normalization (BN) and ReLU activation, using 3 × 3 kernels and preserving spatial dimensions (stride 1, \"same\" padding).\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1417.png)\n",
    "\n",
    "Note that the number of feature maps is doubled every few residual units, at the same time as their height and width are halved (using a convolutional layer with stride 2). When this happens, the inputs cannot be added directly to the outputs of the residual unit because they don’t have the same shape (for example, this problem affects the skip connection represented by the dashed arrow above). \n",
    "\n",
    "To solve this problem, the inputs are passed through a 1 × 1 convolutional layer with stride 2 and the right number of output feature maps (see Figure below)\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1418.png)\n",
    "\n",
    "ResNet-34 is the ResNet with 34 layers (only counting the convolutional layers and the fully connected layer) containing 3 residual units that output 64 feature maps, 4 RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps. We will implement this architecture later in this chapter.\n",
    "\n",
    "ResNets deeper than that use different residual units. \n",
    "\n",
    "- **NOTE**: oogle’s Inception-v4 architecture merged the ideas of GoogLeNet and ResNet and achieved a top-five error rate of close to 3% on ImageNet classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ec7967",
   "metadata": {},
   "source": [
    "# Xception\n",
    "\n",
    "Another variant of the GoogLeNet architecture is worth noting: Xception (which stands for Extreme Inception). Like inception-v4 it merges ResNet and GoogLeNet, but it replaces the inception modules with a special type of layer called a **depthwise separable convolution layer** (or separable convolution layer for short)\n",
    "\n",
    "A regular convolutional layer uses filters that try to simultaneously capture spatial patterns (e.g., an oval) and cross-channel patterns (e.g., mouth + nose + eyes = face)\n",
    "\n",
    "- **separable convolutional layers**: makes the strong assumption that spatial patterns and cross-channel patterns can be modeled separately\n",
    "\n",
    "Thus, it is composed of two parts: the first part applies a single spatial filter for each input feature map, then the second part looks exclusively for cross-channel patterns—it is just a regular convolutional layer with 1 × 1 filters.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1419.png)\n",
    "\n",
    "- Seperable convolution layers have only one spatial filter per channel, avoid using them. An example of this is the input layer. \n",
    "\n",
    "To gain greater access to spatial filters the Xception architecture starts with 2 regular convolutional layers, but then the rest of the architecture uses only separable convolutions (34 in all), plus a few max pooling layers and the usual final layers (a global average pooling layer and a dense output layer).\n",
    "\n",
    "- an inception module contains convolutional layers with 1 × 1 filters: these look exclusively for cross-channel patterns\n",
    "\n",
    "\n",
    "- **TIP**: Separable convolutional layers use fewer parameters, less memory, and fewer computations than regular convolutional layers, and in general they even perform better, so you should consider using them by default (except after layers with few channels).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14d1cb7",
   "metadata": {},
   "source": [
    "# SENet\n",
    "\n",
    "- the **Squeeze-and-Excitation Network (SENet)**:  extends existing architectures such as inception networks and ResNets, and boosts their performance.\n",
    " \n",
    "The boost comes from the fact that a SENet adds a small neural network, called an SE block, to every unit in the original architecture as shown below  \n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1420.png)\n",
    "\n",
    "- An SE block analyzes the output of the unit it is attached to, focusing exclusively on the depth dimension (it does not look for any spatial pattern), and it learns which features are usually most active together.\n",
    "\n",
    "It then uses this information to recalibrate the feature maps, as shown below. \n",
    "\n",
    "- For example, an SE block may learn that mouths, noses, and eyes usually appear together in pictures: if you see a mouth and a nose, you should expect to see eyes as well. \n",
    "\n",
    "So if the block sees a strong activation in the mouth and nose feature maps, but only mild activation in the eye feature map, it will boost the eye feature map (more accurately, it will reduce irrelevant feature maps). If the eyes were somewhat confused with something else, this feature map recalibration will help resolve the ambiguity.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1421.png)\n",
    "\n",
    "An SE block is composed of just three layers: a global average pooling layer, a hidden dense layer using the ReLU activation function, and a dense output layer using the sigmoid activation function.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1422.png)\n",
    "\n",
    "The global average pooling layer computes the mean activation for each feature map.\n",
    "\n",
    "- The next layer is where the “squeeze” happens: this layer has significantly fewer than 256 neurons—typically 16 times fewer than the number of feature maps (e.g., 16 neurons)—so the 256 numbers get compressed into a small vector. This is a low-dimensional vector representation (i.e., an embedding) of the distribution of feature responses.\n",
    "\n",
    "- This bottleneck step forces the SE block to learn a general representation of the feature combinations.  \n",
    "\n",
    "Finally, the output layer takes the embedding and outputs a recalibration vector containing one number per feature map (e.g., 256), each between 0 and 1. The feature maps are then multiplied by this recalibration vector, so irrelevant features (with a low recalibration score) get scaled down while relevant features (with a recalibration score close to 1) are left alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018a78b3",
   "metadata": {},
   "source": [
    "# Implementing a ResNet-34 CNN Using Keras\n",
    "\n",
    "Most CNN architectures described so far are fairly straightforward to implement (although generally you would load a pretrained network instead, as we will see).\n",
    "\n",
    " let’s implement a ResNet-34 from scratch using Keras. First, let’s create a ResidualUnit layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60874b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualUnit(keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        self.main_layers = [\n",
    "            keras.layers.Conv2D(filters, 3, strides=strides,\n",
    "                                padding=\"same\", use_bias=False),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            keras.layers.Conv2D(filters, 3, strides=1,\n",
    "                                padding=\"same\", use_bias=False),\n",
    "            keras.layers.BatchNormalization()]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                keras.layers.Conv2D(filters, 1, strides=strides,\n",
    "                                    padding=\"same\", use_bias=False),\n",
    "                keras.layers.BatchNormalization()]\n",
    "\n",
    "#we make the inputs go through the main layers and the skip layers (if any), \n",
    "#then we add both outputs and apply the activation function.\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        return self.activation(Z + skip_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad2058cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we can build the ResNet-34 using a Sequential model, since it’s really just a long sequence of layers\n",
    "# (we can treat each residual unit as a single layer now that we have the ResidualUnit class):\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(64, 7, strides=2, input_shape=[224, 224, 3],\n",
    "                              padding=\"same\", use_bias=False))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation(\"relu\"))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\"))\n",
    "prev_filters = 64\n",
    "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    model.add(ResidualUnit(filters, strides=strides))\n",
    "    prev_filters = filters\n",
    "model.add(keras.layers.GlobalAvgPool2D())\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105915fc",
   "metadata": {},
   "source": [
    "The only slightly tricky part in this code is the loop that adds the ResidualUnit layers to the model: as explained earlier, the first 3 RUs have 64 filters, then the next 4 RUs have 128 filters, and so on. We then set the stride to 1 when the number of filters is the same as in the previous RU, or else we set it to 2. Then we add the ResidualUnit, and finally we update prev_filters.\n",
    "\n",
    "This short code demonstrates both the elegance of the ResNet model and the expressiveness of the Keras API.\n",
    "\n",
    "# Using Pretrained Models from Keras\n",
    "\n",
    "pretrained networks are readily available with a single line of code in the keras.applications package. \n",
    "\n",
    "For example, you can load the ResNet-50 model, pretrained on ImageNet, with the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85361af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.applications.resnet50.ResNet50(weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258a9bd5",
   "metadata": {},
   "source": [
    "That’s all! This will create a ResNet-50 model and download weights pretrained on the ImageNet dataset\n",
    "\n",
    "To use it, you first need to ensure that the images have the right size. A ResNet-50 model expects 224 × 224-pixel images (other models may expect other sizes, such as 299 × 299), so let’s use TensorFlow’s tf.image.resize() function to resize the images we loaded earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbb877fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_resized = tf.image.resize(images, [224, 224])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa03e2cc",
   "metadata": {},
   "source": [
    "- **TIP**: The tf.image.resize() will not preserve the aspect ratio. If this is a problem, try cropping the images to the appropriate aspect ratio before resizing. Both operations can be done in one shot with tf.image.crop_and_resize().\n",
    "\n",
    "The pretrained models assume that the images are preprocessed in a specific way. In some cases they may expect the inputs to be scaled from 0 to 1, or –1 to 1, and so on. Each model provides a preprocess_input() function that you can use to preprocess your images. These functions assume that the pixel values range from 0 to 255, so we must multiply them by 255 (since earlier we scaled them to the 0–1 range):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e1f7d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.applications.resnet50.preprocess_input(images_resized * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4613c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use the pretrained model to make predictions:\n",
    "Y_proba = model.predict(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae76163",
   "metadata": {},
   "source": [
    "As usual, the output Y_proba is a matrix with one row per image and one column per class \n",
    "\n",
    "If you want to display the top K predictions, including the class name and the estimated probability of each predicted class, use the decode_predictions() function\n",
    "\n",
    "For each image, it returns an array containing the top K predictions, where each prediction is represented as an array containing the class identifier,23 its name, and the corresponding confidence score:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d76ee1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image #0\n",
      "  n02825657 - bell_cote    82.35%\n",
      "  n03877845 - palace       6.75%\n",
      "  n03781244 - monastery    4.19%\n",
      "\n",
      "Image #1\n",
      "  n03530642 - honeycomb    49.83%\n",
      "  n13040303 - stinkhorn    33.96%\n",
      "  n02206856 - bee          4.35%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_K = keras.applications.resnet50.decode_predictions(Y_proba, top=3)\n",
    "for image_index in range(len(images)):\n",
    "    print(\"Image #{}\".format(image_index))\n",
    "    for class_id, name, y_proba in top_K[image_index]:\n",
    "        print(\"  {} - {:12s} {:.2f}%\".format(class_id, name, y_proba * 100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f060b5",
   "metadata": {},
   "source": [
    "It's easy to build good image classifiers with pretrained models. \n",
    "\n",
    "Other vision models are available in keras.applications, including several ResNet variants, GoogLeNet variants like Inception-v3 and Xception, VGGNet variants, and MobileNet and MobileNetV2 (lightweight models for use in mobile applications).\n",
    "\n",
    "But what if you want to use an image classifier for classes of images that are not part of ImageNet? In that case, you may still benefit from the pretrained models to perform transfer learning.\n",
    "\n",
    "# Pretrained Models for Transfer Learning\n",
    "\n",
    "If you want to build an image classifier but you do not have enough training data, then it is often a good idea to reuse the lower layers of a pretrained model, as we discussed in Chapter 11. \n",
    "\n",
    "For example, let’s train a model to classify pictures of flowers, reusing a pretrained Xception model. First, let’s load the dataset using TensorFlow Datasets (see Chapter 13):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0de4578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_datasets in c:\\users\\junglebook\\anaconda3\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\junglebook\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (2.25.1)\n",
      "Requirement already satisfied: termcolor in c:\\users\\junglebook\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow_datasets) (1.1.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in c:\\users\\junglebook\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (20.3.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\junglebook\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (4.59.0)\n",
      "Requirement already satisfied: future in c:\\users\\junglebook\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (0.18.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\junglebook\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (0.12.0)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in c:\\users\\junglebook\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow_datasets) (3.17.3)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\junglebook\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (5.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\junglebook\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow_datasets) (1.19.5)\n",
      "Requirement already satisfied: six in c:\\users\\junglebook\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.15.0)\n",
      "Requirement already satisfied: dill in c:\\users\\junglebook\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (0.3.4)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\junglebook\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (1.2.0)\n",
      "Requirement already satisfied: promise in c:\\users\\junglebook\\anaconda3\\lib\\site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\junglebook\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\junglebook\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\junglebook\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\junglebook\\anaconda3\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (4.0.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\junglebook\\anaconda3\\lib\\site-packages (from importlib-resources->tensorflow_datasets) (3.4.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\users\\junglebook\\anaconda3\\lib\\site-packages (from tensorflow-metadata->tensorflow_datasets) (1.53.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17332276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\n",
    "# with_info=True gives info on Data set\n",
    "# Here, we get the dataset size and the names of the classes.\n",
    "dataset_size = info.splits[\"train\"].num_examples # 3670\n",
    "class_names = info.features[\"label\"].names # [\"dandelion\", \"daisy\", ...]\n",
    "n_classes = info.features[\"label\"].num_classes # 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ba4353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfortunately, there is only a \"train\" dataset, no test set or validation set, so we need to split the training set\n",
    "# The TF Datasets project provides an API for this. For example, let’s take the first 10% of the dataset for testing, \n",
    "# the next 15% for validation, and the remaining 75% for training:\n",
    "test_set_raw, valid_set_raw, train_set_raw = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bfd6e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we must preprocess the images. The CNN expects 224 × 224 images, so we need to resize them. \n",
    "# We also need to run the images through Xception’s preprocess_input() function:\n",
    "def preprocess(image, label):\n",
    "    resized_image = tf.image.resize(image, [224, 224])\n",
    "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    return final_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b365307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s apply this preprocessing function to all three datasets, shuffle the training set,\n",
    "# and add batching and prefetching to all the datasets:\n",
    "batch_size = 32\n",
    "train_set = train_set_raw.shuffle(1000).repeat()\n",
    "train_set = train_set.map(preprocess).batch(batch_size).prefetch(1)\n",
    "valid_set = valid_set_raw.map(preprocess).batch(batch_size).prefetch(1)\n",
    "test_set = test_set_raw.map(preprocess).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4659c59",
   "metadata": {},
   "source": [
    "If you want to perform some data augmentation, change the preprocessing function for the training set, adding some random transformations to the training images. \n",
    "\n",
    "For example, use tf.image.random_crop() to randomly crop the images, use tf.image.random_flip_left_right() to randomly flip the images horizontally, and so on (see the “Pretrained Models for Transfer Learning” section of the notebook for an example).\n",
    " \n",
    "- **TIP**: The keras.preprocessing.image.ImageDataGenerator class makes it easy to load images from disk and augment them in various ways: you can shift each image, rotate it, rescale it, flip it horizontally or vertically, shear it, or apply any transformation function you want to it. This is very convenient for simple projects. However, building a tf.data pipeline has many advantages: it can read the images efficiently (e.g., in parallel) from any source, not just the local disk; you can manipulate the Dataset as you wish; and if you write a preprocessing function based on tf.image operations, this function can be used both in the tf.data pipeline and in the model you will deploy to production (see Chapter 19).\n",
    "\n",
    "Next let’s load an Xception model, pretrained on ImageNet. We exclude the top of the network by setting include_top=False: this excludes the global average pooling layer and the dense output layer. We then add our own global average pooling layer, based on the output of the base model, followed by a dense output layer with one unit per class, using the softmax activation function. Finally, we create the Keras Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ebac4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                  include_top=False)\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "model = keras.Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "867d0e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As explained in chapter 4 it’s usually a good idea to freeze the weights of the pretrained layers,\n",
    "#at least at the beginning of training:\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Since our model uses the base model’s layers directly, rather than the base_model object itself, \n",
    "# setting base_model.trainable=False would have no effect."
   ]
  },
  {
   "cell_type": "raw",
   "id": "dd863643",
   "metadata": {},
   "source": [
    "# Finally, we can compile the model and start training:\n",
    "optimizer = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5, validation_data=valid_set)\n",
    "\n",
    "#Don't run this as it is computationally heavy => slow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71629a3",
   "metadata": {},
   "source": [
    "After training the model for a few epochs, its validation accuracy should reach about 75–80% and stop making much progress\n",
    "\n",
    "This means that the top layers are now pretty well trained, so we are ready to unfreeze all the layers (or you could try unfreezing just the top ones) and continue training (don’t forget to compile the model when you freeze or unfreeze layers). This time we use a much lower learning rate to avoid damaging the pretrained weights:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a285ee8",
   "metadata": {},
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.001)\n",
    "model.compile(...)\n",
    "history = model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab87d82",
   "metadata": {},
   "source": [
    "The above model should hit 95% accuracy. While we can make good classification models, we can also answer the quesiton of \"where\" the object is. \n",
    "\n",
    "# Classification and Localization\n",
    "\n",
    "- Localization is a regression task. To predict a bounding box around the object, a common approach is to predict the horizontal and vertical coordinates of the object’s center, as well as its height and width. \n",
    "\n",
    "This means we have four numbers to predict. It does not require much change to the model; we just need to add a second dense output layer with four units (typically on top of the global average pooling layer), and it can be trained using the MSE loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72672a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JungleBook\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)\n",
    "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                  include_top=False)\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "class_output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "loc_output = keras.layers.Dense(4)(avg)\n",
    "model = keras.Model(inputs=base_model.input,\n",
    "                    outputs=[class_output, loc_output])\n",
    "model.compile(loss=[\"sparse_categorical_crossentropy\", \"mse\"],\n",
    "              loss_weights=[0.8, 0.2], # depends on what you care most about\n",
    "              optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fe9b5e",
   "metadata": {},
   "source": [
    "- Now we have a problem: the flowers dataset does not have bounding boxes around the flowers. So, we need to add them ourselves. This is often one of the hardest and most costly parts of a Machine Learning project: getting the labels.\n",
    "\n",
    "It’s a good idea to spend time looking for the right tools. To annotate images with bounding boxes, you may want to use an open source image labeling tool like VGG Image Annotator, LabelImg, OpenLabeler, or ImgLab, or perhaps a commercial tool like LabelBox or Supervisely. \n",
    "\n",
    "You may also want to consider crowdsourcing platforms such as Amazon Mechanical Turk if you have a very large number of images to annotate.\n",
    " \n",
    "However, it is quite a lot of work to set up a crowdsourcing platform, prepare the form to be sent to the workers, supervise them, and ensure that the quality of the bounding boxes they produce is good, so make sure it is worth the effort. If there are just a few thousand images to label, and you don’t plan to do this frequently, it may be preferable to do it yourself. \n",
    "\n",
    "Adriana Kovashka et al. wrote a very practical paper about crowdsourcing in computer vision. I recommend you check it out, even if you do not plan to use crowdsourcing.\n",
    "\n",
    "Let’s suppose you’ve obtained the bounding boxes for every image in the flowers dataset (for now we will assume there is a single bounding box per image). You then need to create a dataset whose items will be batches of preprocessed images along with their class labels and their bounding boxes. Each item should be a tuple of the form (images, (class_labels, bounding_boxes)). Then you are ready to train your model!\n",
    "\n",
    "- **TIP**: The bounding boxes should be normalized so that the horizontal and vertical coordinates, as well as the height and width, all range from 0 to 1. Also, it is common to predict the square root of the height and width rather than the height and width directly: this way, a 10-pixel error for a large bounding box will not be penalized as much as a 10-pixel error for a small bounding box.\n",
    "\n",
    "- The MSE often works fairly well as a cost function to train the model, but it is not a great metric to evaluate how well the model can predict bounding boxes.\n",
    "\n",
    "- The most common metric for this is the **Intersection over Union (IoU)**: the area of overlap between the predicted bounding box and the target bounding box, divided by the area of their union\n",
    "\n",
    "In tf.keras, it is implemented by the tf.keras.metrics.MeanIoU class.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1423.png)\n",
    "\n",
    "Classifying and localizing a single object is nice, but what if the images contain multiple objects (as is often the case in the flowers dataset)?\n",
    "\n",
    "# Object Detection\n",
    "\n",
    "- **object detection**: The task of classifying and localizing multiple objects in an image.\n",
    "\n",
    "A few years ago the norm was to take a trained CNN and slide it acress the image. Making boxes as it goes. It would scan in 3x3 boxes, then it would look for for 4x4 and so on. Look below for an illustration. \n",
    "\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1424.png)\n",
    "\n",
    "This technique is fairly straightforward, but as you can see it will detect the same object multiple times, at slightly different positions.\n",
    "\n",
    "Some **post-processing** will then be needed to get rid of all the unnecessary bounding boxes. A common approach for this is called **non-max suppression**. Here’s how you do it:\n",
    "\n",
    "1. First, you need to add an extra objectness output to your CNN, to estimate the probability that a flower is indeed present in the image (alternatively, you could add a “no-flower” class, but this usually does not work as well). It must use the sigmoid activation function, and you can train it using binary cross-entropy loss. Then get rid of all the bounding boxes for which the objectness score is below some threshold: this will drop all the bounding boxes that don’t actually contain a flower.\n",
    "\n",
    "2. Find the bounding box with the highest objectness score, and get rid of all the other bounding boxes that overlap a lot with it (e.g., with an IoU greater than 60%). For example, in Figure 14-24, the bounding box with the max objectness score is the thick bounding box over the topmost rose (the objectness score is represented by the thickness of the bounding boxes). The other bounding box over that same rose overlaps a lot with the max bounding box, so we will get rid of it.\n",
    "\n",
    "3. Repeat step two until there are no more bounding boxes to get rid of.\n",
    "\n",
    "This simple approach to object detection works pretty well, but it requires running the CNN many times, so it is quite slow. Fortunately, there is a much faster way to slide a CNN across an image: using a **fully convolutional network (FCN)**.\n",
    "\n",
    "# Fully Convolutional Networks\n",
    "\n",
    "Replaces dense layers with convolutional layers.\n",
    "\n",
    "- **TIP**: To convert a dense layer to a convolutional layer, the number of filters in the convolutional layer must be equal to the number of units in the dense layer, the filter size must be equal to the size of the input feature maps, and you must use \"valid\" padding. The stride may be set to 1 or more, as we will see shortly.\n",
    "\n",
    "While a dense layer expects a specific input size a convolutional layer will happily process images of any size(however, it does expect its inputs to have a specific number of channels, since each kernel contains a different set of weights for each input channel). \n",
    "\n",
    "Since an FCN contains only convolutional layers (and pooling layers, which have the same property), it can be trained and executed on images of any size!\n",
    "\n",
    "For example, suppose we’d already trained a CNN for flower classification and localization. It was trained on 224 × 224 images, and it outputs 10 numbers: outputs 0 to 4 are sent through the softmax activation function, and this gives the class probabilities (one per class); output 5 is sent through the logistic activation function, and this gives the objectness score; outputs 6 to 9 do not use any activation function, and they represent the bounding box’s center coordinates, as well as its height and width. We can now convert its dense layers to convolutional layers. In fact, we don’t even need to retrain it; we can just copy the weights from the dense layers to the convolutional layers! Alternatively, we could have converted the CNN into an FCN before training.\n",
    "\n",
    "Now suppose the last convolutional layer before the output layer (also called the bottleneck layer) outputs 7 × 7 feature maps when the network is fed a 224 × 224 image. If we feed the FCN a 448 × 448 image the bottleneck layer will now output 14 × 14 feature maps. Since the dense output layer was replaced by a convolutional layer using 10 filters of size 7 × 7, with \"valid\" padding and stride 1, the output will be composed of 10 features maps, each of size 8 × 8 (since 14 – 7 + 1 = 8. Difference in filters plus the stride).\n",
    "\n",
    "\n",
    "In other words, the FCN will process the whole image only once, and it will output an 8 × 8 grid where each cell contains 10 numbers (5 class probabilities, 1 objectness score, and 4 bounding box coordinates). It’s exactly like taking the original CNN and sliding it across the image using 8 steps per row and 8 steps per column. To visualize this, imagine chopping the original image into a 14 × 14 grid, then sliding a 7 × 7 window across this grid; there will be 8 × 8 = 64 possible locations for the window, hence 8 × 8 predictions. However, the FCN approach is much more efficient, since the network only looks at the image once. In fact, **You Only Look Once (YOLO)** is the name of a very popular object detection architecture, which we’ll look at next.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1425.png)\n",
    "\n",
    "I don't really get FCNs. \n",
    "\n",
    "# You Only Look Once (YOLO)\n",
    "\n",
    "YOLO is an extremely fast and accurate object detection architecture. It can run in on live video, just youtube it. \n",
    "\n",
    "The current verios is YOLOv3. It has a a similar architecture to FCNs but with a few important differences. \n",
    "\n",
    "- It outputs five bounding boxes for each grid cell (instead of just one), and each bounding box comes with an objectness score. It also outputs 20 class probabilities per grid cell, as it was trained on the PASCAL VOC dataset, which contains 20 classes. That’s a total of 45 numbers per grid cell: 5 bounding boxes, each with 4 coordinates, plus 5 objectness scores, plus 20 class probabilities.\n",
    "\n",
    "- Instead of predicting the absolute coordinates of the bounding box centers, YOLOv3 predicts an offset relative to the coordinates of the grid cell, where (0, 0) means the top left of that cell and (1, 1) means the bottom right. For each grid cell, YOLOv3 is trained to predict only bounding boxes whose center lies in that cell (but the bounding box itself generally extends well beyond the grid cell). YOLOv3 applies the logistic activation function to the bounding box coordinates to ensure they remain in the 0 to 1 range.\n",
    "\n",
    "- Before training the neural net, YOLOv3 finds five representative bounding box dimensions, called **anchor boxes** (or **bounding box** priors). It does this by applying the K-Means algorithm (see Chapter 9) to the height and width of the training set bounding boxes. For example, if the training images contain many pedestrians, then one of the anchor boxes will likely have the dimensions of a typical pedestrian. Then when the neural net predicts five bounding boxes per grid cell, it actually predicts how much to rescale each of the anchor boxes. For example, suppose one anchor box is 100 pixels tall and 50 pixels wide, and the network predicts, say, a vertical rescaling factor of 1.5 and a horizontal rescaling of 0.9 (for one of the grid cells). This will result in a predicted bounding box of size 150 × 45 pixels. To be more precise, for each grid cell and each anchor box, the network predicts the log of the vertical and horizontal rescaling factors. Having these priors makes the network more likely to predict bounding boxes of the appropriate dimensions, and it also speeds up training because it will more quickly learn what reasonable bounding boxes look like.\n",
    "\n",
    "- The network is trained using images of different scales: every few batches during training, the network randomly chooses a new image dimension (from 330 × 330 to 608 × 608 pixels). This allows the network to learn to detect objects at different scales. Moreover, it makes it possible to use YOLOv3 at different scales: the smaller scale will be less accurate but faster than the larger scale, so you can choose the right trade-off for your use case.\n",
    "\n",
    "There are a few more boxing innovations. One of which uses skip connections to recover spatial resolution lost in the CNN. \n",
    " \n",
    "The YOLO9000 model that uses hierarchical classification: the model predicts a probability for each node in a visual hierarchy called WordTree. This makes it possible for the network to predict with high confidence that an image represents, say, a dog, even though it is unsure what specific type of dog.\n",
    "\n",
    "---\n",
    "## MEAN AVERAGE PRECISION (MAP)\n",
    "\n",
    "A very common metric used in object detection tasks is the **mean Average Precision (mAP)**. “Mean Average” sounds a bit redundant, doesn’t it? To understand this metric, let’s go back to two classification metrics we discussed in Chapter 3: precision and recall. Remember the trade-off: the higher the recall, the lower the precision. You can visualize this in a precision/recall curve (see Figure 3-5). To summarize this curve into a single number, we could compute its area under the curve (AUC). But note that the precision/recall curve may contain a few sections where precision actually goes up when recall increases, especially at low recall values (you can see this at the top left of Figure 3-5). This is one of the motivations for the mAP metric.\n",
    "\n",
    "Suppose the classifier has 90% precision at 10% recall, but 96% precision at 20% recall. There’s really no trade-off here: it simply makes more sense to use the classifier at 20% recall rather than at 10% recall, as you will get both higher recall and higher precision. So instead of looking at the precision at 10% recall, we should really be looking at the maximum precision that the classifier can offer with at least 10% recall. It would be 96%, not 90%. Therefore, one way to get a fair idea of the model’s performance is to compute the maximum precision you can get with at least 0% recall, then 10% recall, 20%, and so on up to 100%, and then calculate the mean of these maximum precisions. This is called the Average Precision (AP) metric. Now when there are more than two classes, we can compute the AP for each class, and then compute the mean AP (mAP). That’s it!\n",
    "\n",
    "In an object detection system, there is an additional level of complexity: what if the system detected the correct class, but at the wrong location (i.e., the bounding box is completely off)? Surely we should not count this as a positive prediction. One approach is to define an IOU threshold: for example, we may consider that a prediction is correct only if the IOU is greater than, say, 0.5, and the predicted class is correct. The corresponding mAP is generally noted mAP@0.5 (or mAP@50%, or sometimes just AP50). In some competitions (such as the PASCAL VOC challenge), this is what is done. In others (such as the COCO competition), the mAP is computed for different IOU thresholds (0.50, 0.55, 0.60, …, 0.95), and the final metric is the mean of all these mAPs (noted mAP@[.50:.95] or mAP@[.50:0.05:.95]). Yes, that’s a mean mean average.\n",
    "\n",
    "___\n",
    "\n",
    "Yolo implementaions can be found github, altough the book recomends checking out a specific one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f903663",
   "metadata": {},
   "source": [
    "# Semantic Segmentation\n",
    "\n",
    "- In **semantic segmentation**, each pixel is classified according to the class of the object it belongs to (e.g., road, car, pedestrian, building, etc.). See figure below for what semantic segmentaion visualization \n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1426.png)\n",
    "\n",
    "Often precise information about the object is lost in this process(ie. It knows it is a dog but not what type. It knows it is a human but not the gender. )\n",
    "\n",
    "The reason for this is CNNs with a stride(this causes it to lose spatial resolution.)\n",
    "\n",
    "A solution to this was to use a CNN and turn it into a FCN with a layer of **upsampling**. There are several solutions available for upsampling (increasing the size of an image), such as bilinear interpolation, but that only works reasonably well up to ×4 or ×8. \n",
    "\n",
    "- Instead of the prior upsampling methods. A **Transposed convolutional** layer was used. it is equivalent to first stretching the image by inserting empty rows and columns (full of zeros), then performing a regular convolution.\n",
    "\n",
    "The transposed convolutional layer can be initialized to perform something close to linear interpolation, but since it is a trainable layer, it will learn to do better during training. In tf.keras, you can use the Conv2DTranspose layer.\n",
    "\n",
    "![](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781492032632/files/assets/mls2_1427.png)\n",
    "\n",
    "- **NOTE**: In a transposed convolutional layer, the stride defines how much the input will be stretched, not the size of the filter steps, so the larger the stride, the larger the output (unlike for convolutional layers or pooling layers).\n",
    "\n",
    "- **Note**: Tensorflow offers various kinds of convolutional layers, so give it a look. \n",
    "\n",
    "While transposed convolution did better, they added skip connections to really get the job done. In this process they found they can reproduce the original image or even increase the resolution of the orginal image. This technique of increasing the resolution is called **super resolution**. \n",
    "\n",
    "Look up tensorflow implemenations of semantic segmentation, pretrained models can be found. \n",
    "\n",
    "\n",
    "## Instance segmentation\n",
    "\n",
    "Similar to semantic segmentation but instead of clumping objects, it labels each individual one. \n",
    "\n",
    "Instance segmentaion is based on the Mask R-CNN architecture, it extends the Faster R-CNN model by additionally producing a pixel mask for each bounding box.\n",
    "\n",
    "\n",
    "# Sumamry \n",
    "\n",
    "Deep computer vision is moving fast and is now focused on new problems such as: adversarial learning (which attempts to make the network more resistant to images designed to fool it), explainability (understanding why the network makes a specific classification), realistic image generation and single-shot learning (a system that can recognize an object after it has seen it just once).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
