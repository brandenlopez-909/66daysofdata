{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Support Vector Machine (SVM) is a powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and anyone interested in Machine Learning should have it in their toolbox. SVMs are particularly well suited for classification of complex small- or medium-sized datasets.\n",
    "\n",
    "# Linear SVM Classification\n",
    "\n",
    "The fundamental idea behind SVMs is best explained with some pictures. Figure 5-1 shows part of the iris dataset that was introduced at the end of Chapter 4. The two classes can clearly be separated easily with a straight line (they are linearly separable). The left plot shows the decision boundaries of three possible linear classifiers. The model whose decision boundary is represented by the dashed line is so bad that it does not even separate the classes properly. The other two models work perfectly on this training set, but their decision boundaries come so close to the instances that these models will probably not perform as well on new instances. In contrast, the solid line in the plot on the right represents the decision boundary of an SVM classifier; this line not only separates the two classes but also stays as far away from the closest training instances as possible. You can think of an SVM classifier as fitting the widest possible street (represented by the parallel dashed lines) between the classes. This is called _large margin classification_.\n",
    "\n",
    "![Figure 5-1](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0501.png)\n",
    "\n",
    "Notice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully determined (or “supported”) by the instances located on the edge of the street. These instances are called the _support vectors_ (they are circled in Figure 5-1).\n",
    "\n",
    "![Figure 5-2](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0502.png)\n",
    "\n",
    "- **WARNING** SVMs are sensitive to the feature scales, as you can see in Figure 5-2: in the left plot, the vertical scale is much larger than the horizontal scale, so the widest possible street is close to horizontal. After feature scaling (e.g., using Scikit-Learn’s StandardScaler), the decision boundary in the right plot looks much better.\n",
    "\n",
    "# Soft Margin Classification\n",
    "\n",
    "If we strictly impose that all instances must be off the street and on the correct side, this is called _hard margin \n",
    "classification_. There are two main issues with hard margin classification.\n",
    " \n",
    "First, it only works if the data is linearly separable. Second, it is sensitive to outliers. Figure 5-3 shows the iris dataset with just one additional outlier: on the left, it is impossible to find a hard margin; on the right, the decision boundary ends up very different from the one we saw in Figure 5-1 without the outlier, and it will probably not generalize as well.\n",
    "\n",
    "![Figure 5-3](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0503.png)\n",
    "\n",
    "To avoid these issues, use a more flexible model. The objective is to find a good balance between keeping the street as large as possible and limiting the margin violations (i.e., instances that end up in the middle of the street or even on the wrong side). This is called _soft margin classification_.\n",
    "\n",
    "When creating an SVM model using Scikit-Learn, we can specify a number of hyperparameters. C is one of those hyperparameters. If we set it to a low value, then we end up with the model on the left of Figure 5-4. With a high value, we get the model on the right. Margin violations are bad. It’s usually better to have few of them. However, in this case the model on the left has a lot of margin violations but will probably generalize better.\n",
    "\n",
    "![Figure 5-4](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0504.png)\n",
    "\n",
    "- **TIP**: If your SVM model is overfitting, you can try regularizing it by reducing C.\n",
    "\n",
    " The following Scikit-Learn code loads the iris dataset, scales the features, and then trains a linear SVM model (using the LinearSVC class with C=1 and the hinge loss function, described shortly) to detect Iris virginica flowers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('linear_svc',\n",
       "                 LinearSVC(C=1, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "                           penalty='l2', random_state=None, tol=0.0001,\n",
       "                           verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64)  # Iris virginica\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
    "    ])\n",
    "\n",
    "svm_clf.fit(X, y)\n",
    "#This model is represented on the left of Figure 5-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then, as usual, you can use the model to make predictions:\n",
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **NOTE**: Unlike Logistic Regression classifiers, SVM classifiers do not output probabilities for each class.\n",
    "\n",
    "Instead of using the LinearSVC class, we could use the SVC class with a linear kernel. When creating the SVC model, we would write **SVC(kernel=\"linear\", C=1)**\n",
    "\n",
    "Or we could use the SGDClassifier class, with **SGDClassifier(loss=\"hinge\", alpha=1/(m*C))**\n",
    "\n",
    "This applies regular Stochastic Gradient Descent to train a linear SVM classifier. It does not converge as fast as the LinearSVC class, but it can be useful to handle online classification tasks or huge datasets that do not fit in memory (out-of-core training).\n",
    "\n",
    "- **TIP**: The LinearSVC class regularizes the bias term, so you should center the training set first by subtracting its mean. This is automatic if you scale the data using the StandardScaler. Also make sure you set the loss hyperparameter to \"hinge\", as it is not the default value. Finally, for better performance, you should set the dual hyperparameter to False, unless there are more features than training instances (we will discuss duality later in the chapter).\n",
    "\n",
    "# Nonlinear SVM Classification\n",
    "\n",
    "Although linear SVM classifiers are efficient and work surprisingly well in many cases, many datasets are not even close to being linearly separable. One approach to handling nonlinear datasets is to add more features, such as polynomial features; in some cases this can result in a linearly separable dataset. Consider the left plot in Figure 5-5: it represents a simple dataset with just one feature, $x_1$. This dataset is not linearly separable, as you can see. But if you add a second feature $x_2 = (x_1)^2$, the resulting 2D dataset is perfectly linearly separable.\n",
    "\n",
    "![Figure 5-5](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0505.png)\n",
    "\n",
    "To implement this idea using Scikit-Learn, create a Pipeline containing a PolynomialFeatures transformer followed by a StandardScaler and a LinearSVC. Let’s test this on the moons dataset: this is a toy dataset for binary classification in which the data points are shaped as two interleaving half circles (see Figure 5-6). You can generate this dataset using the make_moons() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JungleBook\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#generate ds \n",
    "X, y = make_moons(n_samples=100, noise=0.15) \n",
    "\n",
    "#Pipeline a transformer \n",
    "polynomial_svm_clf = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n",
    "    ])\n",
    "\n",
    "#Train the model\n",
    "polynomial_svm_clf.fit(X, y)\n",
    "\n",
    "#Clf is a trained classifier \n",
    "def plot_predictions(clf, axes):\n",
    "    x0s = np.linspace(axes[0], axes[1], 100)\n",
    "    x1s = np.linspace(axes[2], axes[3], 100)\n",
    "    x0, x1 = np.meshgrid(x0s, x1s)\n",
    "    X = np.c_[x0.ravel(), x1.ravel()]\n",
    "    y_pred = clf.predict(X).reshape(x0.shape)\n",
    "    y_decision = clf.decision_function(X).reshape(x0.shape)\n",
    "    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n",
    "    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n",
    "\n",
    "def plot_dataset(X, y, axes):\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
    "    plt.axis(axes)\n",
    "    plt.grid(True, which='both')\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=20)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n",
    "\n",
    "    \n",
    "plot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Kernel\n",
    "\n",
    "Adding polynomial features is simple to implement and can work great with all sorts of Machine Learning algorithms (not just SVMs). That said, at a low polynomial degree, this method cannot deal with very complex datasets, and with a high polynomial degree it creates a huge number of features, making the model too slow.\n",
    "\n",
    "Fortunately, when using SVMs you can apply an almost miraculous mathematical technique called the kernel trick (explained in a moment). The kernel trick makes it possible to get the same result as if you had added many polynomial features, even with very high-degree polynomials, without actually having to add them. So there is no combinatorial explosion of the number of features because you don’t actually add any features. This trick is implemented by the SVC class. Let’s test it \n",
    "on the moons dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('svm_clf',\n",
       "                 SVC(C=5, break_ties=False, cache_size=200, class_weight=None,\n",
       "                     coef0=1, decision_function_shape='ovr', degree=3,\n",
       "                     gamma='scale', kernel='poly', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "    ])\n",
    "poly_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code trains an SVM classifier using a third-degree polynomial kernel. It is represented on the left in Figure 5-7. On the right is another SVM classifier using a 10th-degree polynomial kernel. Obviously, if your model is overfitting, you might want to reduce the polynomial degree. Conversely, if it is underfitting, you can try increasing it. The hyperparameter coef0 controls how much the model is influenced by high-degree polynomials versus low-degree polynomials.\n",
    "\n",
    "![Figure 5-7](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0507.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TIP**: A common approach to finding the right hyperparameter values is to use grid search (see Chapter 2). It is often faster to first do a very coarse grid search, then a finer grid search around the best values found. Having a good sense of what each hyperparameter actually does can also help you search in the right part of the hyperparameter space.\n",
    "\n",
    "# Similarity Features\n",
    "\n",
    "Another technique to tackle nonlinear problems is to add features computed using a similarity function, which measures how much each instance resembles a particular landmark. For example, let’s take the 1D dataset discussed earlier and add two landmarks to it at $x_1 = –2$ and $x_1 = 1$ (see the left plot in Figure 5-8). Next, let’s define the similarity function to be the Gaussian Radial Basis Function (RBF) with $\\gamma = 0.3$ (see Equation 5-1).\n",
    "\n",
    "Eqn 5-1 \n",
    "$$\n",
    "\\phi_{\\gamma}(\\mathbf{x}, \\ell)=\\exp \\left(-\\gamma\\|\\mathbf{x}-\\ell\\|^{2}\\right)\n",
    "$$\n",
    "\n",
    "This is a bell-shaped function varying from 0 (very far away from the landmark) to 1 (at the landmark). Now we are ready to compute the new features. For example, let’s look at the instance $x_1 = –1$: it is located at a distance of 1 from the first landmark and 2 from the second landmark. Therefore its new features are $x_2 = \\exp(–0.3 × 12) \\approx 0.74 $and $x_3 = \\exp(–0.3 × 22) \\approx 0.30$. The plot on the right in Figure 5-8 shows the transformed dataset (dropping the original features). As you can see, it is now linearly separable.\n",
    "\n",
    "![Figure 5-8](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0508.png)\n",
    "\n",
    "You may wonder how to select the landmarks. The simplest approach is to create a landmark at the location of each and every instance in the dataset. Doing that creates many dimensions and thus increases the chances that the transformed training set will be linearly separable. The downside is that a training set with m instances and n features gets transformed into a training set with m instances and m features (assuming you drop the original features). If your training set is very large, you end up with an equally large number of features.\n",
    "\n",
    "# Gaussian RBF Kernel\n",
    "\n",
    "Just like the polynomial features method, the similarity features method can be useful with any Machine Learning algorithm, but it may be computationally expensive to compute all the additional features, especially on large training sets. Once again the kernel trick does its SVM magic, making it possible to obtain a similar result as if you had added many similarity features. Let’s try the SVC class with the Gaussian RBF kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('svm_clf',\n",
       "                 SVC(C=0.001, break_ties=False, cache_size=200,\n",
       "                     class_weight=None, coef0=0.0,\n",
       "                     decision_function_shape='ovr', degree=3, gamma=5,\n",
       "                     kernel='rbf', max_iter=-1, probability=False,\n",
       "                     random_state=None, shrinking=True, tol=0.001,\n",
       "                     verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "    ])\n",
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is represented at the bottom left in Figure 5-9. The other plots show models trained with different values of hyperparameters gamma (γ) and C. Increasing gamma makes the bell-shaped curve narrower (see the lefthand plots in Figure 5-8). As a result, each instance’s range of influence is smaller: the decision boundary ends up being more irregular, wiggling around individual instances. Conversely, a small gamma value makes the bell-shaped curve wider: instances have a larger range of influence, and the decision boundary ends up smoother. So γ acts like a regularization hyperparameter: if your model is overfitting, you should reduce it; if it is underfitting, you should increase it (similar to the C hyperparameter).\n",
    "\n",
    "![Figure 5-9](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0509.png)\n",
    "\n",
    "Other kernels exist but are used much more rarely. Some kernels are specialized for specific data structures. _String kernels_ are sometimes used when classifying text documents or DNA sequences (e.g., using the string subsequence kernel or kernels based on the Levenshtein distance).\n",
    "\n",
    "- **TIP**: With so many kernels to choose from, how can you decide which one to use? As a rule of thumb, you should always try the linear kernel first (remember that LinearSVC is much faster than SVC(kernel=\"linear\")), especially if the training set is very large or if it has plenty of features. If the training set is not too large, you should also try the Gaussian RBF kernel; it works well in most cases. Then if you have spare time and computing power, you can experiment with a few other kernels, using cross-validation and grid search. You’d want to experiment like that especially if there are kernels specialized for your training set’s data structure.\n",
    "\n",
    "# Computational Complexity\n",
    "\n",
    "The LinearSVC class is based on the liblinear library, which implements an optimized algorithm for linear SVMs. It does not support the kernel trick, but it scales almost linearly with the number of training instances and the number of features. Its training time complexity is roughly $O(m × n)$.\n",
    "\n",
    "The algorithm takes longer if you require very high precision. This is controlled by the tolerance hyperparameter ϵ (called tol in Scikit-Learn). In most classification tasks, the default tolerance is fine.\n",
    "\n",
    "The SVC class is based on the libsvm library, which implements an algorithm that supports the kernel trick. The training time complexity is usually between $O(m^2 × n)$ and $O(m^3 × n)$. Unfortunately, this means that it gets dreadfully slow when the number of training instances gets large (e.g., hundreds of thousands of instances). This algorithm is perfect for complex small or medium-sized training sets. It scales well with the number of features, especially with sparse features (i.e., when each instance has few nonzero features). In this case, the algorithm scales roughly with the average number of nonzero features per instance. Table 5-1 compares Scikit-Learn’s SVM classification classes.\n",
    "\n",
    "\n",
    "|Class|\tTime complexity |\tOut-of-core support\t| Scaling required\t| Kernel trick| \n",
    "|-----|\t--------------- |\t------------------- |---------------\t| ------------|\n",
    "|LinearSVC|O(m × n) |\tNo | yes\t| No|\n",
    "|SGDClassifier|\tO(m × n)|yes|yes| no|\n",
    "|SVC | O(m² × n) to O(m³ × n) |no|yes| yes|\n",
    "\n",
    "\n",
    "# SVM Regression\n",
    "As mentioned earlier, the SVM algorithm is versatile: not only does it support linear and nonlinear classification, but it also supports linear and nonlinear regression. To use SVMs for regression instead of classification, the trick is to reverse the objective: instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM Regression tries to fit as many instances as possible on the street while limiting margin violations (i.e., instances off the street). The width of the street is controlled by a hyperparameter, ϵ. Figure 5-10 shows two linear SVM Regression models trained on some random linear data, one with a large margin (ϵ = 1.5) and the other with a small margin (ϵ = 0.5).\n",
    "\n",
    "![Figure 5-10](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0510.png)\n",
    "\n",
    "Adding more training instances within the margin does not affect the model’s predictions; thus, the model is said to be ϵ-insensitive.\n",
    "\n",
    "You can use Scikit-Learn’s LinearSVR class to perform linear SVM Regression. The following code produces the model represented on the left in Figure 5-10 (the training data should be scaled and centered first):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=1.5, fit_intercept=True,\n",
       "          intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
       "          random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "svm_reg = LinearSVR(epsilon=1.5)\n",
    "svm_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tackle nonlinear regression tasks, you can use a kernelized SVM model. Figure 5-11 shows SVM Regression on a random quadratic training set, using a second-degree polynomial kernel. There is little regularization in the left plot (i.e., a large C value), and much more regularization in the right plot (i.e., a small C value).\n",
    "\n",
    "![Figure 5-11](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0511.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100, cache_size=200, coef0=0.0, degree=2, epsilon=0.1, gamma='scale',\n",
       "    kernel='poly', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The following code uses Scikit-Learn’s SVR class (which supports the kernel trick) to produce \n",
    "# the model represented on the left in Figure 5-11`\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
    "svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVR class is the regression equivalent of the SVC class, and the LinearSVR class is the regression equivalent of the LinearSVC class. The LinearSVR class scales linearly with the size of the training set (just like the LinearSVC class), while the SVR class gets much too slow when the training set grows large (just like the SVC class).\n",
    "\n",
    "- **NOTE**: SVMs can also be used for outlier detection; see Scikit-Learn’s documentation for more details.\n",
    "\n",
    "# Under the Hood\n",
    "\n",
    "First, a word about notations. In Chapter 4 we used the convention of putting all the model parameters in one vector θ, including the bias term $θ_0$ and the input feature weights $θ_1$ to $θ_n$, and adding a bias input $x_0 = 1$ to all instances. In this chapter we will use a convention that is more convenient (and more common) when dealing with SVMs: the bias term will be called b, and the feature weights vector will be called w. No bias feature will be added to the input feature vectors.\n",
    "\n",
    "# Decision Function and Predictions\n",
    "\n",
    "The linear SVM classifier model predicts the class of a new instance x by simply computing the decision function $W^TX+b = w_1x_1 + w_2x_2 + ... + w_nx_n + b$. If the  If the result is positive, the predicted class $\\hat{y}$ is the positive class (1), and otherwise it is the negative class (0);\n",
    "\n",
    "Eqn 5-2\n",
    "$$ \\hat{y} = \n",
    "\\begin{cases} \n",
    "      0 & \\text{ if } \\space W^TX+b < 0 \\\\\n",
    "      1 & \\text{ if } \\space W^TX+b \\geq 0 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Figure 5-12 shows the decision function that corresponds to the model in the right in Figure 5-4: it is a 2D plane because this dataset has two features (petal width and petal length). The decision boundary is the set of points where the decision function is equal to 0: it is the intersection of two planes, which is a straight line (represented by the thick solid line).\n",
    "\n",
    "![Figure 5-12](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0512.png)\n",
    "\n",
    "The dashed lines represent the points where the decision function is equal to 1 or –1: they are parallel and at equal distance to the decision boundary, and they form a margin around it. Training a linear SVM classifier means finding the values of w and b that make this margin as wide as possible while avoiding margin violations (hard margin) or limiting them (soft margin).\n",
    "\n",
    "# Training Objective\n",
    "\n",
    "Consider the slope of the decision function: it is equal to the norm of the weight vector, ∥ w ∥. If we divide this slope by 2, the points where the decision function is equal to ±1 are going to be twice as far away from the decision boundary. In other words, dividing the slope by 2 will multiply the margin by 2. This may be easier to visualize in 2D, as shown in Figure 5-13. The smaller the weight vector w, the larger the margin.\n",
    "\n",
    "![Figure 5-13](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0513.png)\n",
    "\n",
    "So we want to minimize ∥ w ∥ to get a large margin. If we also want to avoid any margin violations (hard margin), then we need the decision function to be greater than 1 for all positive training instances and lower than –1 for negative training instances. If we define $t^{(i)} = –1$ for negative instances (if $y^{(i)} = 0$) and $t^{(i)} = 1$ for positive instances (if $y^{(i)} = 1$), then we can express this constraint as $t^{(i)}(w^⊺ x^{(i)} + b) ≥ 1$ for all instances.\n",
    "\n",
    "We can therefore express the hard margin linear SVM classifier objective as the constrained optimization problem in Equation 5-3.\n",
    "\n",
    "Eqn 5-3 \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\underset{\\mathbf{w}, b}{\\operatorname{minimize}} &  \\quad \\frac{1}{2} \\mathbf{w}^{\\top} \\mathbf{w} \\\\\n",
    "\\text { subject to } & \\quad t^{(i)}\\left(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)}+b\\right) \\geq 1 \\quad \\text { for } i=1,2, \\cdots, m\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- **NOTE**: We are minimizing $\\frac{1}{2} w^T w $, which is equal to $\\frac{1}{2} \\| w\\|_2$, rather than minimizing $\\| w\\|$. Indeed,  $\\frac{1}{2} \\| w\\|_2$ has a nice, simple derivative (it is just w), while $\\| w\\|$ is not differentiable at w = 0. Optimization algorithms work much better on differentiable functions.\n",
    "\n",
    "To get the soft margin objective, we need to introduce a slack variable $\\xi(i) ≥ 0$ for each instance: $\\xi(i)$ measures how much the $i^{th}$ instance is allowed to violate the margin. We now have two conflicting objectives: make the slack variables as small as possible to reduce the margin violations, and make  $\\frac{1}{2} w^T w $ as small as possible to increase the margin. This is where the C hyperparameter comes in: it allows us to define the tradeoff between these two objectives. This gives us the constrained optimization problem in Equation 5-4.\n",
    "\n",
    "Eqn 5-4 Softmargin linear svm classifier\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\underset{\\mathbf{w}, b, \\zeta}{\\operatorname{minimize}} & \\quad \\frac{1}{2} \\mathbf{w}^{\\top} \\mathbf{w}+C \\sum_{i=1}^{m} \\zeta^{(i)} \\\\\n",
    "\\text { subject to } & \\quad t^{(i)}\\left(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)}+b\\right) \\geq 1-\\zeta^{(i)} \\quad \\text { and } \\quad \\zeta^{(i)} \\geq 0 \\text { for } i=1,2, \\cdots, m\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "# Quadratic Programming\n",
    "\n",
    "The hard margin and soft margin problems are both convex quadratic optimization problems with linear constraints. Such problems are known as Quadratic Programming (QP) problems. Many off-the-shelf solvers are available to solve QP problems by using a variety of techniques that are outside the scope of this book.\n",
    "\n",
    "The general problem formulation is given by Equation 5-5.\n",
    "\n",
    "Eqn 5-5 \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\underset{\\mathbf{w}, b, \\zeta}{\\operatorname{minimize}} & \\quad \\frac{1}{2} p^T H p+f^⊺p \\\\\n",
    "\\text { subject to } & Ap \\leq b\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where \n",
    "\n",
    "$$ \n",
    "\\begin{cases} \n",
    "      p & \\text{is an} \\space n_p - \\text{dimensional vector} \\space (n_p=\\text{number of parameters}), \\\\ \n",
    "      H & \\text{is an} \\space n_p×n_p \\space \\text{matrix} \\\\\n",
    "      f & \\text{is an} \\space n_p - \\text{dimensional vector} \\\\\n",
    "      A & \\text{is an} \\space n_c × n_p \\text{matrix}(n_c=\\text{numberof constraints}) \\\\\n",
    "      b & \\text{is an} \\space n_c - \\text{dimensional vector} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Note that the expression $A p ≤ b$ defines $n_c$ constraints: $p^⊺ a^{(i)} ≤ b^{(i)} \\text{for} i = 1, 2, ⋯, n_c$, where $a^{(i)}$ is the vector containing the elements of the $i^{th}$ row of $A$ and $b^{(i)}$ is the $i^{th}$ element of b.\n",
    "\n",
    "You can easily verify that if you set the QP parameters in the following way, you get the hard margin linear SVM classifier objective:\n",
    "\n",
    "- $n_p = n + 1$, where $n$ is the number of features (the +1 is for the bias term).\n",
    "\n",
    "- $n_c = m$, where m is the number of training instances.\n",
    "\n",
    "- $H$ is the $n_p × n_p$ identity matrix, except with a zero in the top-left cell (to ignore the bias term).\n",
    "\n",
    "- $f = 0$, an $n_p$-dimensional vector full of 0s.\n",
    "\n",
    "- b = –1, an $n_c$-dimensional vector full of –1s.\n",
    "\n",
    "- $a^{(i)} = –t^{(i)} \\dot{x}^{(i)}$, where $\\dot{x}^{(i)}$ is equal to $x^{(i)}$ with an extra bias feature $\\dot{x}_0 = 1$.\n",
    "\n",
    "One way to train a hard margin linear SVM classifier is to use an off-the-shelf QP solver and pass it the preceding parameters. The resulting vector $p$ will contain the bias term $b = p_0$ and the feature weights $w_i = p_i$ for $i = 1, 2, ⋯, n$. Similarly, you can use a QP solver to solve the soft margin problem (see the exercises at the end of the chapter).\n",
    "\n",
    "To use the kernel trick, we are going to look at a different constrained optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dual Problem\n",
    "\n",
    "Given a constrained optimization problem, known as the primal problem, it is possible to express a different but closely related problem, called its dual problem. The solution to the dual problem typically gives a lower bound to the solution of the primal problem, but under some conditions it can have the same solution as the primal problem. Luckily, the SVM problem happens to meet these conditions, so you can choose to solve the primal problem or the dual problem; both will have the same solution. Equation 5-6 shows the dual form of the linear SVM objective (if you are interested in knowing how to derive the dual problem from the primal problem, see Appendix C).\n",
    "\n",
    "Eqn 5-6 Dual form of the linear SVM objective\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\underset{\\boldsymbol{\\alpha}}{\\operatorname{minimize}} \\quad \\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha^{(i)} \\alpha^{(j)} t^{(i)} t^{(j)} \\mathbf{x}^{(i)^{\\top}} \\mathbf{x}^{(j)}-\\sum_{i=1}^{m} \\alpha^{(i)}\\\\\n",
    "\\text { subject to } \\quad \\alpha^{(i)} \\geq 0 \\text { for all } i=1,2, \\ldots, m \\text { and } \\sum_{i=1}^{m} \\alpha^{(i)} t^{(i)}=0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Once you find the vector $\\hat{\\alpha}$ that minimizes this equation (using a QP solver), use Equation 5-7 to compute $\\hat{w}$ and $\\hat{b}$ that minimize the primal problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempting to Summarize Appendix C\n",
    "Using the _Lagrange multipliers_ method we can find a stationary point( a point where the partial derivative for our object function and constraints equal zero), the stationary point is important to optimizing what every it is we are working on. \n",
    "\n",
    "Apply the langrange method out hard margin Optimization problem we get: \n",
    "\n",
    "$$\n",
    "\\begin{array}{r}\n",
    "\\mathscr{L}(\\mathbf{w}, b, \\alpha)=\\frac{1}{2} \\mathbf{w}^{\\top} \\mathbf{w}-\\sum_{i=1}^{m} \\alpha^{(i)}\\left(t^{(i)}\\left(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)}+b\\right)-1\\right) \\\\\n",
    "\\text { with } \\alpha^{(i)} \\geq 0 \\quad \\text { for } i=1,2, \\cdots, m\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $t^{(i)}(w^⊺ x^{(i)} + b) ≥ 1$\n",
    "- $\\frac{1}{2} \\mathbf{w}^{\\top}w$ has a derivative. \n",
    "- $ \\hat{\\alpha} \\geq 0 $\n",
    "- Either $\\hat{\\alpha}=0$ or the ith constraint must be an active constraint, meaning it must hold by equality:  $t^{(i)}(w^⊺ x^{(i)} + b) ≥ 1$. This condition is called the complementary slackness condition. It implies that either $\\hat{\\alpha}^{(i)}=0$ or the $i^{th}$ instance lies on the boundary (it is a support vector).\n",
    "\n",
    "With this we can now take the parital deriviates which give us \n",
    "$$\n",
    "\\begin{array}{r}\n",
    "\\nabla_{\\mathbf{w}} \\mathscr{L}(\\mathbf{w}, b, \\alpha)=\\mathbf{w}-\\sum_{i=1}^{m} \\alpha^{(i)} t^{(i)} \\mathbf{x}^{(i)} \\\\\n",
    "\\frac{\\partial}{\\partial b} \\mathscr{L}(\\mathbf{w}, b, \\alpha)=-\\sum_{i=1}^{m} \\alpha^{(i)} t^{(i)}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Now setting the partials equal to zero we have \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\mathbf{w}}=& \\sum_{i=1}^{m} \\widehat{\\alpha}^{(i)} t^{(i)} \\mathbf{x}^{(i)} \\\\\n",
    "& \\sum_{i=1}^{m} \\widehat{\\alpha}^{(i)} t^{(i)}=0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If we expand the generalized lagrange equation and substitute our optimized properties(the ones with the hats)\n",
    "\n",
    "We arrive at the duality equation. \n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\mathscr{L}(\\widehat{\\mathbf{w}}, \\widehat{\\mathbf{b}}, \\boldsymbol{\\alpha})=\\sum_{i=1}^{m} \\alpha^{(i)}-\\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha^{(i)} \\alpha^{(j)} t^{(i)} t^{(j)} \\mathbf{x}^{(i) \\top} \\mathbf{x}^{(j)} \\\\\n",
    "\\text { subject to } \\alpha^{(i)} \\geq 0 \\text { for } i=1,2, \\ldots, m \\text { and } \\sum_{i=1}^{m} \\alpha^{(i)} t^{(i)}=0\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "From there we find an $\\hat{\\alpha}$ that minimizes the objective function \n",
    "\n",
    "To compute the bias term we can use \n",
    "\n",
    "$$\\hat{b}=\\frac{1}{n_{s}} \\sum_{i=1}^{m}\\left[t^{(i)}-\\widehat{\\mathbf{w}}^{\\top} \\mathbf{x}^{(i)}\\right]$$\n",
    "\n",
    "Where ns is the number of support vectors.\n",
    "\n",
    "The dual problem is faster to solve than the primal one when the number of training instances is smaller than the number of features. More importantly, the dual problem makes the kernel trick possible, while the primal does not. So what is this kernel trick, anyway?\n",
    "\n",
    "# Kernelized SVMs\n",
    "\n",
    "Suppose you want to apply a second-degree polynomial transformation to a two-dimensional training set (such as the moons training set), then train a linear SVM classifier on the transformed training set. Equation 5-8 shows the second-degree polynomial mapping function $\\phi$ that you want to apply.\n",
    "\n",
    "Eqn 5-9:Second Degree Polynomial Mapping \n",
    "\n",
    "$$\n",
    "\\phi(\\mathbf{x})=\\phi\\left(\\left(\\begin{array}{c}\n",
    "x_{1} \\\\\n",
    "x_{2}\n",
    "\\end{array}\\right)\\right)=\\left(\\begin{array}{c}\n",
    "x_{1}^{2} \\\\\n",
    "\\sqrt{2} x_{1} x_{2} \\\\\n",
    "x_{2}{ }^{2}\n",
    "\\end{array}\\right)\n",
    "$$ \n",
    "\n",
    "Notice that the transformed vector is 3D instead of 2D. Now let’s look at what happens to a couple of 2D vectors, a and b, if we apply this second-degree polynomial mapping and then compute the dot product of the transformed vectors\n",
    "\n",
    "Eqn 5-9: Kernal trick for a second degree polynomial mapping. \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\phi(\\mathbf{a})^{\\top} \\phi(\\mathbf{b}) &=\\left(\\begin{array}{c}\n",
    "a_{1}^{2} \\\\\n",
    "\\sqrt{2} a_{1} a_{2} \\\\\n",
    "a_{2}^{2}\n",
    "\\end{array}\\right)^{\\top}\\left(\\begin{array}{c}\n",
    "b_{1}^{2} \\\\\n",
    "\\sqrt{2} b_{1} b_{2} \\\\\n",
    "b_{2}^{2}\n",
    "\\end{array}\\right)=a_{1}^{2} b_{1}^{2}+2 a_{1} b_{1} a_{2} b_{2}+a_{2}^{2} b_{2}^{2} \\\\\n",
    "=&\\left(a_{1} b_{1}+a_{2} b_{2}\\right)^{2}=\\left(\\left(\\begin{array}{c}\n",
    "a_{1} \\\\\n",
    "a_{2}\n",
    "\\end{array}\\right)^{\\top}\\left(\\begin{array}{l}\n",
    "b_{1} \\\\\n",
    "b_{2}\n",
    "\\end{array}\\right)\\right)^{2}=\\left(\\mathbf{a}^{\\top} \\mathbf{b}\\right)^{2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "That is, the dot product of the transformed vectors is equal to the square of the dot product of the original vectors: $\n",
    "\\phi(\\mathbf{a})^{\\top} \\phi(\\mathbf{b}) = \\left(\\mathbf{a}^{\\top} \\mathbf{b}\\right)^{2}$ \n",
    "\n",
    "Here is the key insight: if you apply the transformation $\\phi$ to all training instances, then the dual problem will contain the dot product $\\phi(x^{(i)})^T \\phi(x^{(j)})$. But if $\\phi$ is the second-degree polynomial transformation defined in Equation 5-8, then you can replace this dot product of transformed vectors simply by $ (x^{(i)^T} x^{(j)})^2 $. So, you don’t need to transform the training instances at all; just replace the dot product by its square in Equation 5-6. The result will be strictly the same as if you had gone through the trouble of transforming the training set then fitting a linear SVM algorithm, but this trick makes the whole process much more computationally efficient.\n",
    "\n",
    "The function $K(a, b) = (a^T b)^2$ is a second-degree polynomial kernel. In Machine Learning, a kernel is a function capable of computing the dot product $\\phi(a)^T \\phi(b)$, based only on the original vectors a and b, without having to compute (or even to know about) the transformation $\\phi.$ Equation 5-10 lists some of the most commonly used kernels\n",
    "\n",
    "Eqn 5-10:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text { Linear: } & K(\\mathbf{a}, \\mathbf{b})=\\mathbf{a}^{\\top} \\mathbf{b} \\\\\n",
    "\\text { Polynomial: } & K(\\mathbf{a}, \\mathbf{b})=\\left(\\gamma \\mathbf{a}^{\\top} \\mathbf{b}+r\\right)^{d} \\\\\n",
    "\\text { Gaussian RBF: } & K(\\mathbf{a}, \\mathbf{b})=\\exp \\left(-\\gamma\\|\\mathbf{a}-\\mathbf{b}\\|^{2}\\right) \\\\\n",
    "\\text { Sigmoid: } & K(\\mathbf{a}, \\mathbf{b})=\\tanh \\left(\\gamma \\mathbf{a}^{\\top} \\mathbf{b}+r\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "### Mercer's Theorem \n",
    "According to Mercer’s theorem, if a function K(a, b) respects a few mathematical conditions called Mercer’s conditions (e.g., K must be continuous and symmetric in its arguments so that K(a, b) = K(b, a), etc.), then there exists a function ϕ that maps a and b into another space (possibly with much higher dimensions) such that K(a, b) = ϕ(a)⊺ ϕ(b). You can use K as a kernel because you know ϕ exists, even if you don’t know what ϕ is. In the case of the Gaussian RBF kernel, it can be shown that ϕ maps each training instance to an infinite-dimensional space, so it’s a good thing you don’t need to actually perform the mapping!\n",
    "\n",
    "Note that some frequently used kernels (such as the sigmoid kernel) don’t respect all of Mercer’s conditions, yet they generally work well in practice.\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "\n",
    "There is still one loose end we must tie up. Equation 5-7 shows how to go from the dual solution to the primal solution in the case of a linear SVM classifier. But if you apply the kernel trick, you end up with equations that include ϕ(x(i)). In fact, $\\hat{w}$ must have the same number of dimensions as $\\phi(x(i))$, which may be huge or even infinite, so you can’t compute it. But how can you make predictions without knowing wˆ? Well, the good news is that you can plug the formula for $\\hat{w}$ from Equation 5-7 into the decision function for a new instance $x^{(n)}$, and you get an equation with only dot products between input vectors. This makes it possible to use the kernel trick (Equation 5-11).\n",
    "\n",
    "Eqn 5-11: Making prediciton with a kernelized SVM \n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_{\\widehat{\\mathbf{w}}, \\hat{b}}\\left(\\phi\\left(\\mathbf{x}^{(n)}\\right)\\right) &=\\widehat{\\mathbf{w}}^{\\top} \\phi\\left(\\mathbf{x}^{(n)}\\right)+\\hat{b}=\\left(\\sum_{i=1}^{m} \\widehat{\\alpha}^{(i)} t^{(i)} \\phi\\left(\\mathbf{x}^{(i)}\\right)\\right)^{\\top} \\phi\\left(\\mathbf{x}^{(n)}\\right)+\\hat{b} \\\\\n",
    "&=\\sum_{i=1}^{m} \\widehat{\\alpha}^{(i)} t^{(i)}\\left(\\phi\\left(\\mathbf{x}^{(i)}\\right)^{\\top} \\phi\\left(\\mathbf{x}^{(n)}\\right)\\right)+\\hat{b} \\\\\n",
    "&=\\sum_{i=1 \\atop \\hat{\\alpha}^{(i)}>0}^{m} \\widehat{\\alpha}^{(i)} t^{(i)} K\\left(\\mathbf{x}^{(i)}, \\mathbf{x}^{(n)}\\right)+\\hat{b}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that since $\\alpha^{(i)} \\neq 0$ only for support vectors, making predictions involves computing the dot product of the new input vector $x^{(n)}$ with only the support vectors, not all the training instances. Of course, you need to use the same trick to compute the bias term $\\hat{b}$ (Equation 5-12).\n",
    "\n",
    "- Note: Our goal is maximize the margins or street. The streets end where our support vectors are, thus we only care about the support vectors when finding a statonary point($\\alpha$). \n",
    "\n",
    "Equation 5-12. Using the kernel trick to compute the bias term\n",
    "![Eqn 5-12](https://cdn.mathpix.com/snip/images/jb0Qk0vjFHkJVaV2C0ZzImypWBZci1DblWVfYOyNTFo.original.fullsize.png)\n",
    "\n",
    "\n",
    "# Online SVMs\n",
    "\n",
    "Before concluding this chapter, let’s take a quick look at online SVM classifiers (recall that online learning means learning incrementally\n",
    "\n",
    "For linear SVM classifiers, one method for implementing an online SVM classifier is to use Gradient Descent (e.g., using SGDClassifier) to minimize the cost function in Equation 5-13, which is derived from the primal problem. Unfortunately, Gradient Descent converges much more slowly than the methods based on QP.\n",
    "\n",
    "Equation 5-13. Linear SVM classifier cost function\n",
    "$$\n",
    "J(\\mathbf{w}, b)=\\frac{1}{2} \\mathbf{w}^{\\top} \\mathbf{w} \\quad+C \\sum_{i=1}^{m} \\max \\left(0,1-t^{(i)}\\left(\\mathbf{w}^{\\top} \\mathbf{x}^{(i)}+b\\right)\\right)\n",
    "$$\n",
    "\n",
    "The first sum in the cost function will push the model to have a small weight vector $w$, leading to a larger margin. The second sum computes the total of all margin violations. An instance’s margin violation is equal to 0 if it is located off the street and on the correct side, or else it is proportional to the distance to the correct side of the street. Minimizing this term ensures that the model makes the margin violations as small and as few as possible.\n",
    "\n",
    "# HINGE LOSS\n",
    "\n",
    "The function $max(0, 1 – t)$ is called the hinge loss function (see the following image). It is equal to 0 when t ≥ 1. Its derivative (slope) is equal to $–1$ if $t < 1$ and $0$ if $t > 1$. It is not differentiable at $t = 1$, but just like for Lasso Regression (see “Lasso Regression”), you can still use Gradient Descent using any subderivative at $t = 1$ (i.e., any value between $–1$ and $0$).\n",
    "\n",
    "![Hinge Loss](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/assets/mls2_0514.png) \n",
    "\n",
    "It is also possible to implement online kernelized SVMs, as described in the papers “Incremental and Decremental Support Vector Machine Learning” and “Fast Kernel Classifiers with Online and Active Learning”. These kernelized SVMs are implemented in Matlab and C++. For large-scale nonlinear problems, you may want to consider using neural networks instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
