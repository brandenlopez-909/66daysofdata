{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  The goal of this chapter is to answer the following questions:\n",
    "\n",
    "How can we write programs to access text from local files and from the web, in order to get hold of an unlimited range of language material?\n",
    "\n",
    "How can we split documents up into individual words and punctuation symbols, so we can carry out the same kinds of analysis we did with text corpora in earlier chapters?\n",
    "\n",
    "How can we write programs to produce formatted output and save it in a file?\n",
    "\n",
    "In order to address these questions, we will be covering key concepts in NLP, including tokenization and stemming. Along the way you will consolidate your Python knowledge and learn about strings, files, and regular expressions. Since so much text on the web is in HTML format, we will also see how to dispense with markup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Accessing Text from the Web and from Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib import request\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "#The read() process will take a few seconds as it downloads this large book.\n",
    "raw = response.read().decode('utf8')\n",
    "type(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\\r'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[:75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the \\r and \\n in the opening line of the file, which is how Python displays the special carriage return and line feed characters (the file must have been created on a Windows machine). For our language processing, we want to break up the string into words and punctuation, as we saw in 1.. This step is called tokenization, and it produces our familiar structure, a list of words and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffThe',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'EBook',\n",
       " 'of',\n",
       " 'Crime',\n",
       " 'and',\n",
       " 'Punishment',\n",
       " ',',\n",
       " 'by']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(raw)\n",
    "tokens[:10]\n",
    "#I guess we have to like with the \\ufeffThe? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that NLTK was needed for tokenization, but not for any of the earlier tasks of opening a URL and reading it into a string.\n",
    "\n",
    "If we now take the further step of creating an NLTK text from this list, we can carry out all of the other linguistic processing we saw in 1., along with the regular list operations like slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an',\n",
       " 'exceptionally',\n",
       " 'hot',\n",
       " 'evening',\n",
       " 'early',\n",
       " 'in',\n",
       " 'July',\n",
       " 'a',\n",
       " 'young',\n",
       " 'man',\n",
       " 'came',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'garret',\n",
       " 'in',\n",
       " 'which',\n",
       " 'he',\n",
       " 'lodged',\n",
       " 'in',\n",
       " 'S.',\n",
       " 'Place',\n",
       " 'and',\n",
       " 'walked',\n",
       " 'slowly',\n",
       " ',',\n",
       " 'as',\n",
       " 'though',\n",
       " 'in',\n",
       " 'hesitation',\n",
       " ',',\n",
       " 'towards',\n",
       " 'K.',\n",
       " 'bridge',\n",
       " '.',\n",
       " 'He',\n",
       " 'had',\n",
       " 'successfully']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text = nltk.Text(tokens)\n",
    "text[1024:1062]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old woman; Project Gutenberg-tm; Porfiry Petrovitch; Amalia Ivanovna; great deal; young man; Nikodim Fomitch; Ilya Petrovitch; Project Gutenberg; Andrey Semyonovitch; Hay Market; Dmitri Prokofitch; Good heavens\n"
     ]
    }
   ],
   "source": [
    "# the text.collocations() is broken, instead use\n",
    "print(\"; \".join(text.collocation_list()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that Project Gutenberg appears as a collocation. This is because each text downloaded from Project Gutenberg contains a header with the name of the text, the author, the names of people who scanned and corrected the text, a license, and so on. \n",
    "\n",
    "Sometimes this information appears in a footer at the end of the file. We cannot reliably detect where the content begins and ends, and so have to resort to manual inspection of the file, to discover unique strings that mark the beginning and the end, before trimming raw to be just the content and nothing else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rfind() (\"reverse find\")\n",
    "raw.rfind(\"End of Project Gutenberg's Crime\")\n",
    "# methods help us get the right index values to use for slicing the string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web texts found on the web may contain unwanted material, and there may not be an automatic way to remove it. But with a small amount of extra work we can extract the material we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with HTML\n",
    "\n",
    "As you know websites have information stored in HTML (usually), we need to use python libraries to unlock the content. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BBC',\n",
       " 'NEWS',\n",
       " '|',\n",
       " 'Health',\n",
       " '|',\n",
       " 'Blondes',\n",
       " \"'to\",\n",
       " 'die',\n",
       " 'out',\n",
       " 'in',\n",
       " '200',\n",
       " \"years'\",\n",
       " 'NEWS',\n",
       " 'SPORT',\n",
       " 'WEATHER',\n",
       " 'WORLD',\n",
       " 'SERVICE',\n",
       " 'A-Z',\n",
       " 'INDEX',\n",
       " 'SEARCH',\n",
       " 'You',\n",
       " 'are',\n",
       " 'in',\n",
       " ':',\n",
       " 'Health',\n",
       " 'News',\n",
       " 'Front',\n",
       " 'Page',\n",
       " 'Africa',\n",
       " 'Americas',\n",
       " 'Asia-Pacific',\n",
       " 'Europe',\n",
       " 'Middle',\n",
       " 'East',\n",
       " 'South',\n",
       " 'Asia',\n",
       " 'UK',\n",
       " 'Business',\n",
       " 'Entertainment',\n",
       " 'Science/Nature',\n",
       " 'Technology',\n",
       " 'Health',\n",
       " 'Medical',\n",
       " 'notes',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '-',\n",
       " 'Talking',\n",
       " 'Point',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '-',\n",
       " 'Country',\n",
       " 'Profiles',\n",
       " 'In',\n",
       " 'Depth',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '-',\n",
       " 'Programmes',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '-',\n",
       " 'SERVICES',\n",
       " 'Daily',\n",
       " 'E-mail',\n",
       " 'News',\n",
       " 'Ticker',\n",
       " 'Mobile/PDAs',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '-',\n",
       " 'Text',\n",
       " 'Only',\n",
       " 'Feedback',\n",
       " 'Help',\n",
       " 'EDITIONS',\n",
       " 'Change',\n",
       " 'to',\n",
       " 'UK',\n",
       " 'Friday',\n",
       " ',',\n",
       " '27',\n",
       " 'September',\n",
       " ',',\n",
       " '2002',\n",
       " ',',\n",
       " '11:51',\n",
       " 'GMT',\n",
       " '12:51',\n",
       " 'UK',\n",
       " 'Blondes',\n",
       " \"'to\",\n",
       " 'die',\n",
       " 'out',\n",
       " 'in',\n",
       " '200',\n",
       " \"years'\",\n",
       " 'Scientists',\n",
       " 'believe',\n",
       " 'the',\n",
       " 'last',\n",
       " 'blondes',\n",
       " 'will',\n",
       " 'be',\n",
       " 'in',\n",
       " 'Finland',\n",
       " 'The',\n",
       " 'last',\n",
       " 'natural',\n",
       " 'blondes',\n",
       " 'will',\n",
       " 'die',\n",
       " 'out',\n",
       " 'within',\n",
       " '200',\n",
       " 'years',\n",
       " ',',\n",
       " 'scientists',\n",
       " 'believe',\n",
       " '.',\n",
       " 'A',\n",
       " 'study',\n",
       " 'by',\n",
       " 'experts',\n",
       " 'in',\n",
       " 'Germany',\n",
       " 'suggests',\n",
       " 'people',\n",
       " 'with',\n",
       " 'blonde',\n",
       " 'hair',\n",
       " 'are',\n",
       " 'an',\n",
       " 'endangered',\n",
       " 'species',\n",
       " 'and',\n",
       " 'will',\n",
       " 'become',\n",
       " 'extinct',\n",
       " 'by',\n",
       " '2202',\n",
       " '.',\n",
       " 'Researchers',\n",
       " 'predict',\n",
       " 'the',\n",
       " 'last',\n",
       " 'truly',\n",
       " 'natural',\n",
       " 'blonde',\n",
       " 'will',\n",
       " 'be',\n",
       " 'born',\n",
       " 'in',\n",
       " 'Finland',\n",
       " '-',\n",
       " 'the',\n",
       " 'country',\n",
       " 'with',\n",
       " 'the',\n",
       " 'highest',\n",
       " 'proportion',\n",
       " 'of',\n",
       " 'blondes',\n",
       " '.',\n",
       " 'The',\n",
       " 'frequency',\n",
       " 'of',\n",
       " 'blondes',\n",
       " 'may',\n",
       " 'drop',\n",
       " 'but',\n",
       " 'they',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'disappear',\n",
       " 'Prof',\n",
       " 'Jonathan',\n",
       " 'Rees',\n",
       " ',',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Edinburgh',\n",
       " 'But',\n",
       " 'they',\n",
       " 'say',\n",
       " 'too',\n",
       " 'few',\n",
       " 'people',\n",
       " 'now',\n",
       " 'carry',\n",
       " 'the',\n",
       " 'gene',\n",
       " 'for',\n",
       " 'blondes',\n",
       " 'to',\n",
       " 'last',\n",
       " 'beyond',\n",
       " 'the',\n",
       " 'next',\n",
       " 'two',\n",
       " 'centuries',\n",
       " '.',\n",
       " 'The',\n",
       " 'problem',\n",
       " 'is',\n",
       " 'that',\n",
       " 'blonde',\n",
       " 'hair',\n",
       " 'is',\n",
       " 'caused',\n",
       " 'by',\n",
       " 'a',\n",
       " 'recessive',\n",
       " 'gene',\n",
       " '.',\n",
       " 'In',\n",
       " 'order',\n",
       " 'for',\n",
       " 'a',\n",
       " 'child',\n",
       " 'to',\n",
       " 'have',\n",
       " 'blonde',\n",
       " 'hair',\n",
       " ',',\n",
       " 'it',\n",
       " 'must',\n",
       " 'have',\n",
       " 'the',\n",
       " 'gene',\n",
       " 'on',\n",
       " 'both',\n",
       " 'sides',\n",
       " 'of',\n",
       " 'the',\n",
       " 'family',\n",
       " 'in',\n",
       " 'the',\n",
       " 'grandparents',\n",
       " \"'\",\n",
       " 'generation',\n",
       " '.',\n",
       " 'Dyed',\n",
       " 'rivals',\n",
       " 'The',\n",
       " 'researchers',\n",
       " 'also',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'so-called',\n",
       " 'bottle',\n",
       " 'blondes',\n",
       " 'may',\n",
       " 'be',\n",
       " 'to',\n",
       " 'blame',\n",
       " 'for',\n",
       " 'the',\n",
       " 'demise',\n",
       " 'of',\n",
       " 'their',\n",
       " 'natural',\n",
       " 'rivals',\n",
       " '.',\n",
       " 'They',\n",
       " 'suggest',\n",
       " 'that',\n",
       " 'dyed-blondes',\n",
       " 'are',\n",
       " 'more',\n",
       " 'attractive',\n",
       " 'to',\n",
       " 'men',\n",
       " 'who',\n",
       " 'choose',\n",
       " 'them',\n",
       " 'as',\n",
       " 'partners',\n",
       " 'over',\n",
       " 'true',\n",
       " 'blondes',\n",
       " '.',\n",
       " 'Bottle-blondes',\n",
       " 'like',\n",
       " 'Ann',\n",
       " 'Widdecombe',\n",
       " 'may',\n",
       " 'be',\n",
       " 'to',\n",
       " 'blame',\n",
       " 'But',\n",
       " 'Jonathan',\n",
       " 'Rees',\n",
       " ',',\n",
       " 'professor',\n",
       " 'of',\n",
       " 'dermatology',\n",
       " 'at',\n",
       " 'the',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Edinburgh',\n",
       " 'said',\n",
       " 'it',\n",
       " 'was',\n",
       " 'unlikely',\n",
       " 'blondes',\n",
       " 'would',\n",
       " 'die',\n",
       " 'out',\n",
       " 'completely',\n",
       " '.',\n",
       " '``',\n",
       " 'Genes',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'die',\n",
       " 'out',\n",
       " 'unless',\n",
       " 'there',\n",
       " 'is',\n",
       " 'a',\n",
       " 'disadvantage',\n",
       " 'of',\n",
       " 'having',\n",
       " 'that',\n",
       " 'gene',\n",
       " 'or',\n",
       " 'by',\n",
       " 'chance',\n",
       " '.',\n",
       " 'They',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'disappear',\n",
       " ',',\n",
       " \"''\",\n",
       " 'he',\n",
       " 'told',\n",
       " 'BBC',\n",
       " 'News',\n",
       " 'Online',\n",
       " '.',\n",
       " '``',\n",
       " 'The',\n",
       " 'only',\n",
       " 'reason',\n",
       " 'blondes',\n",
       " 'would',\n",
       " 'disappear',\n",
       " 'is',\n",
       " 'if',\n",
       " 'having',\n",
       " 'the',\n",
       " 'gene',\n",
       " 'was',\n",
       " 'a',\n",
       " 'disadvantage',\n",
       " 'and',\n",
       " 'I',\n",
       " 'do',\n",
       " 'not',\n",
       " 'think',\n",
       " 'that',\n",
       " 'is',\n",
       " 'the',\n",
       " 'case',\n",
       " '.',\n",
       " '``',\n",
       " 'The',\n",
       " 'frequency',\n",
       " 'of',\n",
       " 'blondes',\n",
       " 'may',\n",
       " 'drop',\n",
       " 'but',\n",
       " 'they',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'disappear',\n",
       " '.',\n",
       " \"''\",\n",
       " 'See',\n",
       " 'also',\n",
       " ':',\n",
       " '28',\n",
       " 'Mar',\n",
       " '01',\n",
       " '|',\n",
       " 'Education',\n",
       " 'What',\n",
       " 'is',\n",
       " 'it',\n",
       " 'about',\n",
       " 'blondes',\n",
       " '?',\n",
       " '09',\n",
       " 'Apr',\n",
       " '99',\n",
       " '|',\n",
       " 'Health',\n",
       " 'Platinum',\n",
       " 'blondes',\n",
       " 'are',\n",
       " 'labelled',\n",
       " 'as',\n",
       " 'dumb',\n",
       " '17',\n",
       " 'Apr',\n",
       " '02',\n",
       " '|',\n",
       " 'Health',\n",
       " 'Hair',\n",
       " 'dye',\n",
       " 'cancer',\n",
       " 'alert',\n",
       " 'Internet',\n",
       " 'links',\n",
       " ':',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Edinburgh',\n",
       " 'The',\n",
       " 'BBC',\n",
       " 'is',\n",
       " 'not',\n",
       " 'responsible',\n",
       " 'for',\n",
       " 'the',\n",
       " 'content',\n",
       " 'of',\n",
       " 'external',\n",
       " 'internet',\n",
       " 'sites',\n",
       " 'Top',\n",
       " 'Health',\n",
       " 'stories',\n",
       " 'now',\n",
       " ':',\n",
       " 'Heart',\n",
       " 'risk',\n",
       " 'link',\n",
       " 'to',\n",
       " 'big',\n",
       " 'families',\n",
       " 'Back',\n",
       " 'pain',\n",
       " 'drug',\n",
       " \"'may\",\n",
       " 'aid',\n",
       " \"diabetics'\",\n",
       " 'Congo',\n",
       " 'Ebola',\n",
       " 'outbreak',\n",
       " 'confirmed',\n",
       " 'Vegetables',\n",
       " 'ward',\n",
       " 'off',\n",
       " \"Alzheimer's\",\n",
       " 'Polio',\n",
       " 'campaign',\n",
       " 'launched',\n",
       " 'in',\n",
       " 'Iraq',\n",
       " 'Gene',\n",
       " 'defect',\n",
       " 'explains',\n",
       " 'high',\n",
       " 'blood',\n",
       " 'pressure',\n",
       " 'Botox',\n",
       " \"'may\",\n",
       " 'cause',\n",
       " 'new',\n",
       " \"wrinkles'\",\n",
       " 'Alien',\n",
       " \"'abductees\",\n",
       " \"'\",\n",
       " 'show',\n",
       " 'real',\n",
       " 'symptoms',\n",
       " 'Links',\n",
       " 'to',\n",
       " 'more',\n",
       " 'Health',\n",
       " 'stories',\n",
       " 'are',\n",
       " 'at',\n",
       " 'the',\n",
       " 'foot',\n",
       " 'of',\n",
       " 'the',\n",
       " 'page',\n",
       " '.',\n",
       " 'E-mail',\n",
       " 'this',\n",
       " 'story',\n",
       " 'to',\n",
       " 'a',\n",
       " 'friend',\n",
       " 'Links',\n",
       " 'to',\n",
       " 'more',\n",
       " 'Health',\n",
       " 'stories',\n",
       " 'In',\n",
       " 'This',\n",
       " 'Section',\n",
       " 'Heart',\n",
       " 'risk',\n",
       " 'link',\n",
       " 'to',\n",
       " 'big',\n",
       " 'families',\n",
       " 'Back',\n",
       " 'pain',\n",
       " 'drug',\n",
       " \"'may\",\n",
       " 'aid',\n",
       " \"diabetics'\",\n",
       " 'Congo',\n",
       " 'Ebola',\n",
       " 'outbreak',\n",
       " 'confirmed',\n",
       " 'Vegetables',\n",
       " 'ward',\n",
       " 'off',\n",
       " \"Alzheimer's\",\n",
       " 'Polio',\n",
       " 'campaign',\n",
       " 'launched',\n",
       " 'in',\n",
       " 'Iraq',\n",
       " 'Gene',\n",
       " 'defect',\n",
       " 'explains',\n",
       " 'high',\n",
       " 'blood',\n",
       " 'pressure',\n",
       " 'Botox',\n",
       " \"'may\",\n",
       " 'cause',\n",
       " 'new',\n",
       " \"wrinkles'\",\n",
       " 'Alien',\n",
       " \"'abductees\",\n",
       " \"'\",\n",
       " 'show',\n",
       " 'real',\n",
       " 'symptoms',\n",
       " 'How',\n",
       " 'sperm',\n",
       " 'wriggle',\n",
       " 'Bollywood',\n",
       " 'told',\n",
       " 'to',\n",
       " 'stub',\n",
       " 'it',\n",
       " 'out',\n",
       " 'Fears',\n",
       " 'over',\n",
       " 'tuna',\n",
       " 'health',\n",
       " 'risk',\n",
       " 'to',\n",
       " 'babies',\n",
       " 'Public',\n",
       " 'can',\n",
       " 'be',\n",
       " 'taught',\n",
       " 'to',\n",
       " 'spot',\n",
       " 'strokes',\n",
       " '^^',\n",
       " 'Back',\n",
       " 'to',\n",
       " 'top',\n",
       " 'News',\n",
       " 'Front',\n",
       " 'Page',\n",
       " '|',\n",
       " 'Africa',\n",
       " '|',\n",
       " 'Americas',\n",
       " '|',\n",
       " 'Asia-Pacific',\n",
       " '|',\n",
       " 'Europe',\n",
       " '|',\n",
       " 'Middle',\n",
       " 'East',\n",
       " '|',\n",
       " 'South',\n",
       " 'Asia',\n",
       " '|',\n",
       " 'UK',\n",
       " '|',\n",
       " 'Business',\n",
       " '|',\n",
       " 'Entertainment',\n",
       " '|',\n",
       " 'Science/Nature',\n",
       " '|',\n",
       " 'Technology',\n",
       " '|',\n",
       " 'Health',\n",
       " '|',\n",
       " 'Talking',\n",
       " 'Point',\n",
       " '|',\n",
       " 'Country',\n",
       " 'Profiles',\n",
       " '|',\n",
       " 'In',\n",
       " 'Depth',\n",
       " '|',\n",
       " 'Programmes',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " 'To',\n",
       " 'BBC',\n",
       " 'Sport',\n",
       " '>',\n",
       " '>',\n",
       " '|',\n",
       " 'To',\n",
       " 'BBC',\n",
       " 'Weather',\n",
       " '>',\n",
       " '>',\n",
       " '|',\n",
       " 'To',\n",
       " 'BBC',\n",
       " 'World',\n",
       " 'Service',\n",
       " '>',\n",
       " '>',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '--',\n",
       " '©',\n",
       " 'MMIII',\n",
       " '|',\n",
       " 'News',\n",
       " 'Sources',\n",
       " '|',\n",
       " 'Privacy',\n",
       " '<',\n",
       " '!',\n",
       " '--',\n",
       " 'var',\n",
       " 'pCid=',\n",
       " \"''\",\n",
       " 'uk_bbc_0',\n",
       " \"''\",\n",
       " ';',\n",
       " 'var',\n",
       " 'w0=1',\n",
       " ';',\n",
       " 'var',\n",
       " 'refR=escape',\n",
       " '(',\n",
       " 'document.referrer',\n",
       " ')',\n",
       " ';',\n",
       " 'if',\n",
       " '(',\n",
       " 'refR.length',\n",
       " '>',\n",
       " '=252',\n",
       " ')',\n",
       " 'refR=refR.substring',\n",
       " '(',\n",
       " '0,252',\n",
       " ')',\n",
       " '+',\n",
       " \"''\",\n",
       " '...',\n",
       " \"''\",\n",
       " ';',\n",
       " '//',\n",
       " '--',\n",
       " '>',\n",
       " '<',\n",
       " '!',\n",
       " '--',\n",
       " 'var',\n",
       " 'w0=0',\n",
       " ';',\n",
       " '//',\n",
       " '--',\n",
       " '>',\n",
       " '<',\n",
       " '!',\n",
       " '--',\n",
       " 'if',\n",
       " '(',\n",
       " 'w0',\n",
       " ')',\n",
       " '{',\n",
       " 'var',\n",
       " 'imgN=',\n",
       " \"'\",\n",
       " '<',\n",
       " 'img',\n",
       " 'src=',\n",
       " \"''\",\n",
       " 'http',\n",
       " ':',\n",
       " '//server-uk.imrworldwide.com/cgi-bin/count',\n",
       " '?',\n",
       " \"ref='+\",\n",
       " 'refR+',\n",
       " \"'\",\n",
       " '&',\n",
       " \"cid='+pCid+\",\n",
       " \"'\",\n",
       " \"''\",\n",
       " 'width=1',\n",
       " 'height=1',\n",
       " '>',\n",
       " \"'\",\n",
       " ';',\n",
       " 'if',\n",
       " '(',\n",
       " 'navigator.userAgent.indexOf',\n",
       " '(',\n",
       " \"'Mac\",\n",
       " \"'\",\n",
       " ')',\n",
       " '!',\n",
       " '=-1',\n",
       " ')',\n",
       " '{',\n",
       " 'document.write',\n",
       " '(',\n",
       " 'imgN',\n",
       " ')',\n",
       " ';',\n",
       " '}',\n",
       " 'else',\n",
       " '{',\n",
       " 'document.write',\n",
       " '(',\n",
       " \"'\",\n",
       " '<',\n",
       " 'applet',\n",
       " 'code=',\n",
       " \"''\",\n",
       " 'Measure.class',\n",
       " \"''\",\n",
       " \"'+\",\n",
       " \"'codebase=\",\n",
       " \"''\",\n",
       " 'http',\n",
       " ':',\n",
       " '//server-uk.imrworldwide.com/',\n",
       " \"''\",\n",
       " \"'+'width=1\",\n",
       " 'height=2',\n",
       " '>',\n",
       " \"'+\",\n",
       " \"'\",\n",
       " '<',\n",
       " 'param',\n",
       " 'name=',\n",
       " \"''\",\n",
       " 'ref',\n",
       " \"''\",\n",
       " 'value=',\n",
       " \"''\",\n",
       " \"'+refR+\",\n",
       " \"'\",\n",
       " \"''\",\n",
       " '>',\n",
       " \"'+\",\n",
       " \"'\",\n",
       " '<',\n",
       " 'param',\n",
       " 'name=',\n",
       " \"''\",\n",
       " 'cid',\n",
       " \"''\",\n",
       " 'value=',\n",
       " \"''\",\n",
       " \"'+pCid+\",\n",
       " \"'\",\n",
       " \"''\",\n",
       " '>',\n",
       " '<',\n",
       " 'textflow',\n",
       " '>',\n",
       " \"'+imgN+\",\n",
       " \"'\",\n",
       " '<',\n",
       " '/textflow',\n",
       " '>',\n",
       " '<',\n",
       " '/applet',\n",
       " '>',\n",
       " \"'\",\n",
       " ')',\n",
       " ';',\n",
       " '}',\n",
       " '}',\n",
       " 'document.write',\n",
       " '(',\n",
       " '``',\n",
       " '<',\n",
       " 'COMMENT',\n",
       " '>',\n",
       " \"''\",\n",
       " ')',\n",
       " ';',\n",
       " '//',\n",
       " '--',\n",
       " '>',\n",
       " 'var',\n",
       " 'si',\n",
       " '=',\n",
       " 'document.location+',\n",
       " \"''\",\n",
       " \"''\",\n",
       " ';',\n",
       " 'var',\n",
       " 'tsi',\n",
       " '=',\n",
       " 'si.replace',\n",
       " '(',\n",
       " '``',\n",
       " '.stm',\n",
       " \"''\",\n",
       " ',',\n",
       " \"''\",\n",
       " \"''\",\n",
       " ')',\n",
       " '.substr',\n",
       " '(',\n",
       " 'si.length-11',\n",
       " ',',\n",
       " 'si.length',\n",
       " ')',\n",
       " ';',\n",
       " 'if',\n",
       " '(',\n",
       " '!',\n",
       " 'tsi.match',\n",
       " '(',\n",
       " '/\\\\d\\\\d\\\\d\\\\d\\\\d\\\\d\\\\d/',\n",
       " ')',\n",
       " ')',\n",
       " '{',\n",
       " 'tsi',\n",
       " '=',\n",
       " '0',\n",
       " ';',\n",
       " '}',\n",
       " 'document.write',\n",
       " '(',\n",
       " \"'\",\n",
       " '<',\n",
       " 'img',\n",
       " 'src=',\n",
       " \"''\",\n",
       " 'http',\n",
       " ':',\n",
       " '//stats.bbc.co.uk/o.gif',\n",
       " '?',\n",
       " '~RS~s~RS~News~RS~t~RS~HighWeb_Legacy~RS~i~RS~',\n",
       " \"'\",\n",
       " '+',\n",
       " 'tsi',\n",
       " '+',\n",
       " \"'~RS~p~RS~0~RS~u~RS~/2/hi/health/2284783.stm~RS~r~RS~\",\n",
       " '(',\n",
       " 'none',\n",
       " ')',\n",
       " '~RS~a~RS~International~RS~q~RS~~RS~z~RS~32~RS~',\n",
       " \"''\",\n",
       " '>',\n",
       " \"'\",\n",
       " ')',\n",
       " ';']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "raw = BeautifulSoup(html, 'html.parser').get_text()\n",
    "tokens = word_tokenize(raw)\n",
    "tokens\n",
    "#This still contains unwanted material concerning site navigation and related stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 5 matches:\n",
      "hey say too few people now carry the gene for blondes to last beyond the next \n",
      "blonde hair is caused by a recessive gene . In order for a child to have blond\n",
      " have blonde hair , it must have the gene on both sides of the family in the g\n",
      "ere is a disadvantage of having that gene or by chance . They do n't disappear\n",
      "des would disappear is if having the gene was a disadvantage and I do not thin\n"
     ]
    }
   ],
   "source": [
    "# With some trial and error you can find the start and end indexes of the content \n",
    "# and select the tokens of interest, and initialize a text as before.\n",
    "tokens = tokens[110:390]\n",
    "text = nltk.Text(tokens)\n",
    "text.concordance('gene')\n",
    "#Concordense grabs the word and its contextual use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Search Engine Results\n",
    "\n",
    "Unfortunately, search engines have some significant shortcomings. First, the allowable range of search patterns is severely restricted. Unlike local corpora, where you write programs to search for arbitrarily complex patterns, search engines generally only allow you to search for individual words or strings of words, sometimes with wildcards. Second, search engines give inconsistent results, and can give widely different figures when used at different times or in different geographical regions. When content has been duplicated across multiple sites, search results may be boosted. Finally, the markup in the result returned by a search engine may change unpredictably, breaking any pattern-based method of locating particular content "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing RSS Feeds\n",
    "\n",
    "The blogosphere is an important source of text, in both formal and informal registers. With the help of a Python library called the Universal Feed Parser, we can access the content of a blog, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Language Log'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feedparser\n",
    "    \n",
    "llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\n",
    "llog['feed']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Odoriferous Mandarin term for &quot;copycat&quot;'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post = llog.entries[2]\n",
    "post.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>A gēnpìchóng 跟屁虫 (lit., \"follow-fart-bug / worm\") is somebody who t'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = post.content[0].value\n",
    "content[:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'gēnpìchóng',\n",
       " '跟屁虫',\n",
       " '(',\n",
       " 'lit.',\n",
       " ',',\n",
       " '``',\n",
       " 'follow-fart-bug',\n",
       " '/',\n",
       " 'worm',\n",
       " \"''\",\n",
       " ')',\n",
       " 'is',\n",
       " 'somebody',\n",
       " 'who',\n",
       " 'tags',\n",
       " 'along',\n",
       " 'after',\n",
       " 'someone',\n",
       " 'else',\n",
       " 'so',\n",
       " 'as',\n",
       " 'to',\n",
       " 'smell',\n",
       " 'his',\n",
       " 'farts',\n",
       " ',',\n",
       " 'i.e.',\n",
       " ',',\n",
       " 'someone',\n",
       " 'who',\n",
       " 'follows',\n",
       " 'another',\n",
       " 'person',\n",
       " 'all',\n",
       " 'the',\n",
       " 'time',\n",
       " ',',\n",
       " 'a',\n",
       " 'copycat',\n",
       " ',',\n",
       " 'a',\n",
       " 'shadow',\n",
       " ',',\n",
       " 'a',\n",
       " 'flatterer',\n",
       " ',',\n",
       " 'sycophant',\n",
       " ',',\n",
       " 'boot',\n",
       " '/',\n",
       " 'ass',\n",
       " 'licker',\n",
       " ',',\n",
       " 'kiss-ass',\n",
       " ',',\n",
       " 'yes',\n",
       " 'man',\n",
       " '.',\n",
       " 'And',\n",
       " 'here',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'cute',\n",
       " 'little',\n",
       " 'tutorial',\n",
       " 'about',\n",
       " 'how',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'gēnpìchóng',\n",
       " ':',\n",
       " 'Some',\n",
       " 'marketing',\n",
       " 'genius',\n",
       " 'came',\n",
       " 'up',\n",
       " 'with',\n",
       " 'this',\n",
       " 'bright',\n",
       " 'orange',\n",
       " 'gēnpìchóng',\n",
       " '跟屁虫',\n",
       " 'flotation',\n",
       " 'device',\n",
       " 'that',\n",
       " 'you',\n",
       " 'tie',\n",
       " 'to',\n",
       " 'your',\n",
       " 'behind',\n",
       " 'so',\n",
       " 'that',\n",
       " 'you',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'sink',\n",
       " 'beneath',\n",
       " 'the',\n",
       " 'water',\n",
       " 'if',\n",
       " 'you',\n",
       " 'ca',\n",
       " \"n't\",\n",
       " 'swim',\n",
       " '.',\n",
       " 'There',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'certain',\n",
       " 'resonance',\n",
       " ',',\n",
       " 'if',\n",
       " 'not',\n",
       " 'assonance',\n",
       " ',',\n",
       " 'or',\n",
       " 'at',\n",
       " 'least',\n",
       " 'reverberation',\n",
       " ',',\n",
       " 'between',\n",
       " 'gēnpìchóng',\n",
       " 'and',\n",
       " 'pāimǎpì',\n",
       " '拍马屁',\n",
       " '(',\n",
       " 'lit.',\n",
       " ',',\n",
       " '``',\n",
       " 'pat',\n",
       " 'horse',\n",
       " \"'s\",\n",
       " 'buttocks',\n",
       " \"''\",\n",
       " ',',\n",
       " 'i.e.',\n",
       " ',',\n",
       " '``',\n",
       " 'to',\n",
       " 'brownnose',\n",
       " ';',\n",
       " 'suck',\n",
       " 'up',\n",
       " 'to',\n",
       " ';',\n",
       " 'kiss',\n",
       " 'ass',\n",
       " \"''\",\n",
       " ')',\n",
       " '.',\n",
       " 'Our',\n",
       " 'malodorous',\n",
       " 'morpheme',\n",
       " ',',\n",
       " 'pì',\n",
       " '屁',\n",
       " '(',\n",
       " '``',\n",
       " 'fart',\n",
       " \"''\",\n",
       " ')',\n",
       " ',',\n",
       " 'is',\n",
       " 'very',\n",
       " 'productive',\n",
       " 'in',\n",
       " 'forming',\n",
       " 'a',\n",
       " 'wide',\n",
       " 'variety',\n",
       " 'of',\n",
       " 'colorful',\n",
       " 'lexemes',\n",
       " ':',\n",
       " '(',\n",
       " 'colloquial',\n",
       " ')',\n",
       " 'flatulence',\n",
       " ';',\n",
       " 'fart',\n",
       " '(',\n",
       " 'Classifier',\n",
       " ':',\n",
       " 'dǔ篤／笃',\n",
       " 'c',\n",
       " ')',\n",
       " '放屁',\n",
       " '―',\n",
       " 'fàngpì',\n",
       " '―',\n",
       " 'to',\n",
       " 'fart',\n",
       " ';',\n",
       " 'to',\n",
       " 'bullshit',\n",
       " '(',\n",
       " 'colloquial',\n",
       " ')',\n",
       " 'buttocks',\n",
       " ';',\n",
       " 'backside',\n",
       " '馬屁',\n",
       " '/',\n",
       " '马屁',\n",
       " '―',\n",
       " 'mǎpì',\n",
       " '―',\n",
       " 'horse',\n",
       " \"'s\",\n",
       " 'ass',\n",
       " ';',\n",
       " 'horse',\n",
       " \"'s\",\n",
       " 'backside',\n",
       " ';',\n",
       " 'flattery',\n",
       " '狗屁',\n",
       " '―',\n",
       " 'gǒupì',\n",
       " '―',\n",
       " 'dog',\n",
       " 'fart',\n",
       " ';',\n",
       " 'dog',\n",
       " \"'s\",\n",
       " 'backside',\n",
       " ';',\n",
       " 'nonsense',\n",
       " ';',\n",
       " 'worthless',\n",
       " 'thing',\n",
       " ';',\n",
       " 'bullshit',\n",
       " '屁眼',\n",
       " '―',\n",
       " 'pìyǎn',\n",
       " '―',\n",
       " 'anus',\n",
       " ';',\n",
       " 'asshole',\n",
       " '(',\n",
       " 'colloquial',\n",
       " ')',\n",
       " 'to',\n",
       " 'fart',\n",
       " ';',\n",
       " 'to',\n",
       " 'pass',\n",
       " 'gas',\n",
       " '屁者先知',\n",
       " '―',\n",
       " 'pìzhěxiānzhī',\n",
       " '―',\n",
       " '[',\n",
       " 'jocular',\n",
       " ']',\n",
       " 'The',\n",
       " 'farter',\n",
       " 'is',\n",
       " 'usually',\n",
       " 'the',\n",
       " 'first',\n",
       " 'to',\n",
       " 'realize',\n",
       " '(',\n",
       " 'and',\n",
       " 'complain',\n",
       " 'about',\n",
       " 'the',\n",
       " 'fart',\n",
       " ')',\n",
       " '.',\n",
       " '臭屁不響，響屁不臭',\n",
       " '―',\n",
       " 'chòu',\n",
       " 'pì',\n",
       " 'bù',\n",
       " 'xiǎng',\n",
       " ',',\n",
       " 'xiǎng',\n",
       " 'pì',\n",
       " 'bù',\n",
       " 'chòu',\n",
       " '(',\n",
       " '``',\n",
       " 'a',\n",
       " 'stinky',\n",
       " 'fart',\n",
       " 'is',\n",
       " 'silent',\n",
       " ',',\n",
       " 'a',\n",
       " 'loud',\n",
       " 'fart',\n",
       " 'is',\n",
       " 'not',\n",
       " 'stinky',\n",
       " \"''\",\n",
       " ')',\n",
       " ',',\n",
       " 'cf',\n",
       " '.',\n",
       " 'English',\n",
       " '``',\n",
       " 'silent',\n",
       " 'but',\n",
       " 'deadly',\n",
       " \"''\",\n",
       " '―',\n",
       " 'for',\n",
       " 'which',\n",
       " 'medical',\n",
       " 'science',\n",
       " 'tells',\n",
       " 'us',\n",
       " 'there',\n",
       " 'are',\n",
       " 'firm',\n",
       " 'physiological',\n",
       " 'reasons',\n",
       " '(',\n",
       " 'vulgar',\n",
       " ')',\n",
       " 'rubbish',\n",
       " ';',\n",
       " 'worthless',\n",
       " ';',\n",
       " 'useless',\n",
       " ';',\n",
       " 'insignificant',\n",
       " ';',\n",
       " 'trivial',\n",
       " '屁話',\n",
       " '/',\n",
       " '屁话',\n",
       " '―',\n",
       " 'pìhuà',\n",
       " '―',\n",
       " 'nonsense',\n",
       " '屁事',\n",
       " '―',\n",
       " 'pìshì',\n",
       " '―',\n",
       " 'trivial',\n",
       " 'thing',\n",
       " '(',\n",
       " 'vulgar',\n",
       " ',',\n",
       " 'chiefly',\n",
       " 'in',\n",
       " 'the',\n",
       " 'negative',\n",
       " ')',\n",
       " 'a',\n",
       " 'damn',\n",
       " 'thing',\n",
       " ';',\n",
       " '(',\n",
       " 'no',\n",
       " ')',\n",
       " 'thing',\n",
       " ';',\n",
       " 'damn',\n",
       " 'all',\n",
       " ';',\n",
       " 'jack',\n",
       " 'shit',\n",
       " ';',\n",
       " 'bugger',\n",
       " 'all',\n",
       " '他屁都不是。',\n",
       " '―',\n",
       " 'Tā',\n",
       " 'pì',\n",
       " 'dōu',\n",
       " 'bùshì',\n",
       " '.',\n",
       " '―',\n",
       " 'He',\n",
       " 'is',\n",
       " 'an',\n",
       " 'absolute',\n",
       " 'shit',\n",
       " '.',\n",
       " '你懂個屁！',\n",
       " '/',\n",
       " '你懂个屁！',\n",
       " '―',\n",
       " 'Nǐ',\n",
       " 'dǒnggepì',\n",
       " '!',\n",
       " '*',\n",
       " '―',\n",
       " 'You',\n",
       " 'know',\n",
       " 'jack',\n",
       " 'shit',\n",
       " '!',\n",
       " '關我屁事！',\n",
       " '/',\n",
       " '关我屁事！',\n",
       " '―',\n",
       " 'Guān',\n",
       " 'wǒ',\n",
       " 'pì',\n",
       " 'shì',\n",
       " '!',\n",
       " '―',\n",
       " 'Like',\n",
       " 'I',\n",
       " 'give',\n",
       " 'a',\n",
       " 'damn',\n",
       " '!',\n",
       " '*Uttered',\n",
       " 'with',\n",
       " 'the',\n",
       " 'requisite',\n",
       " 'hauteur',\n",
       " ',',\n",
       " 'contemptuous',\n",
       " 'intonation',\n",
       " ',',\n",
       " 'and',\n",
       " 'utter',\n",
       " 'disdain',\n",
       " ',',\n",
       " 'with',\n",
       " 'or',\n",
       " 'without',\n",
       " 'the',\n",
       " 'nǐ',\n",
       " '你',\n",
       " '(',\n",
       " '``',\n",
       " 'you',\n",
       " \"''\",\n",
       " ')',\n",
       " 'at',\n",
       " 'the',\n",
       " 'beginning',\n",
       " ',',\n",
       " 'but',\n",
       " 'with',\n",
       " 'heavy',\n",
       " 'emphasis',\n",
       " 'on',\n",
       " 'the',\n",
       " 'last',\n",
       " 'syllable',\n",
       " ',',\n",
       " 'like',\n",
       " 'a',\n",
       " 'saber',\n",
       " 'cut',\n",
       " ',',\n",
       " 'this',\n",
       " 'expression',\n",
       " 'can',\n",
       " 'be',\n",
       " 'absolutely',\n",
       " 'devastating',\n",
       " '(',\n",
       " 'vulgar',\n",
       " ',',\n",
       " 'negates',\n",
       " 'the',\n",
       " 'meaning',\n",
       " 'of',\n",
       " 'the',\n",
       " 'sentence',\n",
       " ')',\n",
       " 'like',\n",
       " 'hell/like',\n",
       " 'fuck',\n",
       " ';',\n",
       " 'my',\n",
       " 'ass',\n",
       " ';',\n",
       " 'your',\n",
       " 'ass',\n",
       " ';',\n",
       " 'damn',\n",
       " 'all/bugger',\n",
       " 'all/fuck',\n",
       " 'all',\n",
       " '你有屁用！',\n",
       " '―',\n",
       " 'Nǐ',\n",
       " 'yǒu',\n",
       " 'pì',\n",
       " 'yòng',\n",
       " '!',\n",
       " '―',\n",
       " 'You',\n",
       " 'are',\n",
       " 'of',\n",
       " 'fuck',\n",
       " 'all',\n",
       " 'use',\n",
       " '!',\n",
       " '吃屁飯啊！',\n",
       " '/',\n",
       " '吃屁饭啊！',\n",
       " '―',\n",
       " 'Chī',\n",
       " 'pì',\n",
       " 'fàn',\n",
       " 'a',\n",
       " '!',\n",
       " '―',\n",
       " 'Fuck',\n",
       " 'having',\n",
       " 'a',\n",
       " 'meal',\n",
       " '!',\n",
       " '/Have',\n",
       " 'fuck-all',\n",
       " 'meal',\n",
       " '!',\n",
       " '道個屁歉！',\n",
       " '/',\n",
       " '道个屁歉！',\n",
       " '―',\n",
       " 'Dào',\n",
       " 'ge',\n",
       " 'pì',\n",
       " 'qiàn',\n",
       " '!',\n",
       " '―',\n",
       " 'Apologize',\n",
       " 'my',\n",
       " 'ass',\n",
       " '!',\n",
       " 'Now',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'stick',\n",
       " 'out',\n",
       " 'my',\n",
       " 'long',\n",
       " 'nose',\n",
       " 'with',\n",
       " 'its',\n",
       " 'deep',\n",
       " 'nostrils',\n",
       " '(',\n",
       " 'not',\n",
       " 'my',\n",
       " 'neck',\n",
       " '!',\n",
       " ')',\n",
       " 'and',\n",
       " 'point',\n",
       " 'out',\n",
       " ':',\n",
       " 'From',\n",
       " 'Proto-Sino-Tibetan',\n",
       " '*pja-n/t/s',\n",
       " '(',\n",
       " '“',\n",
       " 'fart',\n",
       " ';',\n",
       " 'shit',\n",
       " '”',\n",
       " ')',\n",
       " '.',\n",
       " 'Cognate',\n",
       " 'with',\n",
       " 'Tibetan',\n",
       " 'ཕྱེན',\n",
       " '(',\n",
       " 'phyen',\n",
       " ')',\n",
       " ',',\n",
       " 'འཕྱེན',\n",
       " '(',\n",
       " \"'phyen\",\n",
       " ',',\n",
       " '“',\n",
       " 'flatulence',\n",
       " '”',\n",
       " ')',\n",
       " ',',\n",
       " 'Jingpho',\n",
       " 'hpyet',\n",
       " '(',\n",
       " '“',\n",
       " 'to',\n",
       " 'fart',\n",
       " '”',\n",
       " ')',\n",
       " '.',\n",
       " '(',\n",
       " 'source',\n",
       " ')',\n",
       " 'Old',\n",
       " 'English',\n",
       " 'feortan',\n",
       " ',',\n",
       " 'ultimately',\n",
       " 'from',\n",
       " 'PIE',\n",
       " '*perd-',\n",
       " '(',\n",
       " 'source',\n",
       " 'also',\n",
       " 'of',\n",
       " 'Old',\n",
       " 'High',\n",
       " 'German',\n",
       " 'ferzan',\n",
       " ',',\n",
       " 'Old',\n",
       " 'Norse',\n",
       " 'freta',\n",
       " ',',\n",
       " 'Danish',\n",
       " 'fjerte',\n",
       " ',',\n",
       " 'Sanskrit',\n",
       " 'pard',\n",
       " ',',\n",
       " 'Greek',\n",
       " 'perdein',\n",
       " ',',\n",
       " 'Lithuanian',\n",
       " 'perdžiu',\n",
       " ',',\n",
       " 'persti',\n",
       " ',',\n",
       " 'Russian',\n",
       " 'perdet',\n",
       " ')',\n",
       " ',',\n",
       " 'of',\n",
       " 'imitative',\n",
       " 'origin',\n",
       " '.',\n",
       " 'Related',\n",
       " ':',\n",
       " 'Farted',\n",
       " ';',\n",
       " 'farting',\n",
       " '.',\n",
       " 'As',\n",
       " 'a',\n",
       " 'noun',\n",
       " ',',\n",
       " 'from',\n",
       " 'late',\n",
       " '14c',\n",
       " '.',\n",
       " '(',\n",
       " 'source',\n",
       " ')',\n",
       " 'Alas',\n",
       " ',',\n",
       " 'there',\n",
       " \"'s\",\n",
       " 'no',\n",
       " 'medial',\n",
       " '``',\n",
       " 'r',\n",
       " \"''\",\n",
       " 'in',\n",
       " 'the',\n",
       " 'ST',\n",
       " 'words',\n",
       " '.',\n",
       " 'Middle',\n",
       " 'Sinitic',\n",
       " ':',\n",
       " '/pʰiɪH/',\n",
       " 'Old',\n",
       " 'Sinitic',\n",
       " '(',\n",
       " 'Zhengzhang',\n",
       " ')',\n",
       " ':',\n",
       " '/*pʰis/',\n",
       " 'Different',\n",
       " 'enunciation',\n",
       " '?',\n",
       " 'Selected',\n",
       " 'readings',\n",
       " \"''\",\n",
       " 'Bull',\n",
       " 'Fart',\n",
       " \"''\",\n",
       " '(',\n",
       " '9/30/09',\n",
       " ')',\n",
       " \"''\",\n",
       " 'Full',\n",
       " 'fart',\n",
       " \"''\",\n",
       " '(',\n",
       " '9/16/14',\n",
       " ')',\n",
       " \"''\",\n",
       " 'Artsy-fartsy',\n",
       " \"''\",\n",
       " '(',\n",
       " '8/2/18',\n",
       " ')',\n",
       " \"''\",\n",
       " 'No',\n",
       " 'more',\n",
       " 'plosive',\n",
       " 'consonants',\n",
       " ':',\n",
       " 'flay',\n",
       " 'your',\n",
       " 'fart',\n",
       " '!',\n",
       " \"''\",\n",
       " '(',\n",
       " '12/22/20',\n",
       " ')',\n",
       " \"''\",\n",
       " 'Vowel',\n",
       " 'movement',\n",
       " \"''\",\n",
       " '(',\n",
       " '8/13/15',\n",
       " ')',\n",
       " \"''\",\n",
       " \"'Take\",\n",
       " 'off',\n",
       " 'your',\n",
       " 'pants',\n",
       " 'and',\n",
       " 'fart',\n",
       " \"'\",\n",
       " \"''\",\n",
       " '(',\n",
       " '3/23/21',\n",
       " ')',\n",
       " '[',\n",
       " 'Thanks',\n",
       " 'to',\n",
       " 'Mark',\n",
       " 'Metcalf',\n",
       " ',',\n",
       " 'Julie',\n",
       " 'Lee',\n",
       " ',',\n",
       " 'and',\n",
       " 'Shuheng',\n",
       " 'Zhang',\n",
       " ']']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = BeautifulSoup(content, 'html.parser').get_text()\n",
    "word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Local Files \n",
    "\n",
    "In order to read a local file, we need to use Python's built-in open() function, followed by the read() method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test for the nltk chapter 3 module. \\nWhen you use f.read() it reads the entire file as one string. '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('document.txt')\n",
    "raw = f.read()\n",
    "raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that the file that you are trying to open is really in the right directory, use IDLE's Open command in the File menu; this will display a list of all the files in the directory where IDLE is running.\n",
    "\n",
    "An alternative is to examine the current directory from within Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '0. Preface.docx',\n",
       " '1. Language Processing and Python .ipynb',\n",
       " '2. Accessing Text Corpora and Lexical Resources .ipynb',\n",
       " '3. Processing Raw Text.ipynb',\n",
       " 'document.txt']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test for the nltk chapter 3 module.\n",
      "When you use f.read() it reads the entire file as one string.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JungleBook\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: 'U' mode is deprecated\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#We can also read a file one line at a time using a for loop:\n",
    "f = open('document.txt', 'rU')\n",
    "for line in f:\n",
    "    # Here we use the strip() method to remove the newline character at the end of the input line.\n",
    "    print(line.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK's corpus files can also be accessed using these methods.\n",
    "#We simply have to use nltk.data.find() to get the filename for any corpus item. \n",
    "# Then we can open and read it in the way we just demonstrated above:\n",
    "path = nltk.data.find('corpora/gutenberg/melville-moby_dick.txt')\n",
    "raw = open(path).read()\n",
    "# raw this a lot of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from PDF, MSWord and other Binary Formats\n",
    "\n",
    "ASCII text and HTML text are human readable formats. Text often comes in binary formats — like PDF and MSWord — that can only be opened using specialized software. \n",
    "\n",
    "Third-party libraries such as pypdf and pywin32 provide access to these formats. Extracting text from multi-column documents is particularly challenging. For once-off conversion of a few documents, it is simpler to open the document with a suitable application, then save it as text to your local drive, and access it as described below. \n",
    "\n",
    "If the document is already on the web, you can enter its URL in Google's search box. The search result often includes a link to an HTML version of the document, which you can save as text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capturing User Input\n",
    "\n",
    "To prompt the user to type a line of input, call the Python function input(). After saving the input to a variable, we can manipulate it just as we have done for other strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter some text: This is a demo\n",
      "You typed 4 words.\n"
     ]
    }
   ],
   "source": [
    "s = input(\"Enter some text: \")\n",
    "print(\"You typed\", len(word_tokenize(s)), \"words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2   Strings: Text Processing at the Lowest Level\n",
    "\n",
    "Sometimes strings go over several lines. Python provides us with various ways of entering them. In the next example, a sequence of two strings is joined into a single string. We need to use backslash or parentheses so that the interpreter knows that the statement is not complete after the first line.\n",
    "\n",
    "Unfortunately the above methods do not give us a newline between the two lines of the sonnet. Instead, we can use a triple-quoted string as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall I compare thee to a Summer's day?\n",
      "Thou are more lovely and more temperate:\n"
     ]
    }
   ],
   "source": [
    "couplet = \"\"\"Shall I compare thee to a Summer's day?\n",
    "Thou are more lovely and more temperate:\"\"\"\n",
    "print(couplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Montey Python Holy Grail\n"
     ]
    }
   ],
   "source": [
    "# The print statement allows us to display more than one item on a line in various ways, as shown below:\n",
    "monty = \"Montey Python\"\n",
    "grail = 'Holy Grail'\n",
    "print(monty, grail)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Montey Python and the Holy Grail\n"
     ]
    }
   ],
   "source": [
    "print(monty, \"and the\", grail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e', 117092), ('t', 87996), ('a', 77916), ('o', 69326), ('n', 65617)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can count individual characters. \n",
    "#We should ignore the case distinction by normalizing everything to lowercase, \n",
    "#and filter out non-alphabetic characters:\n",
    "from nltk.corpus import gutenberg\n",
    "raw = gutenberg.raw('melville-moby_dick.txt')\n",
    "fdist = nltk.FreqDist(ch.lower() for ch in raw if ch.isalpha())\n",
    "fdist.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3   Text Processing with Unicode\n",
    "\n",
    "Our programs will often need to deal with different languages, and different character sets.If you live in the English-speaking world you probably use ASCII, possibly without realizing it.\n",
    "\n",
    "## What is Unicode?\n",
    "Unicode supports over a million characters. Each character is assigned a number, called a **code point**. In Python, code points are written in the form \\uXXXX, where XXXX is the number in 4-digit hexadecimal form.\n",
    "\n",
    "When Unicode characters are stored in files or displayed on a terminal, they must be encoded as a stream of bytes. Some encodings (such as ASCII and Latin-2) use a single byte per code point, so they can only support a small subset of Unicode, enough for a single language. Other encodings (such as UTF-8) use multiple bytes and can represent the full range of Unicode characters.\n",
    "\n",
    "Text in files will be in a particular encoding, so we need some mechanism for translating it into Unicode — translation into Unicode is called **decoding**. Conversely, to write out Unicode to a file or a terminal, we first need to translate it into a suitable encoding — this translation out of Unicode is called **encoding**,\n",
    "\n",
    "\n",
    "From a Unicode perspective, characters are abstract entities which can be realized as one or more **glyphs**. Only glyphs can appear on a screen or be printed on paper. A font is a mapping from characters to glyphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Extracting encoded text from files\n",
    "\n",
    "et's assume that we have a small text file, and that we know how it is encoded. For example, polish-lat2.txt, as the name suggests, is a snippet of Polish text (from the Polish Wikipedia; see http://pl.wikipedia.org/wiki/Biblioteka_Pruska). This file is encoded as Latin-2, also known as ISO-8859-2. The function nltk.data.find() locates the file for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruska Biblioteka Państwowa. Jej dawne zbiory znane pod nazwą\n",
      "\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez\n",
      "Niemców pod koniec II wojny światowej na Dolny Śląsk, zostały\n",
      "odnalezione po 1945 r. na terytorium Polski. Trafiły do Biblioteki\n",
      "Jagiellońskiej w Krakowie, obejmują ponad 500 tys. zabytkowych\n",
      "archiwaliów, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.\n"
     ]
    }
   ],
   "source": [
    "# The Python open() function can read encoded data into Unicode strings, and write out Unicode strings in encoded form.\n",
    "# It takes a parameter to specify the encoding of the file being read or written. \n",
    "# So let's open our Polish file with the encoding 'latin2' and inspect the contents of the file:\n",
    "f = open(path, encoding='latin2')\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    print(line)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Pruska Biblioteka Pa\\\\u0144stwowa. Jej dawne zbiory znane pod nazw\\\\u0105'\n",
      "b'\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez'\n",
      "b'Niemc\\\\xf3w pod koniec II wojny \\\\u015bwiatowej na Dolny \\\\u015al\\\\u0105sk, zosta\\\\u0142y'\n",
      "b'odnalezione po 1945 r. na terytorium Polski. Trafi\\\\u0142y do Biblioteki'\n",
      "b'Jagiello\\\\u0144skiej w Krakowie, obejmuj\\\\u0105 ponad 500 tys. zabytkowych'\n",
      "b'archiwali\\\\xf3w, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.'\n"
     ]
    }
   ],
   "source": [
    "# If this does not display correctly on your terminal,\n",
    "#or if we want to see the underlying numerical values (or \"codepoints\") of the characters, \n",
    "#then we can convert all non-ASCII characters into their two-digit and four-digit representations\n",
    "f = open(path, encoding='latin2')\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    print(line.encode('unicode_escape'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python 3, source code is encoded using UTF-8 by default, and you can include Unicode characters in strings if you are using IDLE or another program editor that supports Unicode. \n",
    "\n",
    "Arbitrary Unicode characters can be included using the \\uXXXX escape sequence. We find the integer ordinal of a character using ord(). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('ń')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ń'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The hexadecimal 4 digit notation for 324 is 0144 \n",
    "#watch \n",
    "nacute = '\\u0144'\n",
    "nacute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Niemc\\\\xf3w pod koniec II wojny \\\\u015bwiatowej na Dolny \\\\u015al\\\\u0105sk, zosta\\\\u0142y\\\\n'\n"
     ]
    }
   ],
   "source": [
    "# The module unicodedata lets us inspect the properties of Unicode characters.\n",
    "# we select all characters in the third line of our Polish text outside\n",
    "# the ASCII range and print their UTF-8 byte sequence\n",
    "import unicodedata\n",
    "lines = open(path, encoding='latin2').readlines()\n",
    "line = lines[2]\n",
    "print(line.encode('unicode_escape'))\n",
    "# Alternatively, you may need to replace the encoding 'utf8' in the example by 'latin2', \n",
    "#again depending on the details of your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xc3\\xb3' U+00f3 LATIN SMALL LETTER O WITH ACUTE\n",
      "b'\\xc5\\x9b' U+015b LATIN SMALL LETTER S WITH ACUTE\n",
      "b'\\xc5\\x9a' U+015a LATIN CAPITAL LETTER S WITH ACUTE\n",
      "b'\\xc4\\x85' U+0105 LATIN SMALL LETTER A WITH OGONEK\n",
      "b'\\xc5\\x82' U+0142 LATIN SMALL LETTER L WITH STROKE\n"
     ]
    }
   ],
   "source": [
    "for c in line:\n",
    "    if ord(c) > 127:\n",
    "        print('{} U+{:04x} {}'.format(c.encode('utf8'), ord(c), unicodedata.name(c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4   Regular Expressions for Detecting Word Patterns\n",
    "\n",
    "To use regular expressions in Python we need to import the re library using: import re. We also need a list of words to search; we'll use the Words Corpus again (4). We will preprocess it to remove any proper names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abaissed',\n",
       " 'abandoned',\n",
       " 'abased',\n",
       " 'abashed',\n",
       " 'abatised',\n",
       " 'abed',\n",
       " 'aborted',\n",
       " 'abridged',\n",
       " 'abscessed',\n",
       " 'absconded',\n",
       " 'absorbed',\n",
       " 'abstracted',\n",
       " 'abstricted',\n",
       " 'accelerated',\n",
       " 'accepted',\n",
       " 'accidented',\n",
       " 'accoladed',\n",
       " 'accolated',\n",
       " 'accomplished',\n",
       " 'accosted',\n",
       " 'accredited',\n",
       " 'accursed',\n",
       " 'accused',\n",
       " 'accustomed',\n",
       " 'acetated',\n",
       " 'acheweed',\n",
       " 'aciculated',\n",
       " 'aciliated',\n",
       " 'acknowledged',\n",
       " 'acorned',\n",
       " 'acquainted',\n",
       " 'acquired',\n",
       " 'acquisited',\n",
       " 'acred',\n",
       " 'aculeated',\n",
       " 'addebted',\n",
       " 'added',\n",
       " 'addicted',\n",
       " 'addlebrained',\n",
       " 'addleheaded',\n",
       " 'addlepated',\n",
       " 'addorsed',\n",
       " 'adempted',\n",
       " 'adfected',\n",
       " 'adjoined',\n",
       " 'admired',\n",
       " 'admitted',\n",
       " 'adnexed',\n",
       " 'adopted',\n",
       " 'adossed',\n",
       " 'adreamed',\n",
       " 'adscripted',\n",
       " 'aduncated',\n",
       " 'advanced',\n",
       " 'advised',\n",
       " 'aeried',\n",
       " 'aethered',\n",
       " 'afeared',\n",
       " 'affected',\n",
       " 'affectioned',\n",
       " 'affined',\n",
       " 'afflicted',\n",
       " 'affricated',\n",
       " 'affrighted',\n",
       " 'affronted',\n",
       " 'aforenamed',\n",
       " 'afterfeed',\n",
       " 'aftershafted',\n",
       " 'afterthoughted',\n",
       " 'afterwitted',\n",
       " 'agazed',\n",
       " 'aged',\n",
       " 'agglomerated',\n",
       " 'aggrieved',\n",
       " 'agminated',\n",
       " 'agnamed',\n",
       " 'agonied',\n",
       " 'agreed',\n",
       " 'agueweed',\n",
       " 'ahungered',\n",
       " 'aiguilletted',\n",
       " 'ailweed',\n",
       " 'airbrained',\n",
       " 'airified',\n",
       " 'aiseweed',\n",
       " 'aisled',\n",
       " 'alarmed',\n",
       " 'alated',\n",
       " 'alimonied',\n",
       " 'aliped',\n",
       " 'alleyed',\n",
       " 'allied',\n",
       " 'alligatored',\n",
       " 'allseed',\n",
       " 'almsdeed',\n",
       " 'aloed',\n",
       " 'altared',\n",
       " 'alveolated',\n",
       " 'amazed',\n",
       " 'ameed',\n",
       " 'amiced',\n",
       " 'amphitheatered',\n",
       " 'ampullated',\n",
       " 'amused',\n",
       " 'anchored',\n",
       " 'angled',\n",
       " 'anguiped',\n",
       " 'anguished',\n",
       " 'angulated',\n",
       " 'angulinerved',\n",
       " 'anhungered',\n",
       " 'animated',\n",
       " 'aniseed',\n",
       " 'annodated',\n",
       " 'annulated',\n",
       " 'anomaliped',\n",
       " 'anserated',\n",
       " 'anteflected',\n",
       " 'anteflexed',\n",
       " 'antimoniated',\n",
       " 'antimoniureted',\n",
       " 'antimoniuretted',\n",
       " 'antiquated',\n",
       " 'antired',\n",
       " 'antiweed',\n",
       " 'antlered',\n",
       " 'apertured',\n",
       " 'apexed',\n",
       " 'apicifixed',\n",
       " 'apiculated',\n",
       " 'apocopated',\n",
       " 'apostrophied',\n",
       " 'appearanced',\n",
       " 'appellatived',\n",
       " 'appendaged',\n",
       " 'appendiculated',\n",
       " 'applied',\n",
       " 'appressed',\n",
       " 'aralkylated',\n",
       " 'arbored',\n",
       " 'arched',\n",
       " 'architraved',\n",
       " 'arcked',\n",
       " 'arcuated',\n",
       " 'ared',\n",
       " 'areolated',\n",
       " 'ariled',\n",
       " 'arillated',\n",
       " 'armchaired',\n",
       " 'armed',\n",
       " 'armied',\n",
       " 'armillated',\n",
       " 'armored',\n",
       " 'armoried',\n",
       " 'arpeggiated',\n",
       " 'arpeggioed',\n",
       " 'arrased',\n",
       " 'arrowed',\n",
       " 'arrowheaded',\n",
       " 'arrowweed',\n",
       " 'arseneted',\n",
       " 'arsenetted',\n",
       " 'arseniureted',\n",
       " 'articled',\n",
       " 'articulated',\n",
       " 'ashamed',\n",
       " 'ashlared',\n",
       " 'ashweed',\n",
       " 'aspersed',\n",
       " 'asphyxied',\n",
       " 'assented',\n",
       " 'assessed',\n",
       " 'assigned',\n",
       " 'assistanted',\n",
       " 'associated',\n",
       " 'assonanced',\n",
       " 'assorted',\n",
       " 'assumed',\n",
       " 'assured',\n",
       " 'asteriated',\n",
       " 'astonied',\n",
       " 'aswooned',\n",
       " 'atrophiated',\n",
       " 'atrophied',\n",
       " 'attached',\n",
       " 'attired',\n",
       " 'attrited',\n",
       " 'augmented',\n",
       " 'aurated',\n",
       " 'auricled',\n",
       " 'auriculated',\n",
       " 'authorized',\n",
       " 'autoinhibited',\n",
       " 'autosensitized',\n",
       " 'autosled',\n",
       " 'averted',\n",
       " 'avowed',\n",
       " 'awearied',\n",
       " 'awned',\n",
       " 'awninged',\n",
       " 'axed',\n",
       " 'axhammered',\n",
       " 'axised',\n",
       " 'axled',\n",
       " 'axseed',\n",
       " 'axweed',\n",
       " 'azoted',\n",
       " 'azured',\n",
       " 'babied',\n",
       " 'babished',\n",
       " 'babyfied',\n",
       " 'baccated',\n",
       " 'backboned',\n",
       " 'backed',\n",
       " 'backhanded',\n",
       " 'backwatered',\n",
       " 'baconweed',\n",
       " 'badgerweed',\n",
       " 'bagged',\n",
       " 'bagwigged',\n",
       " 'baked',\n",
       " 'balanced',\n",
       " 'balconied',\n",
       " 'baldachined',\n",
       " 'baldricked',\n",
       " 'balled',\n",
       " 'ballweed',\n",
       " 'balsamweed',\n",
       " 'balustered',\n",
       " 'balustraded',\n",
       " 'bandannaed',\n",
       " 'banded',\n",
       " 'bandoleered',\n",
       " 'bangled',\n",
       " 'banked',\n",
       " 'bankweed',\n",
       " 'bannered',\n",
       " 'barbated',\n",
       " 'barbed',\n",
       " 'barebacked',\n",
       " 'bareboned',\n",
       " 'barefaced',\n",
       " 'barefooted',\n",
       " 'barehanded',\n",
       " 'bareheaded',\n",
       " 'barelegged',\n",
       " 'barenecked',\n",
       " 'barmybrained',\n",
       " 'barred',\n",
       " 'barreled',\n",
       " 'bartizaned',\n",
       " 'basebred',\n",
       " 'based',\n",
       " 'basehearted',\n",
       " 'basifixed',\n",
       " 'basilweed',\n",
       " 'basined',\n",
       " 'basinerved',\n",
       " 'basqued',\n",
       " 'bastioned',\n",
       " 'bated',\n",
       " 'bathroomed',\n",
       " 'battered',\n",
       " 'batteried',\n",
       " 'battled',\n",
       " 'battlemented',\n",
       " 'bayed',\n",
       " 'bayoneted',\n",
       " 'beached',\n",
       " 'beaded',\n",
       " 'beaked',\n",
       " 'bealtared',\n",
       " 'beamed',\n",
       " 'beanweed',\n",
       " 'beaproned',\n",
       " 'bearded',\n",
       " 'beautied',\n",
       " 'beavered',\n",
       " 'beballed',\n",
       " 'bebannered',\n",
       " 'bebed',\n",
       " 'bebelted',\n",
       " 'bebled',\n",
       " 'bebothered',\n",
       " 'bebouldered',\n",
       " 'bebuttoned',\n",
       " 'becassocked',\n",
       " 'bechained',\n",
       " 'bechignoned',\n",
       " 'becircled',\n",
       " 'becoiffed',\n",
       " 'becombed',\n",
       " 'becousined',\n",
       " 'becrinolined',\n",
       " 'becuffed',\n",
       " 'becurtained',\n",
       " 'becushioned',\n",
       " 'bed',\n",
       " 'bedaggered',\n",
       " 'bedangled',\n",
       " 'bedded',\n",
       " 'bediademed',\n",
       " 'bediamonded',\n",
       " 'beedged',\n",
       " 'beefheaded',\n",
       " 'beeheaded',\n",
       " 'beeswinged',\n",
       " 'beetled',\n",
       " 'beetleheaded',\n",
       " 'beetleweed',\n",
       " 'beeweed',\n",
       " 'befamilied',\n",
       " 'befanned',\n",
       " 'befathered',\n",
       " 'beferned',\n",
       " 'befetished',\n",
       " 'befezzed',\n",
       " 'befilleted',\n",
       " 'befilmed',\n",
       " 'beforested',\n",
       " 'befountained',\n",
       " 'befrocked',\n",
       " 'befrogged',\n",
       " 'befurbelowed',\n",
       " 'befurred',\n",
       " 'begabled',\n",
       " 'begarlanded',\n",
       " 'begartered',\n",
       " 'beggarweed',\n",
       " 'beglobed',\n",
       " 'begoggled',\n",
       " 'begowned',\n",
       " 'behatted',\n",
       " 'behaviored',\n",
       " 'beheadlined',\n",
       " 'behooped',\n",
       " 'beinked',\n",
       " 'bekilted',\n",
       " 'beknived',\n",
       " 'beknotted',\n",
       " 'belaced',\n",
       " 'belated',\n",
       " 'belatticed',\n",
       " 'belavendered',\n",
       " 'beledgered',\n",
       " 'belfried',\n",
       " 'beliked',\n",
       " 'belimousined',\n",
       " 'belled',\n",
       " 'bellied',\n",
       " 'bellmouthed',\n",
       " 'bellweed',\n",
       " 'beloved',\n",
       " 'belozenged',\n",
       " 'belted',\n",
       " 'bemazed',\n",
       " 'bemedaled',\n",
       " 'bemedalled',\n",
       " 'bemitered',\n",
       " 'bemitred',\n",
       " 'bemused',\n",
       " 'bemuslined',\n",
       " 'bended',\n",
       " 'beneaped',\n",
       " 'beneficed',\n",
       " 'beneighbored',\n",
       " 'benempted',\n",
       " 'benighted',\n",
       " 'bennetweed',\n",
       " 'benumbed',\n",
       " 'benweed',\n",
       " 'benzoated',\n",
       " 'benzoinated',\n",
       " 'bepastured',\n",
       " 'bepatched',\n",
       " 'beperiwigged',\n",
       " 'bepewed',\n",
       " 'bepillared',\n",
       " 'bepistoled',\n",
       " 'beplaided',\n",
       " 'beplumed',\n",
       " 'beribanded',\n",
       " 'beribboned',\n",
       " 'beringed',\n",
       " 'beringleted',\n",
       " 'berobed',\n",
       " 'berouged',\n",
       " 'berried',\n",
       " 'berthed',\n",
       " 'beruffed',\n",
       " 'beruffled',\n",
       " 'beshawled',\n",
       " 'besieged',\n",
       " 'beslushed',\n",
       " 'besotted',\n",
       " 'bespecked',\n",
       " 'bespectacled',\n",
       " 'besped',\n",
       " 'bespeed',\n",
       " 'bespelled',\n",
       " 'bespurred',\n",
       " 'bestatued',\n",
       " 'bestayed',\n",
       " 'bestrapped',\n",
       " 'bestubbled',\n",
       " 'besweatered',\n",
       " 'betattered',\n",
       " 'betaxed',\n",
       " 'betowered',\n",
       " 'betrothed',\n",
       " 'betrousered',\n",
       " 'betted',\n",
       " 'betuckered',\n",
       " 'beturbaned',\n",
       " 'betusked',\n",
       " 'betutored',\n",
       " 'betwattled',\n",
       " 'beuniformed',\n",
       " 'beveled',\n",
       " 'bevelled',\n",
       " 'bevesseled',\n",
       " 'bevesselled',\n",
       " 'bevined',\n",
       " 'bevoiled',\n",
       " 'bewaitered',\n",
       " 'bewhiskered',\n",
       " 'bewigged',\n",
       " 'bewildered',\n",
       " 'bewinged',\n",
       " 'bewired',\n",
       " 'bewrathed',\n",
       " 'biangulated',\n",
       " 'biarcuated',\n",
       " 'biarticulated',\n",
       " 'bicarbureted',\n",
       " 'biciliated',\n",
       " 'bicolored',\n",
       " 'bicorned',\n",
       " 'bidented',\n",
       " 'bifanged',\n",
       " 'bifidated',\n",
       " 'biflected',\n",
       " 'biforked',\n",
       " 'biformed',\n",
       " 'bifronted',\n",
       " 'bifurcated',\n",
       " 'bigeminated',\n",
       " 'bighearted',\n",
       " 'bigmouthed',\n",
       " 'bigoted',\n",
       " 'bigwigged',\n",
       " 'bilamellated',\n",
       " 'bilaminated',\n",
       " 'billed',\n",
       " 'bilobated',\n",
       " 'bilobed',\n",
       " 'bilsted',\n",
       " 'bimaculated',\n",
       " 'bimotored',\n",
       " 'bindweed',\n",
       " 'bineweed',\n",
       " 'binominated',\n",
       " 'binucleated',\n",
       " 'biparted',\n",
       " 'bipectinated',\n",
       " 'biped',\n",
       " 'bipennated',\n",
       " 'bipinnated',\n",
       " 'bipinnatiparted',\n",
       " 'bipinnatisected',\n",
       " 'biradiated',\n",
       " 'birdmouthed',\n",
       " 'birdseed',\n",
       " 'birdweed',\n",
       " 'birostrated',\n",
       " 'birthbed',\n",
       " 'bisexed',\n",
       " 'bishopweed',\n",
       " 'bistered',\n",
       " 'bistipuled',\n",
       " 'bisubstituted',\n",
       " 'bitted',\n",
       " 'bitterhearted',\n",
       " 'bitterweed',\n",
       " 'bituberculated',\n",
       " 'bitumed',\n",
       " 'bivalved',\n",
       " 'bivaulted',\n",
       " 'bivocalized',\n",
       " 'blackhearted',\n",
       " 'blackseed',\n",
       " 'blackshirted',\n",
       " 'bladderseed',\n",
       " 'bladderweed',\n",
       " 'bladed',\n",
       " 'blakeberyed',\n",
       " 'blamed',\n",
       " 'blanked',\n",
       " 'blanketed',\n",
       " 'blanketweed',\n",
       " 'blasted',\n",
       " 'bleached',\n",
       " 'bleared',\n",
       " 'bleed',\n",
       " 'blended',\n",
       " 'blessed',\n",
       " 'blighted',\n",
       " 'blinded',\n",
       " 'blindfolded',\n",
       " 'blindweed',\n",
       " 'blinked',\n",
       " 'blinkered',\n",
       " 'blistered',\n",
       " 'blisterweed',\n",
       " 'blithehearted',\n",
       " 'bloated',\n",
       " 'blobbed',\n",
       " 'blocked',\n",
       " 'blockheaded',\n",
       " 'blooded',\n",
       " 'bloodied',\n",
       " 'bloodshed',\n",
       " 'bloodstained',\n",
       " 'bloodweed',\n",
       " 'blossomed',\n",
       " 'blotched',\n",
       " 'bloused',\n",
       " 'blowzed',\n",
       " 'bludgeoned',\n",
       " 'bluebelled',\n",
       " 'bluehearted',\n",
       " 'blueweed',\n",
       " 'blunderheaded',\n",
       " 'blunthearted',\n",
       " 'blurred',\n",
       " 'bobbed',\n",
       " 'bobsled',\n",
       " 'bobtailed',\n",
       " 'bodiced',\n",
       " 'bodied',\n",
       " 'boiled',\n",
       " 'boldhearted',\n",
       " 'bolectioned',\n",
       " 'boled',\n",
       " 'boleweed',\n",
       " 'bolled',\n",
       " 'bombed',\n",
       " 'bonded',\n",
       " 'boned',\n",
       " 'boneheaded',\n",
       " 'bonneted',\n",
       " 'booked',\n",
       " 'booted',\n",
       " 'bootied',\n",
       " 'boozed',\n",
       " 'bordered',\n",
       " 'bordured',\n",
       " 'bosomed',\n",
       " 'bossed',\n",
       " 'bosselated',\n",
       " 'botched',\n",
       " 'botherheaded',\n",
       " 'bothsided',\n",
       " 'bottled',\n",
       " 'bottomed',\n",
       " 'boughed',\n",
       " 'bounded',\n",
       " 'bountied',\n",
       " 'bowed',\n",
       " 'boweled',\n",
       " 'bowlegged',\n",
       " 'bowstringed',\n",
       " 'braced',\n",
       " 'braceleted',\n",
       " 'brackened',\n",
       " 'bracted',\n",
       " 'braided',\n",
       " 'brambled',\n",
       " 'branched',\n",
       " 'branded',\n",
       " 'brandied',\n",
       " 'brangled',\n",
       " 'bravehearted',\n",
       " 'brawned',\n",
       " 'brazenfaced',\n",
       " 'breasted',\n",
       " 'breastweed',\n",
       " 'breathed',\n",
       " 'brecciated',\n",
       " 'bred',\n",
       " 'breeched',\n",
       " 'breed',\n",
       " 'breviped',\n",
       " 'bridebed',\n",
       " 'brideweed',\n",
       " 'bridged',\n",
       " 'bridled',\n",
       " 'briered',\n",
       " 'brimmed',\n",
       " 'bristled',\n",
       " 'broadhearted',\n",
       " 'brocaded',\n",
       " 'brocked',\n",
       " 'brokenhearted',\n",
       " 'bromoiodized',\n",
       " 'bronzed',\n",
       " 'brooked',\n",
       " 'brookweed',\n",
       " 'broomweed',\n",
       " 'broozled',\n",
       " 'browed',\n",
       " 'brownweed',\n",
       " 'bruckled',\n",
       " 'brushed',\n",
       " 'buboed',\n",
       " 'bucked',\n",
       " 'buckled',\n",
       " 'buckskinned',\n",
       " 'buffed',\n",
       " 'bugled',\n",
       " 'bugleweed',\n",
       " 'bugseed',\n",
       " 'bugweed',\n",
       " 'bulbed',\n",
       " 'bulked',\n",
       " 'bulkheaded',\n",
       " 'bullated',\n",
       " 'bulldogged',\n",
       " 'bulleted',\n",
       " 'bulletheaded',\n",
       " 'bullheaded',\n",
       " 'bullweed',\n",
       " 'bummed',\n",
       " 'bundlerooted',\n",
       " 'bundweed',\n",
       " 'bunted',\n",
       " 'buried',\n",
       " 'burled',\n",
       " 'burned',\n",
       " 'burnoosed',\n",
       " 'burntweed',\n",
       " 'burred',\n",
       " 'burroweed',\n",
       " 'burseed',\n",
       " 'burweed',\n",
       " 'bushed',\n",
       " 'busied',\n",
       " 'busked',\n",
       " 'buskined',\n",
       " 'busted',\n",
       " 'bustled',\n",
       " 'busybodied',\n",
       " 'buttered',\n",
       " 'butterfingered',\n",
       " 'butterweed',\n",
       " 'butteryfingered',\n",
       " 'buttocked',\n",
       " 'buttoned',\n",
       " 'buttonweed',\n",
       " 'cabled',\n",
       " 'caboshed',\n",
       " 'caddiced',\n",
       " 'caddised',\n",
       " 'cadenced',\n",
       " 'cadweed',\n",
       " 'caftaned',\n",
       " 'caged',\n",
       " 'cairned',\n",
       " 'caissoned',\n",
       " 'calced',\n",
       " 'calcified',\n",
       " 'calcined',\n",
       " 'calculated',\n",
       " 'calibered',\n",
       " 'calicoed',\n",
       " 'caligated',\n",
       " 'calpacked',\n",
       " 'calved',\n",
       " 'calycled',\n",
       " 'calyculated',\n",
       " 'camailed',\n",
       " 'camerated',\n",
       " 'cammed',\n",
       " 'campanulated',\n",
       " 'campshed',\n",
       " 'camused',\n",
       " 'canaliculated',\n",
       " 'cancellated',\n",
       " 'cancered',\n",
       " 'cancerweed',\n",
       " 'candied',\n",
       " 'candlelighted',\n",
       " 'candlesticked',\n",
       " 'candyweed',\n",
       " 'canioned',\n",
       " 'cankered',\n",
       " 'cankerweed',\n",
       " 'canned',\n",
       " 'cannelated',\n",
       " 'cannelured',\n",
       " 'cannoned',\n",
       " 'cannulated',\n",
       " 'canted',\n",
       " 'cantilevered',\n",
       " 'cantoned',\n",
       " 'cantred',\n",
       " 'caped',\n",
       " 'capernoited',\n",
       " 'capeweed',\n",
       " 'capitaled',\n",
       " 'capitated',\n",
       " 'capped',\n",
       " 'capriped',\n",
       " 'capsulated',\n",
       " 'capuched',\n",
       " 'carapaced',\n",
       " 'carbolated',\n",
       " 'carboyed',\n",
       " 'carbuncled',\n",
       " 'carcaneted',\n",
       " 'carded',\n",
       " 'carinated',\n",
       " 'carkled',\n",
       " 'carnaged',\n",
       " 'carnationed',\n",
       " 'carpetweed',\n",
       " 'carried',\n",
       " 'carrotweed',\n",
       " 'carucated',\n",
       " 'carunculated',\n",
       " 'cased',\n",
       " 'casemated',\n",
       " 'casemented',\n",
       " 'caseweed',\n",
       " 'casqued',\n",
       " 'castellated',\n",
       " 'castled',\n",
       " 'castorized',\n",
       " 'catamited',\n",
       " 'cataracted',\n",
       " 'catarrhed',\n",
       " 'catchweed',\n",
       " 'catenated',\n",
       " 'caterpillared',\n",
       " 'catfaced',\n",
       " 'catfooted',\n",
       " 'cathedraled',\n",
       " 'caudated',\n",
       " 'caverned',\n",
       " 'cavitied',\n",
       " 'cayenned',\n",
       " 'cedared',\n",
       " 'ceilinged',\n",
       " 'celebrated',\n",
       " 'cellated',\n",
       " 'celled',\n",
       " 'cellulated',\n",
       " 'celluloided',\n",
       " 'centered',\n",
       " 'centriffed',\n",
       " 'centuried',\n",
       " 'cerated',\n",
       " 'cered',\n",
       " 'certified',\n",
       " 'chafeweed',\n",
       " 'chaffseed',\n",
       " 'chaffweed',\n",
       " 'chafted',\n",
       " 'chained',\n",
       " 'chaliced',\n",
       " 'chambered',\n",
       " 'chamberleted',\n",
       " 'chamberletted',\n",
       " 'chanceled',\n",
       " 'channeled',\n",
       " 'channelled',\n",
       " 'chaped',\n",
       " 'chapleted',\n",
       " 'chapournetted',\n",
       " 'chapped',\n",
       " 'charioted',\n",
       " 'charqued',\n",
       " 'chartered',\n",
       " 'chasmed',\n",
       " 'chasteweed',\n",
       " 'chasubled',\n",
       " 'checked',\n",
       " 'checkered',\n",
       " 'checkrowed',\n",
       " 'cheered',\n",
       " 'cheliped',\n",
       " 'cherried',\n",
       " 'chickenbreasted',\n",
       " 'chickenhearted',\n",
       " 'chickenweed',\n",
       " 'chickweed',\n",
       " 'chicqued',\n",
       " 'chiggerweed',\n",
       " 'chignoned',\n",
       " 'childbed',\n",
       " 'childed',\n",
       " 'chilled',\n",
       " 'chined',\n",
       " 'chinned',\n",
       " 'chipped',\n",
       " 'chiseled',\n",
       " 'chitinized',\n",
       " 'chokered',\n",
       " 'chokeweed',\n",
       " 'cholterheaded',\n",
       " 'chopped',\n",
       " 'choppered',\n",
       " 'chorded',\n",
       " 'chowderheaded',\n",
       " 'christened',\n",
       " 'chubbed',\n",
       " 'chuckleheaded',\n",
       " 'churchified',\n",
       " 'churled',\n",
       " 'ciliated',\n",
       " 'cingulated',\n",
       " 'cinnamoned',\n",
       " 'cinquefoiled',\n",
       " 'circled',\n",
       " 'circumscribed',\n",
       " 'circumstanced',\n",
       " 'cirrated',\n",
       " 'cirrhosed',\n",
       " 'cirriped',\n",
       " 'cisted',\n",
       " 'citied',\n",
       " 'citified',\n",
       " 'citrated',\n",
       " 'civilized',\n",
       " 'clammed',\n",
       " 'clammyweed',\n",
       " 'clanned',\n",
       " 'clapped',\n",
       " 'classed',\n",
       " 'classified',\n",
       " 'clavated',\n",
       " 'clavellated',\n",
       " 'clawed',\n",
       " 'claybrained',\n",
       " 'clayweed',\n",
       " 'cleaded',\n",
       " 'cleanhanded',\n",
       " 'cleanhearted',\n",
       " 'clearheaded',\n",
       " 'clearhearted',\n",
       " 'clearweed',\n",
       " 'cled',\n",
       " 'cleeked',\n",
       " 'clefted',\n",
       " 'clerestoried',\n",
       " 'cliented',\n",
       " 'cliffed',\n",
       " 'cliffweed',\n",
       " 'clipped',\n",
       " 'cloaked',\n",
       " 'clocked',\n",
       " 'clodpated',\n",
       " 'cloistered',\n",
       " 'closed',\n",
       " 'closefisted',\n",
       " 'closehanded',\n",
       " 'closehearted',\n",
       " 'closemouthed',\n",
       " 'clotweed',\n",
       " 'clouded',\n",
       " 'clouted',\n",
       " 'clovered',\n",
       " 'clubbed',\n",
       " 'clubfisted',\n",
       " 'clubfooted',\n",
       " 'clubweed',\n",
       " 'clustered',\n",
       " 'coaged',\n",
       " 'coaggregated',\n",
       " 'coated',\n",
       " 'coattailed',\n",
       " 'cobbed',\n",
       " 'cocashweed',\n",
       " 'cochleated',\n",
       " 'cockaded',\n",
       " 'cocked',\n",
       " 'cockeyed',\n",
       " 'cockled',\n",
       " 'cockneybred',\n",
       " 'cockscombed',\n",
       " 'cockweed',\n",
       " 'codheaded',\n",
       " 'coed',\n",
       " 'coelongated',\n",
       " 'coembedded',\n",
       " 'coequated',\n",
       " 'coexpanded',\n",
       " 'coffeeweed',\n",
       " 'cogged',\n",
       " 'coifed',\n",
       " 'coiled',\n",
       " 'coldhearted',\n",
       " 'coleseed',\n",
       " 'colicweed',\n",
       " 'collared',\n",
       " 'collected',\n",
       " 'collied',\n",
       " 'colloped',\n",
       " 'colonnaded',\n",
       " 'colored',\n",
       " 'columnated',\n",
       " 'columned',\n",
       " 'combed',\n",
       " 'combined',\n",
       " 'compacted',\n",
       " 'complected',\n",
       " 'complexioned',\n",
       " 'complicated',\n",
       " 'componed',\n",
       " 'componented',\n",
       " 'composed',\n",
       " 'compressed',\n",
       " 'comprised',\n",
       " 'compulsed',\n",
       " 'conamed',\n",
       " 'concamerated',\n",
       " 'concealed',\n",
       " 'conceded',\n",
       " 'conceited',\n",
       " 'concentrated',\n",
       " 'concerned',\n",
       " 'concerted',\n",
       " 'conched',\n",
       " 'conchyliated',\n",
       " 'condemned',\n",
       " 'condensed',\n",
       " 'conditioned',\n",
       " 'conduplicated',\n",
       " 'coned',\n",
       " 'confated',\n",
       " 'conferted',\n",
       " 'confined',\n",
       " 'confirmed',\n",
       " 'conflated',\n",
       " 'confounded',\n",
       " 'confused',\n",
       " 'congested',\n",
       " 'conjoined',\n",
       " 'conjugated',\n",
       " 'connected',\n",
       " 'conred',\n",
       " 'consecrated',\n",
       " 'considered',\n",
       " 'consolidated',\n",
       " 'constrained',\n",
       " 'constricted',\n",
       " 'consumpted',\n",
       " 'contagioned',\n",
       " 'contented',\n",
       " 'contextured',\n",
       " 'continued',\n",
       " 'contorted',\n",
       " 'contortioned',\n",
       " 'contracted',\n",
       " 'contractured',\n",
       " 'contusioned',\n",
       " 'converted',\n",
       " 'convexed',\n",
       " 'convinced',\n",
       " 'convoluted',\n",
       " 'coolheaded',\n",
       " 'coolweed',\n",
       " 'copied',\n",
       " 'copleased',\n",
       " 'copped',\n",
       " 'coppernosed',\n",
       " 'copperytailed',\n",
       " 'coppiced',\n",
       " 'coppled',\n",
       " 'copsewooded',\n",
       " 'copygraphed',\n",
       " 'coraled',\n",
       " 'corded',\n",
       " 'corduroyed',\n",
       " 'cored',\n",
       " 'coreflexed',\n",
       " 'corked',\n",
       " 'cornered',\n",
       " 'cornified',\n",
       " 'cornuated',\n",
       " 'cornuted',\n",
       " 'corollated',\n",
       " 'coronaled',\n",
       " 'coronated',\n",
       " 'coroneted',\n",
       " 'coronetted',\n",
       " 'corpusculated',\n",
       " 'corrected',\n",
       " 'correlated',\n",
       " 'corridored',\n",
       " ...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's find words ending with ed using the regular expression\n",
    "[w for w in wordlist if re.search('ed$', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **. wildcard symbol** matches any single character. Suppose we have room in a crossword puzzle for an 8-letter word with j as its third letter and t as its sixth letter. In place of each blank cell we use a period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abjectly',\n",
       " 'adjuster',\n",
       " 'dejected',\n",
       " 'dejectly',\n",
       " 'injector',\n",
       " 'majestic',\n",
       " 'objectee',\n",
       " 'objector',\n",
       " 'rejecter',\n",
       " 'rejector',\n",
       " 'unjilted',\n",
       " 'unjolted',\n",
       " 'unjustly']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wordlist if re.search('^..j..t..$', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the **? symbol** specifies that the previous character is optional. Thus «^e-?mail$» will match both email and e-mail.\n",
    "\n",
    "![T9 System](https://cdn.mathpix.com/snip/images/mHuwW-6HJFGYyLhcV3xmkfRKrVOVAtMQYUFNbJBXnVU.original.fullsize.png)\n",
    "\n",
    "The T9 system is used for entering text on mobile phones. Two or more words that are entered with the same sequence of keystrokes are known as textonyms. For example, both hole and golf are entered by pressing the sequence 4653. What other words could be produced with the same sequence? Here we use the regular expression «^[ghi][mno][jlk][def]$»:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gold', 'golf', 'hold', 'hole']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee',\n",
       " 'miiiiiinnnnnnnnnneeeeeeee',\n",
       " 'mine',\n",
       " 'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's explore the + symbol a bit further. \n",
    "# Notice that it can be applied to individual letters, or to bracketed sets of letters:\n",
    "chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))\n",
    "[w for w in chat_words if re.search('^m+i+n+e+$', w)]\n",
    "#It should be clear that + simply means \"one or more instances of the preceding item\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'e',\n",
       " 'i',\n",
       " 'in',\n",
       " 'm',\n",
       " 'me',\n",
       " 'meeeeeeeeeeeee',\n",
       " 'mi',\n",
       " 'miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee',\n",
       " 'miiiiiinnnnnnnnnneeeeeeee',\n",
       " 'min',\n",
       " 'mine',\n",
       " 'mm',\n",
       " 'mmm',\n",
       " 'mmmm',\n",
       " 'mmmmm',\n",
       " 'mmmmmm',\n",
       " 'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee',\n",
       " 'mmmmmmmmmm',\n",
       " 'mmmmmmmmmmmmm',\n",
       " 'mmmmmmmmmmmmmm',\n",
       " 'n',\n",
       " 'ne']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's replace + with *, which means \"zero or more instances of the preceding item\"\n",
    "[w for w in chat_words if re.search('^m*i*n*e*$', w)]\n",
    "#Note:the + and * symbols are sometimes referred to as Kleene closures, or simply closures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.0085',\n",
       " '0.05',\n",
       " '0.1',\n",
       " '0.16',\n",
       " '0.2',\n",
       " '0.25',\n",
       " '0.28',\n",
       " '0.3',\n",
       " '0.4',\n",
       " '0.5',\n",
       " '0.50',\n",
       " '0.54',\n",
       " '0.56',\n",
       " '0.60',\n",
       " '0.7',\n",
       " '0.82',\n",
       " '0.84',\n",
       " '0.9',\n",
       " '0.95',\n",
       " '0.99',\n",
       " '1.01',\n",
       " '1.1',\n",
       " '1.125',\n",
       " '1.14',\n",
       " '1.1650',\n",
       " '1.17',\n",
       " '1.18',\n",
       " '1.19',\n",
       " '1.2',\n",
       " '1.20',\n",
       " '1.24',\n",
       " '1.25',\n",
       " '1.26',\n",
       " '1.28',\n",
       " '1.35',\n",
       " '1.39',\n",
       " '1.4',\n",
       " '1.457',\n",
       " '1.46',\n",
       " '1.49',\n",
       " '1.5',\n",
       " '1.50',\n",
       " '1.55',\n",
       " '1.56',\n",
       " '1.5755',\n",
       " '1.5805',\n",
       " '1.6',\n",
       " '1.61',\n",
       " '1.637',\n",
       " '1.64',\n",
       " '1.65',\n",
       " '1.7',\n",
       " '1.75',\n",
       " '1.76',\n",
       " '1.8',\n",
       " '1.82',\n",
       " '1.8415',\n",
       " '1.85',\n",
       " '1.8500',\n",
       " '1.9',\n",
       " '1.916',\n",
       " '1.92',\n",
       " '10.19',\n",
       " '10.2',\n",
       " '10.5',\n",
       " '107.03',\n",
       " '107.9',\n",
       " '109.73',\n",
       " '11.10',\n",
       " '11.5',\n",
       " '11.57',\n",
       " '11.6',\n",
       " '11.72',\n",
       " '11.95',\n",
       " '112.9',\n",
       " '113.2',\n",
       " '116.3',\n",
       " '116.4',\n",
       " '116.7',\n",
       " '116.9',\n",
       " '118.6',\n",
       " '12.09',\n",
       " '12.5',\n",
       " '12.52',\n",
       " '12.68',\n",
       " '12.7',\n",
       " '12.82',\n",
       " '12.97',\n",
       " '120.7',\n",
       " '1206.26',\n",
       " '121.6',\n",
       " '126.1',\n",
       " '126.15',\n",
       " '127.03',\n",
       " '129.91',\n",
       " '13.1',\n",
       " '13.15',\n",
       " '13.5',\n",
       " '13.50',\n",
       " '13.625',\n",
       " '13.65',\n",
       " '13.73',\n",
       " '13.8',\n",
       " '13.90',\n",
       " '130.6',\n",
       " '130.7',\n",
       " '131.01',\n",
       " '132.9',\n",
       " '133.7',\n",
       " '133.8',\n",
       " '14.00',\n",
       " '14.13',\n",
       " '14.26',\n",
       " '14.28',\n",
       " '14.43',\n",
       " '14.5',\n",
       " '14.53',\n",
       " '14.54',\n",
       " '14.6',\n",
       " '14.75',\n",
       " '14.99',\n",
       " '141.9',\n",
       " '142.84',\n",
       " '142.85',\n",
       " '143.08',\n",
       " '143.80',\n",
       " '143.93',\n",
       " '148.9',\n",
       " '149.9',\n",
       " '15.5',\n",
       " '150.00',\n",
       " '153.3',\n",
       " '154.2',\n",
       " '16.05',\n",
       " '16.09',\n",
       " '16.125',\n",
       " '16.2',\n",
       " '16.5',\n",
       " '16.68',\n",
       " '16.7',\n",
       " '16.9',\n",
       " '169.9',\n",
       " '17.3',\n",
       " '17.4',\n",
       " '17.5',\n",
       " '17.95',\n",
       " '1738.1',\n",
       " '176.1',\n",
       " '18.3',\n",
       " '18.6',\n",
       " '18.95',\n",
       " '185.9',\n",
       " '188.84',\n",
       " '19.3',\n",
       " '19.50',\n",
       " '19.6',\n",
       " '19.94',\n",
       " '19.95',\n",
       " '191.9',\n",
       " '2.07',\n",
       " '2.1',\n",
       " '2.15',\n",
       " '2.19',\n",
       " '2.2',\n",
       " '2.25',\n",
       " '2.29',\n",
       " '2.3',\n",
       " '2.30',\n",
       " '2.35',\n",
       " '2.375',\n",
       " '2.4',\n",
       " '2.42',\n",
       " '2.44',\n",
       " '2.46',\n",
       " '2.47',\n",
       " '2.5',\n",
       " '2.50',\n",
       " '2.6',\n",
       " '2.62',\n",
       " '2.65',\n",
       " '2.7',\n",
       " '2.75',\n",
       " '2.8',\n",
       " '2.80',\n",
       " '2.87',\n",
       " '2.875',\n",
       " '2.9',\n",
       " '2.95',\n",
       " '20.07',\n",
       " '20.5',\n",
       " '21.1',\n",
       " '21.9',\n",
       " '2141.7',\n",
       " '2160.1',\n",
       " '2163.2',\n",
       " '22.75',\n",
       " '220.45',\n",
       " '221.4',\n",
       " '225.6',\n",
       " '23.25',\n",
       " '23.4',\n",
       " '23.5',\n",
       " '23.72',\n",
       " '234.4',\n",
       " '236.74',\n",
       " '236.79',\n",
       " '24.95',\n",
       " '25.50',\n",
       " '25.6',\n",
       " '251.2',\n",
       " '26.2',\n",
       " '26.5',\n",
       " '26.8',\n",
       " '263.07',\n",
       " '2645.90',\n",
       " '2691.19',\n",
       " '27.1',\n",
       " '27.4',\n",
       " '273.5',\n",
       " '278.7',\n",
       " '28.25',\n",
       " '28.36',\n",
       " '28.4',\n",
       " '28.5',\n",
       " '28.53',\n",
       " '28.6',\n",
       " '29.3',\n",
       " '29.4',\n",
       " '29.9',\n",
       " '292.32',\n",
       " '3.01',\n",
       " '3.04',\n",
       " '3.1',\n",
       " '3.16',\n",
       " '3.18',\n",
       " '3.19',\n",
       " '3.2',\n",
       " '3.20',\n",
       " '3.23',\n",
       " '3.253',\n",
       " '3.28',\n",
       " '3.3',\n",
       " '3.35',\n",
       " '3.375',\n",
       " '3.4',\n",
       " '3.42',\n",
       " '3.43',\n",
       " '3.5',\n",
       " '3.55',\n",
       " '3.6',\n",
       " '3.61',\n",
       " '3.625',\n",
       " '3.7',\n",
       " '3.75',\n",
       " '3.8',\n",
       " '3.80',\n",
       " '3.9',\n",
       " '30.6',\n",
       " '30.9',\n",
       " '319.75',\n",
       " '32.8',\n",
       " '334.5',\n",
       " '34.625',\n",
       " '341.20',\n",
       " '3436.58',\n",
       " '35.2',\n",
       " '35.7',\n",
       " '352.7',\n",
       " '352.9',\n",
       " '35500.64',\n",
       " '35564.43',\n",
       " '36.9',\n",
       " '361.8',\n",
       " '3648.82',\n",
       " '37.3',\n",
       " '37.5',\n",
       " '372.14',\n",
       " '372.9',\n",
       " '374.19',\n",
       " '374.20',\n",
       " '377.60',\n",
       " '38.3',\n",
       " '38.375',\n",
       " '38.5',\n",
       " '38.875',\n",
       " '387.8',\n",
       " '4.1',\n",
       " '4.10',\n",
       " '4.2',\n",
       " '4.25',\n",
       " '4.3',\n",
       " '4.4',\n",
       " '4.5',\n",
       " '4.55',\n",
       " '4.6',\n",
       " '4.7',\n",
       " '4.75',\n",
       " '4.8',\n",
       " '4.875',\n",
       " '4.898',\n",
       " '4.9',\n",
       " '40.21',\n",
       " '41.60',\n",
       " '415.6',\n",
       " '415.8',\n",
       " '42.1',\n",
       " '42.5',\n",
       " '422.5',\n",
       " '43.875',\n",
       " '434.4',\n",
       " '436.01',\n",
       " '446.62',\n",
       " '449.04',\n",
       " '45.2',\n",
       " '45.3',\n",
       " '45.75',\n",
       " '456.64',\n",
       " '46.1',\n",
       " '47.1',\n",
       " '47.125',\n",
       " '47.5',\n",
       " '47.6',\n",
       " '49.9',\n",
       " '494.50',\n",
       " '497.34',\n",
       " '5.1',\n",
       " '5.2180',\n",
       " '5.276',\n",
       " '5.29',\n",
       " '5.3',\n",
       " '5.39',\n",
       " '5.4',\n",
       " '5.435',\n",
       " '5.5',\n",
       " '5.57',\n",
       " '5.6',\n",
       " '5.63',\n",
       " '5.7',\n",
       " '5.70',\n",
       " '5.8',\n",
       " '5.82',\n",
       " '5.9',\n",
       " '5.92',\n",
       " '50.1',\n",
       " '50.38',\n",
       " '50.45',\n",
       " '51.25',\n",
       " '51.6',\n",
       " '55.1',\n",
       " '566.54',\n",
       " '57.50',\n",
       " '57.6',\n",
       " '57.7',\n",
       " '58.64',\n",
       " '59.6',\n",
       " '59.9',\n",
       " '6.03',\n",
       " '6.1',\n",
       " '6.20',\n",
       " '6.21',\n",
       " '6.25',\n",
       " '6.4',\n",
       " '6.40',\n",
       " '6.44',\n",
       " '6.5',\n",
       " '6.50',\n",
       " '6.53',\n",
       " '6.6',\n",
       " '6.7',\n",
       " '6.70',\n",
       " '6.79',\n",
       " '6.84',\n",
       " '6.9',\n",
       " '60.36',\n",
       " '618.1',\n",
       " '62.1',\n",
       " '62.5',\n",
       " '62.625',\n",
       " '63.79',\n",
       " '630.9',\n",
       " '64.5',\n",
       " '66.5',\n",
       " '7.15',\n",
       " '7.2',\n",
       " '7.20',\n",
       " '7.272',\n",
       " '7.3',\n",
       " '7.4',\n",
       " '7.40',\n",
       " '7.422',\n",
       " '7.45',\n",
       " '7.458',\n",
       " '7.5',\n",
       " '7.50',\n",
       " '7.52',\n",
       " '7.55',\n",
       " '7.60',\n",
       " '7.62',\n",
       " '7.63',\n",
       " '7.65',\n",
       " '7.74',\n",
       " '7.78',\n",
       " '7.79',\n",
       " '7.8',\n",
       " '7.80',\n",
       " '7.84',\n",
       " '7.88',\n",
       " '7.90',\n",
       " '7.95',\n",
       " '70.2',\n",
       " '70.7',\n",
       " '705.6',\n",
       " '72.7',\n",
       " '734.9',\n",
       " '737.5',\n",
       " '77.56',\n",
       " '77.6',\n",
       " '77.70',\n",
       " '8.04',\n",
       " '8.06',\n",
       " '8.07',\n",
       " '8.1',\n",
       " '8.12',\n",
       " '8.14',\n",
       " '8.15',\n",
       " '8.19',\n",
       " '8.2',\n",
       " '8.22',\n",
       " '8.25',\n",
       " '8.30',\n",
       " '8.35',\n",
       " '8.45',\n",
       " '8.467',\n",
       " '8.47',\n",
       " '8.48',\n",
       " '8.5',\n",
       " '8.50',\n",
       " '8.53',\n",
       " '8.55',\n",
       " '8.56',\n",
       " '8.575',\n",
       " '8.60',\n",
       " '8.64',\n",
       " '8.65',\n",
       " '8.70',\n",
       " '8.75',\n",
       " '8.9',\n",
       " '80.50',\n",
       " '80.8',\n",
       " '81.8',\n",
       " '811.9',\n",
       " '83.4',\n",
       " '84.29',\n",
       " '84.9',\n",
       " '85.1',\n",
       " '85.7',\n",
       " '86.12',\n",
       " '87.5',\n",
       " '88.32',\n",
       " '89.7',\n",
       " '89.9',\n",
       " '9.3',\n",
       " '9.32',\n",
       " '9.37',\n",
       " '9.45',\n",
       " '9.5',\n",
       " '9.625',\n",
       " '9.75',\n",
       " '9.8',\n",
       " '9.82',\n",
       " '9.9',\n",
       " '92.9',\n",
       " '93.3',\n",
       " '93.9',\n",
       " '94.2',\n",
       " '94.8',\n",
       " '95.09',\n",
       " '96.4',\n",
       " '98.3',\n",
       " '99.1',\n",
       " '99.3']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "#Here the [numerical] tellls us any numerical character from 0 to 9.\n",
    "#the \\. is likely to mean literal character instead of the . wildcard symbol\n",
    "# Of course the + means one or more instances\n",
    "[w for w in wsj if re.search('^[0-9]+\\.[0-9]+$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C$', 'US$']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here [A-Z] is any capital character, one or more. \n",
    "#The \\$ means literaly $ instead of the special use\n",
    "[w for w in wsj if re.search('^[A-Z]+\\$$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1614',\n",
       " '1637',\n",
       " '1787',\n",
       " '1901',\n",
       " '1903',\n",
       " '1917',\n",
       " '1925',\n",
       " '1929',\n",
       " '1933',\n",
       " '1934',\n",
       " '1948',\n",
       " '1953',\n",
       " '1955',\n",
       " '1956',\n",
       " '1961',\n",
       " '1965',\n",
       " '1966',\n",
       " '1967',\n",
       " '1968',\n",
       " '1969',\n",
       " '1970',\n",
       " '1971',\n",
       " '1972',\n",
       " '1973',\n",
       " '1975',\n",
       " '1976',\n",
       " '1977',\n",
       " '1979',\n",
       " '1980',\n",
       " '1981',\n",
       " '1982',\n",
       " '1983',\n",
       " '1984',\n",
       " '1985',\n",
       " '1986',\n",
       " '1987',\n",
       " '1988',\n",
       " '1989',\n",
       " '1990',\n",
       " '1991',\n",
       " '1992',\n",
       " '1993',\n",
       " '1994',\n",
       " '1995',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '1999',\n",
       " '2000',\n",
       " '2005',\n",
       " '2009',\n",
       " '2017',\n",
       " '2019',\n",
       " '2029',\n",
       " '3057',\n",
       " '8300']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The {4} denotes groups of 4\n",
    "[w for w in wsj if re.search('^[0-9]{4}$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10-day',\n",
       " '10-lap',\n",
       " '10-year',\n",
       " '100-share',\n",
       " '12-point',\n",
       " '12-year',\n",
       " '14-hour',\n",
       " '15-day',\n",
       " '150-point',\n",
       " '190-point',\n",
       " '20-point',\n",
       " '20-stock',\n",
       " '21-month',\n",
       " '237-seat',\n",
       " '240-page',\n",
       " '27-year',\n",
       " '30-day',\n",
       " '30-point',\n",
       " '30-share',\n",
       " '30-year',\n",
       " '300-day',\n",
       " '36-day',\n",
       " '36-store',\n",
       " '42-year',\n",
       " '50-state',\n",
       " '500-stock',\n",
       " '52-week',\n",
       " '69-point',\n",
       " '84-month',\n",
       " '87-store',\n",
       " '90-day']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The \"-\" character has no special operation in regex\n",
    "#the {3,5} tells us groups of 3 to 5 characters\n",
    "[w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['black-and-white',\n",
       " 'bread-and-butter',\n",
       " 'father-in-law',\n",
       " 'machine-gun-toting',\n",
       " 'savings-and-loan']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The {5,} tells us groups of 5 characters or more.\n",
    "#The {2,3} tell us groups of 2 to 3 characters\n",
    "#The {,6} tells us groups of 1 to 6 characters \n",
    "[w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['62%-owned',\n",
       " 'Absorbed',\n",
       " 'According',\n",
       " 'Adopting',\n",
       " 'Advanced',\n",
       " 'Advancing',\n",
       " 'Alfred',\n",
       " 'Allied',\n",
       " 'Annualized',\n",
       " 'Anything',\n",
       " 'Arbitrage-related',\n",
       " 'Arbitraging',\n",
       " 'Asked',\n",
       " 'Assuming',\n",
       " 'Atlanta-based',\n",
       " 'Baking',\n",
       " 'Banking',\n",
       " 'Beginning',\n",
       " 'Beijing',\n",
       " 'Being',\n",
       " 'Bermuda-based',\n",
       " 'Betting',\n",
       " 'Boeing',\n",
       " 'Broadcasting',\n",
       " 'Bucking',\n",
       " 'Buying',\n",
       " 'Calif.-based',\n",
       " 'Change-ringing',\n",
       " 'Citing',\n",
       " 'Concerned',\n",
       " 'Confronted',\n",
       " 'Conn.based',\n",
       " 'Consolidated',\n",
       " 'Continued',\n",
       " 'Continuing',\n",
       " 'Declining',\n",
       " 'Defending',\n",
       " 'Depending',\n",
       " 'Designated',\n",
       " 'Determining',\n",
       " 'Developed',\n",
       " 'Died',\n",
       " 'During',\n",
       " 'Encouraged',\n",
       " 'Encouraging',\n",
       " 'English-speaking',\n",
       " 'Estimated',\n",
       " 'Everything',\n",
       " 'Excluding',\n",
       " 'Exxon-owned',\n",
       " 'Faulding',\n",
       " 'Fed',\n",
       " 'Feeding',\n",
       " 'Filling',\n",
       " 'Filmed',\n",
       " 'Financing',\n",
       " 'Following',\n",
       " 'Founded',\n",
       " 'Fracturing',\n",
       " 'Francisco-based',\n",
       " 'Fred',\n",
       " 'Funded',\n",
       " 'Funding',\n",
       " 'Generalized',\n",
       " 'Germany-based',\n",
       " 'Getting',\n",
       " 'Guaranteed',\n",
       " 'Having',\n",
       " 'Heating',\n",
       " 'Heightened',\n",
       " 'Holding',\n",
       " 'Housing',\n",
       " 'Illuminating',\n",
       " 'Indeed',\n",
       " 'Indexing',\n",
       " 'Irving',\n",
       " 'Jersey-based',\n",
       " 'Judging',\n",
       " 'Knowing',\n",
       " 'Learning',\n",
       " 'Legislating',\n",
       " 'Leming',\n",
       " 'Limited',\n",
       " 'London-based',\n",
       " 'Manfred',\n",
       " 'Manufacturing',\n",
       " 'Melamed',\n",
       " 'Miami-based',\n",
       " 'Mich.-based',\n",
       " 'Mining',\n",
       " 'Minneapolis-based',\n",
       " 'Mo.-based',\n",
       " 'Mortgage-Backed',\n",
       " 'Moving',\n",
       " 'Muzzling',\n",
       " 'N.J.-based',\n",
       " 'NBC-owned',\n",
       " 'NIH-appointed',\n",
       " 'Named',\n",
       " 'No-Smoking',\n",
       " 'Observing',\n",
       " 'Offering',\n",
       " 'Ohio-based',\n",
       " 'Orleans-based',\n",
       " 'Packaging',\n",
       " 'Performing',\n",
       " 'Philadelphia-based',\n",
       " 'Posted',\n",
       " 'Provided',\n",
       " 'Publishing',\n",
       " 'Purchasing',\n",
       " 'Rated',\n",
       " 'Reached',\n",
       " 'Red',\n",
       " 'Red-blooded',\n",
       " 'Reducing',\n",
       " 'Reed',\n",
       " 'Regarded',\n",
       " 'Rekindled',\n",
       " 'Related',\n",
       " 'Ringing',\n",
       " 'Rolling',\n",
       " 'Sacramento-based',\n",
       " 'Scoring',\n",
       " 'Seattle-based',\n",
       " 'Seed',\n",
       " 'Skilled',\n",
       " 'Smelting',\n",
       " 'Something',\n",
       " 'Spending',\n",
       " 'Standardized',\n",
       " 'Standing',\n",
       " 'Starting',\n",
       " 'Sterling',\n",
       " 'Taking',\n",
       " 'Texas-based',\n",
       " 'Toronto-based',\n",
       " 'Traded',\n",
       " 'Trading',\n",
       " 'Troubled',\n",
       " 'U.N.-supervised',\n",
       " 'U.S.-backed',\n",
       " 'United',\n",
       " 'Used',\n",
       " 'Varying',\n",
       " 'Washington-based',\n",
       " 'Whiting',\n",
       " 'Wilfred',\n",
       " 'Winning',\n",
       " 'Xiaoping',\n",
       " 'York-based',\n",
       " 'Zayed',\n",
       " 'abandoned',\n",
       " 'abating',\n",
       " 'abolishing',\n",
       " 'abortion-related',\n",
       " 'abounding',\n",
       " 'abridging',\n",
       " 'absorbed',\n",
       " 'acceded',\n",
       " 'accelerated',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'according',\n",
       " 'accounted',\n",
       " 'accounting',\n",
       " 'accrued',\n",
       " 'accumulated',\n",
       " 'accused',\n",
       " 'accusing',\n",
       " 'achieved',\n",
       " 'achieving',\n",
       " 'acknowledging',\n",
       " 'acquired',\n",
       " 'acquiring',\n",
       " 'acquisition-minded',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'adapted',\n",
       " 'adapting',\n",
       " 'added',\n",
       " 'adding',\n",
       " 'addressing',\n",
       " 'adjusted',\n",
       " 'adjusting',\n",
       " 'admitted',\n",
       " 'admitting',\n",
       " 'adopted',\n",
       " 'advanced',\n",
       " 'advancing',\n",
       " 'advertised',\n",
       " 'advertising',\n",
       " 'advised',\n",
       " 'advocated',\n",
       " 'advocating',\n",
       " 'affecting',\n",
       " 'afflicted',\n",
       " 'aggravated',\n",
       " 'agreed',\n",
       " 'agreeing',\n",
       " 'ailing',\n",
       " 'aimed',\n",
       " 'aiming',\n",
       " 'aired',\n",
       " 'airline-related',\n",
       " 'alarmed',\n",
       " 'alienated',\n",
       " 'alleged',\n",
       " 'alleging',\n",
       " 'allocated',\n",
       " 'allowed',\n",
       " 'altered',\n",
       " 'altering',\n",
       " 'amended',\n",
       " 'amending',\n",
       " 'amounted',\n",
       " 'amusing',\n",
       " 'angered',\n",
       " 'announced',\n",
       " 'annoyed',\n",
       " 'annualized',\n",
       " 'answered',\n",
       " 'anti-dumping',\n",
       " 'anticipated',\n",
       " 'anticipating',\n",
       " 'anything',\n",
       " 'apologizing',\n",
       " 'appealing',\n",
       " 'appeared',\n",
       " 'appearing',\n",
       " 'applied',\n",
       " 'appointed',\n",
       " 'approached',\n",
       " 'appropriated',\n",
       " 'approved',\n",
       " 'arched',\n",
       " 'argued',\n",
       " 'arguing',\n",
       " 'arising',\n",
       " 'armed',\n",
       " 'arranged',\n",
       " 'arrested',\n",
       " 'arrived',\n",
       " 'asbestos-related',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'assassinated',\n",
       " 'assembled',\n",
       " 'asserted',\n",
       " 'asserting',\n",
       " 'assessed',\n",
       " 'assigned',\n",
       " 'assisted',\n",
       " 'associated',\n",
       " 'assumed',\n",
       " 'assuming',\n",
       " 'assured',\n",
       " 'attached',\n",
       " 'attacking',\n",
       " 'attempted',\n",
       " 'attempting',\n",
       " 'attended',\n",
       " 'attending',\n",
       " 'attracted',\n",
       " 'attracting',\n",
       " 'attributed',\n",
       " 'auctioned',\n",
       " 'authorized',\n",
       " 'authorizing',\n",
       " 'automated',\n",
       " 'automotive-lighting',\n",
       " 'averaged',\n",
       " 'averted',\n",
       " 'avoiding',\n",
       " 'awarded',\n",
       " 'awarding',\n",
       " 'backed',\n",
       " 'backing',\n",
       " 'balanced',\n",
       " 'bald-faced',\n",
       " 'balkanized',\n",
       " 'balked',\n",
       " 'balloting',\n",
       " 'bank-backed',\n",
       " 'banking',\n",
       " 'banned',\n",
       " 'banning',\n",
       " 'barking',\n",
       " 'barred',\n",
       " 'based',\n",
       " 'battered',\n",
       " 'battery-operated',\n",
       " 'batting',\n",
       " 'bearing',\n",
       " 'becoming',\n",
       " 'bedding',\n",
       " 'befuddled',\n",
       " 'beginning',\n",
       " 'behaving',\n",
       " 'beheading',\n",
       " 'being',\n",
       " 'beleaguered',\n",
       " 'believed',\n",
       " 'bell-ringing',\n",
       " 'belonging',\n",
       " 'benefited',\n",
       " 'best-selling',\n",
       " 'betting',\n",
       " 'bickering',\n",
       " 'bidding',\n",
       " 'billed',\n",
       " 'billing',\n",
       " 'blamed',\n",
       " 'bled',\n",
       " 'blessing',\n",
       " 'blighted',\n",
       " 'blocked',\n",
       " 'blurred',\n",
       " 'boarding',\n",
       " 'bolstered',\n",
       " 'bombarding',\n",
       " 'booked',\n",
       " 'booming',\n",
       " 'boosted',\n",
       " 'boosting',\n",
       " 'borrowed',\n",
       " 'borrowing',\n",
       " 'botched',\n",
       " 'bothered',\n",
       " 'bounced',\n",
       " 'bowed',\n",
       " 'breaking',\n",
       " 'breathed',\n",
       " 'breathtaking',\n",
       " 'breed',\n",
       " 'bribed',\n",
       " 'bribing',\n",
       " 'briefing',\n",
       " 'brightened',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'broad-based',\n",
       " 'broadcasting',\n",
       " 'broadened',\n",
       " 'brokering',\n",
       " 'brushed',\n",
       " 'budding',\n",
       " 'building',\n",
       " 'bundling',\n",
       " 'buoyed',\n",
       " 'burned',\n",
       " 'buying',\n",
       " 'calculated',\n",
       " 'called',\n",
       " 'calling',\n",
       " 'campaigning',\n",
       " 'cancer-causing',\n",
       " 'capitalized',\n",
       " 'capped',\n",
       " 'captivating',\n",
       " 'cared',\n",
       " 'carried',\n",
       " 'carrying',\n",
       " 'cascading',\n",
       " 'casting',\n",
       " 'caused',\n",
       " 'causing',\n",
       " 'cautioned',\n",
       " 'ceiling',\n",
       " 'centralized',\n",
       " 'certified',\n",
       " 'chaired',\n",
       " 'challenging',\n",
       " 'championing',\n",
       " 'change-ringing',\n",
       " 'changed',\n",
       " 'changing',\n",
       " 'characterized',\n",
       " 'characterizing',\n",
       " 'charged',\n",
       " 'charging',\n",
       " 'chastised',\n",
       " 'cheating',\n",
       " 'checking',\n",
       " 'cheerleading',\n",
       " 'chilled',\n",
       " 'choosing',\n",
       " 'chopped',\n",
       " 'circulated',\n",
       " 'cited',\n",
       " 'citing',\n",
       " 'citizen-sparked',\n",
       " 'city-owned',\n",
       " 'claimed',\n",
       " 'claiming',\n",
       " 'clamped',\n",
       " 'clarified',\n",
       " 'clashed',\n",
       " 'classed',\n",
       " 'classified',\n",
       " 'cleaned',\n",
       " 'cleaner-burning',\n",
       " 'cleared',\n",
       " 'clearing',\n",
       " 'clicked',\n",
       " 'climbed',\n",
       " 'climbing',\n",
       " 'clipped',\n",
       " 'clobbered',\n",
       " 'closed',\n",
       " 'closing',\n",
       " 'clothing',\n",
       " 'clouding',\n",
       " 'cluttered',\n",
       " 'co-founded',\n",
       " 'coaching',\n",
       " 'coal-fired',\n",
       " 'coated',\n",
       " 'codified',\n",
       " 'collaborated',\n",
       " 'collapsed',\n",
       " 'collected',\n",
       " 'collecting',\n",
       " 'collective-bargaining',\n",
       " 'colored',\n",
       " 'combined',\n",
       " 'coming',\n",
       " 'commanded',\n",
       " 'commenting',\n",
       " 'committed',\n",
       " 'committing',\n",
       " 'compared',\n",
       " 'compelling',\n",
       " 'competed',\n",
       " 'competing',\n",
       " 'compiled',\n",
       " 'complained',\n",
       " 'complaining',\n",
       " 'completed',\n",
       " 'completing',\n",
       " 'complicated',\n",
       " 'composed',\n",
       " 'composting',\n",
       " 'compressed',\n",
       " 'computer-aided',\n",
       " 'computer-assisted',\n",
       " 'computer-generated',\n",
       " 'computerized',\n",
       " 'computing',\n",
       " 'conceding',\n",
       " 'concentrated',\n",
       " 'concentrating',\n",
       " 'concerned',\n",
       " 'concluded',\n",
       " 'condemned',\n",
       " 'condemning',\n",
       " 'conducted',\n",
       " 'conducting',\n",
       " 'confined',\n",
       " 'confirmed',\n",
       " 'confused',\n",
       " 'connected',\n",
       " 'consented',\n",
       " 'considered',\n",
       " 'considering',\n",
       " 'consisting',\n",
       " 'construed',\n",
       " 'consulting',\n",
       " 'contacted',\n",
       " 'contained',\n",
       " 'containing',\n",
       " 'contesting',\n",
       " 'continued',\n",
       " 'continuing',\n",
       " 'contracted',\n",
       " 'contributed',\n",
       " 'contributing',\n",
       " 'controlled',\n",
       " 'controlling',\n",
       " 'converted',\n",
       " 'converting',\n",
       " 'convicted',\n",
       " 'convinced',\n",
       " 'cooled',\n",
       " 'cooperating',\n",
       " 'copied',\n",
       " 'copying',\n",
       " 'corn-buying',\n",
       " 'corrected',\n",
       " 'correcting',\n",
       " 'cost-cutting',\n",
       " 'cost-sharing',\n",
       " 'counseling',\n",
       " 'counting',\n",
       " 'coupled',\n",
       " 'court-ordered',\n",
       " 'covered',\n",
       " 'covering',\n",
       " 'cranked',\n",
       " 'crashing',\n",
       " 'created',\n",
       " 'creating',\n",
       " 'credit-rating',\n",
       " 'crippled',\n",
       " 'criticized',\n",
       " 'crossed',\n",
       " 'crossing',\n",
       " 'crowded',\n",
       " 'cruising',\n",
       " 'crushed',\n",
       " 'crying',\n",
       " 'cultivated',\n",
       " 'curbed',\n",
       " 'curbing',\n",
       " 'curled',\n",
       " 'current-carrying',\n",
       " 'curtailed',\n",
       " 'cushioned',\n",
       " 'customized',\n",
       " 'cutting',\n",
       " 'damaged',\n",
       " 'damaging',\n",
       " 'dancing',\n",
       " 'darned',\n",
       " 'dashed',\n",
       " 'dating',\n",
       " 'dead-eyed',\n",
       " 'dealing',\n",
       " 'decided',\n",
       " 'declared',\n",
       " 'declaring',\n",
       " 'declined',\n",
       " 'declining',\n",
       " 'decorated',\n",
       " 'decried',\n",
       " 'deducting',\n",
       " 'deemed',\n",
       " 'defeated',\n",
       " 'defended',\n",
       " 'defined',\n",
       " 'defying',\n",
       " 'delayed',\n",
       " 'deliberating',\n",
       " 'delisted',\n",
       " 'delivered',\n",
       " 'delivering',\n",
       " 'demanding',\n",
       " 'demonstrating',\n",
       " 'denied',\n",
       " 'denouncing',\n",
       " 'denying',\n",
       " 'depended',\n",
       " 'depending',\n",
       " 'depleted',\n",
       " 'depressed',\n",
       " 'deprived',\n",
       " 'derived',\n",
       " 'descending',\n",
       " 'described',\n",
       " 'deserving',\n",
       " 'designated',\n",
       " 'designed',\n",
       " 'designing',\n",
       " 'desired',\n",
       " 'despised',\n",
       " 'detailed',\n",
       " 'deteriorated',\n",
       " 'deteriorating',\n",
       " 'determined',\n",
       " 'deterring',\n",
       " 'devastating',\n",
       " 'developed',\n",
       " 'developing',\n",
       " 'devised',\n",
       " 'devoted',\n",
       " 'devouring',\n",
       " 'diagnosed',\n",
       " 'died',\n",
       " 'diluted',\n",
       " 'diming',\n",
       " 'diminished',\n",
       " 'directed',\n",
       " 'directing',\n",
       " 'disaffected',\n",
       " 'disagreed',\n",
       " 'disappointed',\n",
       " 'disappointing',\n",
       " 'disapproved',\n",
       " 'discarded',\n",
       " 'disciplined',\n",
       " 'disclosed',\n",
       " 'disclosing',\n",
       " 'discontinued',\n",
       " 'discontinuing',\n",
       " 'discouraging',\n",
       " 'discovered',\n",
       " 'discussed',\n",
       " 'discussing',\n",
       " 'disembodied',\n",
       " 'dismayed',\n",
       " 'dismissed',\n",
       " 'disposed',\n",
       " 'disputed',\n",
       " 'disseminating',\n",
       " 'distinguished',\n",
       " 'distorted',\n",
       " 'distributed',\n",
       " 'disturbing',\n",
       " 'diversified',\n",
       " 'diversifying',\n",
       " 'divided',\n",
       " 'dividing',\n",
       " 'documented',\n",
       " 'doing',\n",
       " 'doling',\n",
       " 'dollar-denominated',\n",
       " 'dominated',\n",
       " 'dominating',\n",
       " 'doubled',\n",
       " 'doubted',\n",
       " 'downgraded',\n",
       " 'downgrading',\n",
       " 'drafted',\n",
       " 'drawing',\n",
       " 'dreamed',\n",
       " 'dressed',\n",
       " 'drifted',\n",
       " 'drinking',\n",
       " 'driving',\n",
       " 'drooled',\n",
       " 'dropped',\n",
       " 'dubbed',\n",
       " 'duckling',\n",
       " 'dumbfounded',\n",
       " 'dumped',\n",
       " 'during',\n",
       " 'dwindling',\n",
       " 'earned',\n",
       " 'earning',\n",
       " 'eased',\n",
       " 'easing',\n",
       " 'eating',\n",
       " 'echoed',\n",
       " 'edged',\n",
       " 'editing',\n",
       " 'educated',\n",
       " 'elected',\n",
       " 'eliminated',\n",
       " 'eliminating',\n",
       " 'embarrassing',\n",
       " 'embroiled',\n",
       " 'emerged',\n",
       " 'emerging',\n",
       " 'emphasized',\n",
       " 'employed',\n",
       " 'empowered',\n",
       " 'enabled',\n",
       " 'enabling',\n",
       " 'enacted',\n",
       " 'encircling',\n",
       " 'enclosed',\n",
       " 'encouraging',\n",
       " 'encroaching',\n",
       " 'ended',\n",
       " 'ending',\n",
       " 'endorsed',\n",
       " 'engaged',\n",
       " 'engaging',\n",
       " 'engineered',\n",
       " 'engineering',\n",
       " 'enhanced',\n",
       " 'enjoyed',\n",
       " 'enjoying',\n",
       " 'enlarged',\n",
       " 'enraged',\n",
       " 'ensnarled',\n",
       " 'entangled',\n",
       " 'entered',\n",
       " 'entering',\n",
       " 'entertaining',\n",
       " 'enticed',\n",
       " 'entitled',\n",
       " 'entrenched',\n",
       " 'entrusted',\n",
       " 'equaling',\n",
       " 'equipped',\n",
       " 'escalated',\n",
       " 'escaped',\n",
       " 'established',\n",
       " 'establishing',\n",
       " 'estimated',\n",
       " 'evaluated',\n",
       " 'evaluating',\n",
       " 'evaporated',\n",
       " 'evening',\n",
       " 'everything',\n",
       " 'evoking',\n",
       " 'evolved',\n",
       " 'exacerbated',\n",
       " 'examined',\n",
       " 'exceed',\n",
       " 'exceeded',\n",
       " 'exceeding',\n",
       " 'exchanging',\n",
       " 'excited',\n",
       " 'exciting',\n",
       " 'executed',\n",
       " 'executing',\n",
       " 'exercised',\n",
       " 'exerting',\n",
       " 'exhausted',\n",
       " 'exhibited',\n",
       " 'existed',\n",
       " 'existing',\n",
       " 'expanded',\n",
       " 'expanding',\n",
       " 'expected',\n",
       " 'expecting',\n",
       " 'expedited',\n",
       " 'expelled',\n",
       " 'experienced',\n",
       " 'experiencing',\n",
       " 'expired',\n",
       " 'explained',\n",
       " 'explaining',\n",
       " 'exploded',\n",
       " 'export-oriented',\n",
       " 'exposed',\n",
       " 'expressed',\n",
       " 'expressing',\n",
       " 'expunged',\n",
       " 'extended',\n",
       " 'extending',\n",
       " 'exuded',\n",
       " 'eyeing',\n",
       " 'fabled',\n",
       " 'faced',\n",
       " 'facing',\n",
       " 'factoring',\n",
       " 'faded',\n",
       " 'failed',\n",
       " 'failing',\n",
       " 'fainting',\n",
       " 'falling',\n",
       " 'faltered',\n",
       " 'famed',\n",
       " 'family-planning',\n",
       " 'fared',\n",
       " 'fashioned',\n",
       " 'fast-growing',\n",
       " 'fastest-growing',\n",
       " 'fattened',\n",
       " 'favored',\n",
       " 'fawning',\n",
       " 'feared',\n",
       " 'featured',\n",
       " 'featuring',\n",
       " 'fed',\n",
       " 'feed',\n",
       " 'feeling',\n",
       " 'fetching',\n",
       " 'fielded',\n",
       " 'fighting',\n",
       " 'filed',\n",
       " 'filing',\n",
       " 'filled',\n",
       " 'filling',\n",
       " 'finalized',\n",
       " 'financed',\n",
       " 'financing',\n",
       " 'finding',\n",
       " 'fined',\n",
       " 'finished',\n",
       " 'fired',\n",
       " 'firmed',\n",
       " 'fixed',\n",
       " 'fizzled',\n",
       " 'fled',\n",
       " 'fledgling',\n",
       " 'fleeting',\n",
       " 'flirted',\n",
       " 'floated',\n",
       " 'flooded',\n",
       " 'focused',\n",
       " 'focusing',\n",
       " 'folded',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'forced',\n",
       " 'forcing',\n",
       " 'forecasting',\n",
       " 'foreign-led',\n",
       " 'formed',\n",
       " 'forthcoming',\n",
       " 'founded',\n",
       " 'foundering',\n",
       " 'fretted',\n",
       " 'frightened',\n",
       " 'frustrating',\n",
       " 'fueled',\n",
       " 'fueling',\n",
       " 'full-fledged',\n",
       " 'fuming',\n",
       " 'functioning',\n",
       " 'funded',\n",
       " 'funding',\n",
       " 'fundraising',\n",
       " 'futures-related',\n",
       " 'gained',\n",
       " 'gaining',\n",
       " 'galling',\n",
       " 'galvanized',\n",
       " 'gambling',\n",
       " 'gauging',\n",
       " 'generated',\n",
       " 'getting',\n",
       " 'giving',\n",
       " 'going',\n",
       " 'good-hearted',\n",
       " 'good-natured',\n",
       " 'gored',\n",
       " 'government-certified',\n",
       " 'government-funded',\n",
       " 'government-owned',\n",
       " 'graduated',\n",
       " 'granted',\n",
       " 'granting',\n",
       " 'greed',\n",
       " 'gripping',\n",
       " 'growing',\n",
       " 'guaranteed',\n",
       " 'guarding',\n",
       " 'guided',\n",
       " 'gut-wrenching',\n",
       " 'hailed',\n",
       " 'hailing',\n",
       " 'halted',\n",
       " 'hampered',\n",
       " 'handed',\n",
       " 'handled',\n",
       " 'handling',\n",
       " 'happened',\n",
       " 'happening',\n",
       " 'hard-charging',\n",
       " 'hard-drinking',\n",
       " 'hard-hitting',\n",
       " 'harmed',\n",
       " 'harped',\n",
       " 'harvested',\n",
       " 'hauled',\n",
       " 'hauling',\n",
       " 'having',\n",
       " 'headed',\n",
       " 'heading',\n",
       " 'headlined',\n",
       " 'healing',\n",
       " 'hearing',\n",
       " 'heated',\n",
       " 'heating',\n",
       " 'hedging',\n",
       " 'heightened',\n",
       " 'helped',\n",
       " 'helping',\n",
       " 'high-flying',\n",
       " 'high-minded',\n",
       " 'high-polluting',\n",
       " 'high-priced',\n",
       " 'high-rolling',\n",
       " 'high-speed',\n",
       " 'higher-salaried',\n",
       " 'highest-pitched',\n",
       " 'hired',\n",
       " 'hitting',\n",
       " 'holding',\n",
       " 'hoped',\n",
       " 'hosted',\n",
       " 'housing',\n",
       " 'hugging',\n",
       " 'hundred',\n",
       " 'hunted',\n",
       " 'hurting',\n",
       " 'identified',\n",
       " 'ignored',\n",
       " 'ignoring',\n",
       " 'impaired',\n",
       " 'impeding',\n",
       " 'impending',\n",
       " 'implemented',\n",
       " 'implied',\n",
       " 'imported',\n",
       " 'imposed',\n",
       " 'imposing',\n",
       " 'impressed',\n",
       " 'improved',\n",
       " 'improving',\n",
       " 'incentive-backed',\n",
       " 'inched',\n",
       " 'inching',\n",
       " 'included',\n",
       " 'including',\n",
       " 'incorporated',\n",
       " 'increased',\n",
       " 'increasing',\n",
       " 'incurred',\n",
       " 'indeed',\n",
       " 'index-related',\n",
       " 'indicated',\n",
       " 'indicating',\n",
       " 'indulging',\n",
       " 'industrialized',\n",
       " 'industry-supported',\n",
       " 'inflated',\n",
       " 'influenced',\n",
       " 'influencing',\n",
       " 'infringed',\n",
       " 'inherited',\n",
       " 'initialing',\n",
       " 'initiated',\n",
       " 'initiating',\n",
       " 'injecting',\n",
       " 'injuring',\n",
       " 'inkling',\n",
       " 'inquiring',\n",
       " 'inserted',\n",
       " 'insider-trading',\n",
       " 'insinuating',\n",
       " 'insisted',\n",
       " 'inspired',\n",
       " 'installed',\n",
       " 'installing',\n",
       " 'instituted',\n",
       " 'instructed',\n",
       " 'insured',\n",
       " 'integrated',\n",
       " 'intended',\n",
       " 'intentioned',\n",
       " 'interest-bearing',\n",
       " 'interested',\n",
       " 'interesting',\n",
       " 'interrogated',\n",
       " 'interviewed',\n",
       " 'intriguing',\n",
       " 'introduced',\n",
       " 'introducing',\n",
       " 'invented',\n",
       " 'inverted',\n",
       " 'invested',\n",
       " 'investigating',\n",
       " 'investing',\n",
       " 'inviting',\n",
       " 'involved',\n",
       " 'involving',\n",
       " 'issued',\n",
       " 'issuing',\n",
       " 'jeopardizing',\n",
       " 'joined',\n",
       " 'joining',\n",
       " 'judged',\n",
       " 'jumped',\n",
       " 'jumping',\n",
       " 'justified',\n",
       " 'justifying',\n",
       " 'keeping',\n",
       " 'kicked',\n",
       " 'kidnapping',\n",
       " 'killed',\n",
       " 'killing',\n",
       " 'knitted',\n",
       " 'knocked',\n",
       " 'labeled',\n",
       " 'labeling',\n",
       " 'labor-backed',\n",
       " 'lacked',\n",
       " 'lagging',\n",
       " 'land-idling',\n",
       " 'landing',\n",
       " 'lasted',\n",
       " 'lasting',\n",
       " 'lauded',\n",
       " 'laughing',\n",
       " 'launched',\n",
       " 'lawmaking',\n",
       " 'laying',\n",
       " 'leading',\n",
       " 'learned',\n",
       " 'learning',\n",
       " 'leasing',\n",
       " 'leaving',\n",
       " 'led',\n",
       " 'lending',\n",
       " 'lengthened',\n",
       " 'lessening',\n",
       " 'letter-writing',\n",
       " 'letting',\n",
       " 'leveling',\n",
       " 'leveraged',\n",
       " 'leveraging',\n",
       " 'licensed',\n",
       " 'licensing',\n",
       " 'lifted',\n",
       " ...]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The \"|\" means or\n",
    "[w for w in wsj if re.search('(ed|ing)$', w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Other Experessions](https://cdn.mathpix.com/snip/images/40BpK5Dmkjkhw6nxtN1aPk5o2FoE6eXSPJLowrU2_DQ.original.fullsize.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5   Useful Applications of Regular Expressions\n",
    "\n",
    "The re.findall() (\"find all\") method finds all (non-overlapping) matches of the given regular expression. Let's find all the vowels in a word, then count them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u',\n",
       " 'e',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'e',\n",
       " 'i',\n",
       " 'a',\n",
       " 'i',\n",
       " 'o',\n",
       " 'i',\n",
       " 'o',\n",
       " 'u']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'supercalifragilisticexpialidocious'\n",
    "re.findall(r'[aeiou]', word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('io', 549),\n",
       " ('ea', 476),\n",
       " ('ie', 331),\n",
       " ('ou', 329),\n",
       " ('ai', 261),\n",
       " ('ia', 253),\n",
       " ('ee', 217),\n",
       " ('oo', 174),\n",
       " ('ua', 109),\n",
       " ('au', 106),\n",
       " ('ue', 105),\n",
       " ('ui', 95)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look for all sequences of two or more vowels in some text, and determine their relative frequency:\n",
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "fd = nltk.FreqDist(vs for word in wsj for vs in re.findall(r'[aeiou]{2,}', word))\n",
    "fd.most_common(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing More with Word Pieces\n",
    "\n",
    "Once we can use re.findall() to extract material from words, there's interesting things to do with the pieces, like glue them back together or plot them.\n",
    "\n",
    "It is sometimes noted that English text is highly redundant, and it is still easy to read when word-internal vowels are left out. For example, declaration becomes dclrtn, and inalienable becomes inlnble, retaining any initial or final vowel sequences. The regular expression in our next example matches initial vowel sequences, final vowel sequences, and all consonants; everything else is ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unvrsl Dclrtn of Hmn Rghts Prmble Whrs rcgntn of the inhrnt dgnty and\n",
      "of the eql and inlnble rghts of all mmbrs of the hmn fmly is the fndtn\n",
      "of frdm , jstce and pce in the wrld , Whrs dsrgrd and cntmpt fr hmn\n",
      "rghts hve rsltd in brbrs acts whch hve outrgd the cnscnce of mnknd ,\n",
      "and the advnt of a wrld in whch hmn bngs shll enjy frdm of spch and\n"
     ]
    }
   ],
   "source": [
    "regexp = r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]'\n",
    "def compress(word):\n",
    "    pieces = re.findall(regexp, word)\n",
    "    return ''.join(pieces)\n",
    "\n",
    "english_udhr = nltk.corpus.udhr.words('English-Latin1')\n",
    "print(nltk.tokenwrap(compress(w) for w in english_udhr[:75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's combine regular expressions with conditional frequency distributions. Here we will extract all consonant-vowel sequences from the words of Rotokas, such as ka and si.\n",
    "\n",
    "Since each of these is a pair, it can be used to initialize a conditional frequency distribution. We then tabulate the frequency of each pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a   e   i   o   u \n",
      "k 418 148  94 420 173 \n",
      "p  83  31 105  34  51 \n",
      "r 187  63  84  89  79 \n",
      "s   0   0 100   2   1 \n",
      "t  47   8   0 148  37 \n",
      "v  93  27 105  48  49 \n"
     ]
    }
   ],
   "source": [
    "rotokas_words = nltk.corpus.toolbox.words('rotokas.dic')\n",
    "cvs = [cv for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
    "cfd = nltk.ConditionalFreqDist(cvs)\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kasuari']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to be able to inspect the words behind the numbers in the above table,\n",
    "#it would be helpful to have an index, allowing us to quickly find the list of words \n",
    "#that contains a given consonant-vowel pair, e.g. cv_index['su'] should give us all words containing su. \n",
    "#Here's how we can do this:\n",
    "cv_word_pairs = [(cv, w) for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
    "#This is a cool method that it has. It treats the first element in a tuple as the index\n",
    "#Then maps it to the other item in the ordered pair. \n",
    "cv_index = nltk.Index(cv_word_pairs)\n",
    "\n",
    "cv_index['su']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Word Stems\n",
    "\n",
    "When we use a web search engine, we usually don't mind (or even notice) if the words in the document differ from our search terms in having different endings. A query for laptops finds documents containing laptop and vice versa. Indeed, laptop and laptops are just two forms of the same dictionary word (or lemma). For some language processing tasks we want to ignore word endings, and just deal with word stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        return word\n",
    "    \n",
    "#Although we will ultimately use NLTK's built-in stemmers, it's interesting to see how we can use regular expressions \n",
    "#for this task. Our first step is to build up a disjunction of all the suffixes. \n",
    "#We need to enclose it in parentheses in order to limit the scope of the disjunction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ing']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, re.findall() just gave us the suffix even though the regular expression matched the entire word. This is because the parentheses have a second function, to select substrings to be extracted. If we want to use the parentheses to specify the scope of the disjunction, but not to select the material to be output, we have to add ?:, which is just one of many arcane subtleties of regular expressions. Here's the revised version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['processing']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('process', 'ing')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# However, we'd actually like to split the word into stem and suffix. \n",
    "# So we should just parenthesize both parts of the regular expression:\n",
    "re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('processe', 's')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets try another word\n",
    "re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regular expression incorrectly found an -s suffix instead of an -es suffix. This demonstrates another subtlety: the star operator is \"greedy\" and the .* part of the expression tries to consume as much of the input as possible. If we use the \"non-greedy\" version of the star operator, written *?, we get what we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('process', 'es')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('language', '')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This works even when we allow an empty suffix, by making the content of the second parentheses optional:\n",
    "re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$', 'language')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching Tokenized Text\n",
    "\n",
    "You can use a special kind of regular expression for searching across multiple words in a text (where a text is a list of tokens). For example, \"$<a> <man>$\" finds all instances of a man in the text. The **angle brackets** are used to mark token boundaries, and any whitespace between the angle brackets is ignored (behaviors that are unique to NLTK's findall() method for texts). In the following example, we include $<.*>$ \n",
    "\n",
    "[1] which will match any single token, and enclose it in parentheses so only the matched word (e.g. monied) and not the matched phrase (e.g. a monied man) is produced. The second example finds three-word phrases ending with the word bro [2]. The last example finds sequences of three or more words starting with the letter l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
      "mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
      "pale; furious; better; certain; complete; dismasted; younger; brave;\n",
      "brave; brave; brave\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg, nps_chat\n",
    "moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))\n",
    "#1\n",
    "moby.findall(r\"<a> (<.*>) <man>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you rule bro; telling you bro; u twizted bro\n"
     ]
    }
   ],
   "source": [
    "chat = nltk.Text(nps_chat.words())\n",
    "chat.findall(r\"<.*> <.*> <bro>\") #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol lol lol; lmao lol lol; lol lol lol; la la la la la; la la la; la\n",
      "la la; lovely lol lol love; lol lol lol.; la la la; la la la\n"
     ]
    }
   ],
   "source": [
    "chat.findall(r\"<l.*>{3,}\")#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed and other activities; water and other liquids; tomb and other\n",
      "landmarks; Statues and other monuments; pearls and other jewels;\n",
      "charts and other items; roads and other features; figures and other\n",
      "objects; military and other areas; demands and other factors;\n",
      "abstracts and other compilations; iron and other metals\n"
     ]
    }
   ],
   "source": [
    "#It is easy to build search patterns when the linguistic phenomenon we're studying is tied to particular words. \n",
    "#In some cases, a little creativity will go a long way.\n",
    "#For instance, searching a large text corpus for expressions of the form x and other ys allows us to discover hypernyms \n",
    "from nltk.corpus import brown\n",
    "hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))\n",
    "hobbies_learned.findall(r\"<\\w*> <and> <other> <\\w*s>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With enough text, this approach would give us a useful store of information about the taxonomy of objects, without the need for any manual labor. However, our search results will usually contain false positives, i.e. cases that we would want to exclude. For example, the result: demands and other factors suggests that demand is an instance of the type factor, but this sentence is actually about wage demands. Nevertheless, we could construct our own ontology of English concepts by manually correcting the output of such searches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6   Normalizing Text\n",
    "\n",
    "In earlier examples we  converted text to lowercase, e.g. set(w.lower() for w in text). By using lower(), we have **normalized** the text to lowercase so that the distinction between The and the is ignored.\n",
    "\n",
    "Often we want to go further than this, and strip off any affixes, a task known as **stemming**. \n",
    "\n",
    "A further step is to make sure that the resulting form is a known word in a dictionary, a task known as **lemmatization**. \n",
    "\n",
    "We discuss each of these in turn. First, we need to define the data we will use in this section:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemmers\n",
    "\n",
    "NLTK includes several off-the-shelf stemmers. The Porter and Lancaster stemmers follow their own rules for stripping affixes. Observe that the Porter stemmer correctly handles the word lying (mapping it to lie), while the Lancaster stemmer does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['denni',\n",
       " ':',\n",
       " 'listen',\n",
       " ',',\n",
       " 'strang',\n",
       " 'women',\n",
       " 'lie',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distribut',\n",
       " 'sword',\n",
       " 'is',\n",
       " 'no',\n",
       " 'basi',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'govern',\n",
       " '.',\n",
       " 'suprem',\n",
       " 'execut',\n",
       " 'power',\n",
       " 'deriv',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mandat',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'some',\n",
       " 'farcic',\n",
       " 'aquat',\n",
       " 'ceremoni',\n",
       " '.']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "[porter.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['den',\n",
       " ':',\n",
       " 'list',\n",
       " ',',\n",
       " 'strange',\n",
       " 'wom',\n",
       " 'lying',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distribut',\n",
       " 'sword',\n",
       " 'is',\n",
       " 'no',\n",
       " 'bas',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'govern',\n",
       " '.',\n",
       " 'suprem',\n",
       " 'execut',\n",
       " 'pow',\n",
       " 'der',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mand',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'som',\n",
       " 'farc',\n",
       " 'aqu',\n",
       " 'ceremony',\n",
       " '.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lancaster.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is not a well-defined process, and we typically pick the stemmer that best suits the application we have in mind. The Porter Stemmer is a good choice if you are indexing some texts and want to support search using alternative forms of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedText(object):\n",
    "\n",
    "    def __init__(self, stemmer, text):\n",
    "        self._text = text\n",
    "        self._stemmer = stemmer\n",
    "        #Great was to use enumerate\n",
    "        self._index = nltk.Index((self._stem(word), i)\n",
    "                                 for (i, word) in enumerate(text))\n",
    "\n",
    "    def concordance(self, word, width=40):\n",
    "        key = self._stem(word)\n",
    "        wc = int(width/4)                # words of context\n",
    "        for i in self._index[key]:\n",
    "            lcontext = ' '.join(self._text[i-wc:i])\n",
    "            rcontext = ' '.join(self._text[i:i+wc])\n",
    "            #I'm a little lost on how ldisplat and rcontext work. \n",
    "            ldisplay = '{:>{width}}'.format(lcontext[-width:], width=width)\n",
    "            rdisplay = '{:{width}}'.format(rcontext[:width], width=width)\n",
    "            print(ldisplay, rdisplay)\n",
    "\n",
    "    def _stem(self, word):\n",
    "        #I'm assuming .stem() comes from the _stemmer which is a real stemmer\n",
    "        return self._stemmer.stem(word).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r king ! DENNIS : Listen , strange women lying in ponds distributing swords is no\n",
      " beat a very brave retreat . ROBIN : All lies ! MINSTREL : [ singing ] Bravest of\n",
      "       Nay . Nay . Come . Come . You may lie here . Oh , but you are wounded !   \n",
      "doctors immediately ! No , no , please ! Lie down . [ clap clap ] PIGLET : Well  \n",
      "ere is much danger , for beyond the cave lies the Gorge of Eternal Peril , which \n",
      "   you . Oh ... TIM : To the north there lies a cave -- the cave of Caerbannog --\n",
      "h it and lived ! Bones of full fifty men lie strewn about its lair . So , brave k\n",
      "not stop our fight ' til each one of you lies dead , and the Holy Grail returns t\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "grail = nltk.corpus.webtext.words('grail.txt')\n",
    "text = IndexedText(porter, grail)\n",
    "text.concordance('lie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "The WordNet lemmatizer only removes affixes if the resulting word is in its dictionary. This additional checking process makes the lemmatizer slower than the above stemmers. Notice that it doesn't handle lying, but it converts women to woman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DENNIS',\n",
       " ':',\n",
       " 'Listen',\n",
       " ',',\n",
       " 'strange',\n",
       " 'woman',\n",
       " 'lying',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distributing',\n",
       " 'sword',\n",
       " 'is',\n",
       " 'no',\n",
       " 'basis',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'government',\n",
       " '.',\n",
       " 'Supreme',\n",
       " 'executive',\n",
       " 'power',\n",
       " 'derives',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mandate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'some',\n",
       " 'farcical',\n",
       " 'aquatic',\n",
       " 'ceremony',\n",
       " '.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "[wnl.lemmatize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WordNet lemmatizer is a good choice if you want to compile the vocabulary of some texts and want a list of valid lemmas (or lexicon headwords).\n",
    "\n",
    "\n",
    "Another normalization task involves identifying **non-standard words** including numbers, abbreviations, and dates, and mapping any such tokens to a special vocabulary. For example, every decimal number could be mapped to a single token 0.0, and every acronym could be mapped to AAA. This keeps the vocabulary small and improves the accuracy of many language modeling tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.7   Regular Expressions for Tokenizing Text\n",
    "\n",
    "**Tokenization** is the task of cutting a string into identifiable linguistic units that constitute a piece of language data. Although it is a fundamental task, we have been able to delay it until now because many corpora are already tokenized, and because NLTK includes some tokenizers. Now that you are familiar with regular expressions, you can learn how to use them to tokenize text, and to have much more control over the process.\n",
    "\n",
    "## Simple Approaches to Tokenization\n",
    "\n",
    "The very simplest method for tokenizing text is to split on whitespace. Consider the following text from Alice's Adventures in Wonderland:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"'When I'M a Duchess,' she said to herself, (not in a very hopeful tone\n",
    "though), 'I won't have any pepper in my kitchen AT ALL. Soup does very\n",
    "well without--Maybe it's always pepper that makes people hot-tempered,'...\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could split this raw text on whitespace using raw.split(). To do the same using a regular expression, it is not enough to match any space characters in the string [1] since this results in tokens that contain a \\n newline character; instead we need to match any number of spaces, tabs, or newlines [2]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'When\",\n",
       " \"I'M\",\n",
       " 'a',\n",
       " \"Duchess,'\",\n",
       " 'she',\n",
       " 'said',\n",
       " 'to',\n",
       " 'herself,',\n",
       " '(not',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'hopeful',\n",
       " 'tone\\nthough),',\n",
       " \"'I\",\n",
       " \"won't\",\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'AT',\n",
       " 'ALL.',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very\\nwell',\n",
       " 'without--Maybe',\n",
       " \"it's\",\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " \"hot-tempered,'...\"]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " re.split(r' ', raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'When\",\n",
       " \"I'M\",\n",
       " 'a',\n",
       " \"Duchess,'\",\n",
       " 'she',\n",
       " 'said',\n",
       " 'to',\n",
       " 'herself,',\n",
       " '(not',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'hopeful',\n",
       " 'tone',\n",
       " 'though),',\n",
       " \"'I\",\n",
       " \"won't\",\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'AT',\n",
       " 'ALL.',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very',\n",
       " 'well',\n",
       " 'without--Maybe',\n",
       " \"it's\",\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " \"hot-tempered,'...\"]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'[ \\t\\n]+', raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other whitespace characters, such as carriage-return and form-feed should really be included too. Instead, we will use a built-in re abbreviation, \\s, which means any whitespace character. The above statement can be rewritten as re.split(r'\\s+', raw).\n",
    "\n",
    "\n",
    "Splitting on whitespace gives us tokens like '(not' and 'herself,'. An alternative is to use the fact that Python provides us with a character class \\w for word characters, equivalent to [a-zA-Z0-9_].\n",
    "\n",
    "It also defines the complement of this class \\W, i.e. all characters other than letters, digits or underscore. We can use \\W in a simple regular expression to split the input on anything other than a word character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'When',\n",
       " 'I',\n",
       " 'M',\n",
       " 'a',\n",
       " 'Duchess',\n",
       " 'she',\n",
       " 'said',\n",
       " 'to',\n",
       " 'herself',\n",
       " 'not',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'hopeful',\n",
       " 'tone',\n",
       " 'though',\n",
       " 'I',\n",
       " 'won',\n",
       " 't',\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'AT',\n",
       " 'ALL',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very',\n",
       " 'well',\n",
       " 'without',\n",
       " 'Maybe',\n",
       " 'it',\n",
       " 's',\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " 'hot',\n",
       " 'tempered',\n",
       " '']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'\\W+', raw)\n",
    "#Observe that this gives us empty strings at the start and the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'When\",\n",
       " 'I',\n",
       " \"'M\",\n",
       " 'a',\n",
       " 'Duchess',\n",
       " ',',\n",
       " \"'\",\n",
       " 'she',\n",
       " 'said',\n",
       " 'to',\n",
       " 'herself',\n",
       " ',',\n",
       " '(not',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'hopeful',\n",
       " 'tone',\n",
       " 'though',\n",
       " ')',\n",
       " ',',\n",
       " \"'I\",\n",
       " 'won',\n",
       " \"'t\",\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'AT',\n",
       " 'ALL',\n",
       " '.',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very',\n",
       " 'well',\n",
       " 'without',\n",
       " '-',\n",
       " '-Maybe',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " 'hot',\n",
       " '-tempered',\n",
       " ',',\n",
       " \"'\",\n",
       " '.',\n",
       " '.',\n",
       " '.']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The regular expression «\\w+|\\S\\w*» will first try to match any sequence of word characters. \n",
    "#If no match is found, it will try to match any non-whitespace character (\\S is the complement of \\s)\n",
    "#followed by further word characters. This means that punctuation is grouped with any following letters (e.g. 's) \n",
    "#but that sequences of two or more punctuation characters are separated.\n",
    "re.findall(r'\\w+|\\S\\w*', raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", 'When', \"I'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',', '(', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'\", 'I', \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does', 'very', 'well', 'without', '--', 'Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', 'hot-tempered', ',', \"'\", '...']\n"
     ]
    }
   ],
   "source": [
    "# Let's generalize the \\w+ in the above expression to permit word-internal hyphens and apostrophes\n",
    "print(re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", raw))\n",
    "#he above expression also included «[-.(]+» which causes the double hyphen, \n",
    "#ellipsis, and open parenthesis to be tokenized separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Regex Class Symbols](https://cdn.mathpix.com/snip/images/RFu1UNqcY-Rjd5yIWIxyiIKgeUsa38i6X3CQYErjQA8.original.fullsize.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK's Regular Expression Tokenizer\n",
    "\n",
    "The function nltk.regexp_tokenize() is similar to re.findall() (as we've been using it for tokenization). However, nltk.regexp_tokenize() is more efficient for this task, and avoids the need for special treatment of parentheses.\n",
    "\n",
    "For readability we break up the regular expression over several lines and add a comment about each line. The special (?x) \"verbose flag\" tells Python to strip out the embedded whitespace and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "pattern = r'''(?x)     # set flag to allow verbose regexps\n",
    "    (?:[A-Z]\\.)+       # abbreviations, e.g. U.S.A.\n",
    "  | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
    "  | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
    "  | \\.\\.\\.             # ellipsis\n",
    "  | [][.,;\"'?():-_`]   # these are separate tokens; includes ], [\n",
    "'''\n",
    "nltk.regexp_tokenize(text, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the verbose flag, you can no longer use ' ' to match a space character; use \\s instead. The regexp_tokenize() function has an optional gaps parameter. When set to True, the regular expression specifies the gaps between tokens, as with re.split().\n",
    "\n",
    "## Further Issues with Tokenization\n",
    "\n",
    "Tokenization turns out to be a far more difficult task than you might have expected. **No single solution works well across-the-board, and we must decide what counts as a token depending on the application domain**.\n",
    "\n",
    "When developing a tokenizer it helps to have access to raw text which has been manually tokenized, in order to compare the output of your tokenizer with high-quality (or \"gold-standard\") tokens. The NLTK corpus collection includes a sample of Penn Treebank data, including the raw Wall Street Journal text (nltk.corpus.treebank_raw.raw()) and the tokenized version (nltk.corpus.treebank.words()).\n",
    "\n",
    "A final issue for tokenization is the presence of contractions, such as didn't. If we are analyzing the meaning of a sentence, it would probably be more useful to normalize this form to two separate forms: did and n't (or not). We can do this work with the help of a lookup table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.8 Segmentation\n",
    "\n",
    "This section discusses more advanced concepts, skip it.\n",
    "\n",
    "# 3.9   Formatting: From Lists to Strings\n",
    "\n",
    "Often we write a program to report a single data item, such as a particular element in a corpus that meets some complicated criterion, or a single summary statistic such as a word-count or the performance of a tagger. \n",
    "\n",
    "More often, we write a program to produce a structured result; for example, a tabulation of numbers or linguistic forms, or a reformatting of the original data. \n",
    "\n",
    "When the results to be presented are linguistic, textual output is usually the most natural choice. However, when the results are numerical, it may be preferable to produce graphical output. In this section you will learn about a variety of ways to present program output.\n",
    "\n",
    "# From Lists to Strings\n",
    "\n",
    "The simplest kind of structured object we use for text processing is lists of words. When we want to output these to a display or a file, we must convert these lists into strings. To do this in Python we use the join() method, and specify the string to be used as the \"glue\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We called him Tortoise because he taught us .'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silly = ['We', 'called', 'him', 'Tortoise', 'because', 'he', 'taught', 'us', '.']\n",
    "' '.join(silly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strings and Formats\n",
    "\n",
    "The print command yields Python's attempt to produce the most human-readable form of an object. The second method — naming the variable at a prompt — shows us a string that can be used to recreate this object. It is important to keep in mind that both of these are just strings, displayed for the benefit of you, the user. They do not give us any clue as to the actual internal representation of the object.\n",
    "\n",
    "There are many other useful ways to display an object as a string of characters. They may be for the benefit of a human reader, or because we want to export our data to a particular file format for use in an external program.\n",
    "\n",
    "I skipped a lot of formatting I already knew how to do.\n",
    "\n",
    "\n",
    "## Lining Things Up\n",
    "\n",
    "We can add padding to obtain output of a given width by inserting into the brackets a colon ':' followed by an integer. So {:6} specifies that we want a string that is padded to width 6. \n",
    "\n",
    "It is right-justified by default for numbers [1], but we can precede the width specifier with a '<' alignment option to make numbers left-justified [2].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    41'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{:6}'.format(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'41    '"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The carrot tells us where the spacing goes\n",
    "'{:<6}' .format(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    41'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{:6}' .format(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1416'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Other control characters can be used to specify the sign and precision of floating point numbers; for example {:.4f} \n",
    "# indicates that four digits should be displayed after the decimal point for a floating point number.\n",
    "import math\n",
    "'{:.4f}'.format(math.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy for 9375 words: 34.1867%'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The string formatting is smart enough to know that if you include a '%' in your format specification, \n",
    "# then you want to represent the value as a percentage; there's no need to multiply by 100.\n",
    "count, total = 3205, 9375\n",
    "\"accuracy for {} words: {:.4%}\".format(total, count / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category            can  could    may  might   must   will \n",
      "news                 93     86     66     38     50    389 \n",
      "religion             82     59     78     12     54     71 \n",
      "hobbies             268     58    131     22     83    264 \n",
      "science_fiction      16     49      4     12      8     16 \n",
      "romance              74    193     11     51     45     43 \n",
      "humor                16     30      8      8      9     13 \n"
     ]
    }
   ],
   "source": [
    "#An important use of formatting strings is for tabulating data. Recall that in 1 we saw data being \n",
    "#tabulated from a conditional frequency distribution. Let's perform the tabulation ourselves.\n",
    "\n",
    "def tabulate(cfdist, words, categories):\n",
    "    print('{:16}'.format('Category'), end=' ')                    # column headings\n",
    "    for word in words:\n",
    "        print('{:>6}'.format(word), end=' ')\n",
    "    print()\n",
    "    for category in categories:\n",
    "        print('{:16}'.format(category), end=' ')                  # row heading\n",
    "        for word in words:                                        # for each word\n",
    "            print('{:6}'.format(cfdist[category][word]), end=' ') # print table cell\n",
    "        print()                                                   # end the row\n",
    "from nltk.corpus import brown\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (genre, word)\n",
    "    for genre in brown.categories()\n",
    "    for word in brown.words(categories=genre))\n",
    "\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "tabulate(cfd, modals, genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Python   '"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This allows us to specify the width of a field using a variable\n",
    "'{:{width}}'.format('Monty Python', width=15)\n",
    "# We could use this to automatically customize the column to be just wide enough to accommodate\n",
    "# all the words, using width = max(len(w) for w in words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Results to a File\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open('output.txt', 'w')\n",
    "words = set(nltk.corpus.genesis.words('english-kjv.txt'))\n",
    "for word in sorted(words):\n",
    "    print(word, file=output_file)\n",
    "#When we write non-text data to a file we must convert it to a string first.\n",
    "# use str() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Wrapping\n",
    "\n",
    "When the output of our program is text-like, instead of tabular, it will usually be necessary to wrap it so that it can be displayed conveniently. Consider the following output, which overflows its line, and which uses a complicated print statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After (5), all (3), is (2), said (4), and (3), done (4), , (1), more (4), is (2), said (4), than (4), done (4), . (1), "
     ]
    }
   ],
   "source": [
    "saying = ['After', 'all', 'is', 'said', 'and', 'done', ',',\n",
    "          'more', 'is', 'said', 'than', 'done', '.']\n",
    "for word in saying:\n",
    "    print(word, '(' + str(len(word)) + '),', end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 5 all 3 is 2 said 4 and 3 done 4 , 1 more 4 is 2 said 4 than 4\n",
      "done 4 . 1\n"
     ]
    }
   ],
   "source": [
    "# We can take care of line wrapping with the help of Python's textwrap module.\n",
    "from textwrap import fill\n",
    "pieces = [\"{} {}\".format(word, len(word)) for word in saying]\n",
    "output = ' '.join(pieces)\n",
    "wrapped = fill(output)\n",
    "print(wrapped)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
